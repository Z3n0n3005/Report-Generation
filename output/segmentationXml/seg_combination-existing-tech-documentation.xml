---
abstractSeg: "Reading 3 (required). Note: This TOC also provided as a separate file\
  \ to view side-by-side Look over these pages to get a general idea and dig deeper\
  \ if something piques your interest. This is a compilation of materials on the LUXID\
  \ system (and its forerunner Insight [not to be confused with Inxight]) produced\
  \ by TEMIS Corp., and on the SAP BusinessSystems ThingFinder system. ThingFinder\
  \ originated at Inxight corporation, a spinoff from Xerox PARC (Palo Alto Research\
  \ Center of mouse and the MAC and Windows interface fame); Inxight was bought by\
  \ BusinessSystems, which was bought by SAP, a major business software vendor.Both\
  \ systems extract information from text. They process text to extract entities and\
  \ statements that connect entities through relationships. They also assign metadata\
  \ to documents, in other words, they assist in (or take over) cataloging. The marketing\
  \ hype says that these systems convert unstructured data (text, better called data\
  \ with complex structure) into structured data, entity-relationship statements often\
  \ represented in tables and graphs, structures that are simpler and easier to search\
  \ than text.The compilation includes old documents from Inxight because they give\
  \ better examples and a much better explanation of the process."
sectionList:
- header: "Overview"
  content: "With the steady growth of business and scientific activities and the recent\
    \ advances in Information Technology, huge amounts of electronically available\
    \ but unstructured data have to be dealt with. New tools able to analyse and structure\
    \ textual data need to be developed so that non-expert users can understand and\
    \ evaluate the contents of their documents. In this context, a successful information\
    \ extraction technology has a central role to play. Information Extraction is\
    \ the process of identifying relevant information where the criteria for relevance\
    \ are predefined in the form of a template that is to be filled. The template\
    \ pertains to actions between different actors related to events or situations\
    \ and contains slots that denote who did what to whom, when and where, and possibly\
    \ why. The template builder has to predict what information will be of interest\
    \ to the customer according to his field of activity.In an industrial context,\
    \ the aim of Information Extraction is to build an extraction model adapted to\
    \ the customer's application (its information assets: databases, documents and\
    \ all the data issued by or received in a company). It requires the creation and\
    \ validation of terminology resources specific to the described field as well\
    \ as the definition of selection criteria using extraction rules."
- header: "Methodology"
  content: "Our approach is based on discovering knowledge from a corpus in an iterative\
    \ way. We use various text analysis processing: morpho-syntactic analysis, named\
    \ entities recognition, pattern recognition using linguistics and/or semantic\
    \ labels. The main idea is to build patterns 1 . If, for instance, the aim of\
    \ the application is to detect company mergers or acquisitions in press releases,\
    \ it will seek expressions like 'company A acquired company B' or 'company A merged\
    \ with company C'. As company may be a company name and all the common words which\
    \ refer to a company (firm, manufacturer, corporation, …). They are gathered under\
    \ the semantic descriptor 'company.' In the case of an application dedicated to\
    \ the field of Competitive Intelligence, verbs like buy, sell, and acquire, which\
    \ refer to a transfer of possessions, are coded together under the same descriptor.In\
    \ press releases, the economic actors are mentioned within their context. It is\
    \ therefore necessary to associate specific terms under a common descriptor. This\
    \ task is achieved through the use of extraction rules. An extraction rule is\
    \ expressed by a regular expression which may refer to a lemma (canonical form),\
    \ syntactic, or semantic label.Example: \"the first private label pasta maker\"\
    \ will be caught by the rule:(company_Adj|Loc_Adj|#ORD)*/ (brand_product) / (Company)\
    \ Company_Adj refers to adjectives such as \"leading\", \"industrial\" or \"public\"\
    \ LocAdj refers to adjectives describing places such as \"European\" or \"Italian\"\
    \ #ORD refers to ordinals (first, second, …) brand_product private label, branded\
    \ + #NOUN / symbol separates two words # symbol denotes a syntactic tag (differentiates\
    \ the syntactic labels from the semantic ones).An expression like 'company A acquired\
    \ company B' will be extracted by the following rule:1 A pattern is a regular\
    \ expression whose purpose is to identify relevant phrases within a context. Combining\
    \ an action to a pattern, a rule will add information to a word sequence, assigning\
    \ a name concept which can be used by other rules."
- header: "Processing"
  content: "The extraction server proceeds in two stages: 1. the morpho-syntactic\
    \ analysis -each entry is assigned a part of speech and a morphologic feature\
    \ 2. the application of extraction rules."
- header: "Transducers Technology"
  content: "The extraction component, which performs semantic labeling from morpho-syntactically\
    \ tagged texts, has been implemented using a dynamic composition of transducers.\
    \ According to the specific processing, three levels of transducer components\
    \ have been distinguished."
- header: "Results"
  content: "Naturally, identifying names in free text is not a classification task.\
    \ On the contrary, it requires a specific kind of parsing. People's names (and\
    \ to a lesser extent, names of organizations) bear a specific structure that can\
    \ be recognized by pattern matching with sufficient accuracy.Insight Discoverer™\
    \ Extractor uses a restricted form of a regular grammar to perform this recognition\
    \ process. Some 15 rules specify the way in which titles, first names, family\
    \ names and job titles are typically combined to form people's names in texts.\
    \ Large collections of first names, job titles etc. are used as knowledge sources\
    \ for the recognition component.After having recognized proper names in a document,\
    \ Insight Discoverer™ Extractor also tries to find canonical forms for the identified\
    \ names (i.e. if someone is mentioned with a title once, with the first name a\
    \ second time and with neither in a third position, Insight Discoverer™ Extractor\
    \ recognizes that the person which the three forms refer to, has been mentioned\
    \ three times in this document). Insight Discoverer™ Extractor even performs a\
    \ rudimentary kind of anaphora resolution in order to count the number of occurrences\
    \ of a name. That means that in (the corresponding German translation of) a sentence\
    \ like (1)(1)Madonna was not only one of the most controversial artists of the\
    \ nineties, she was also one of the most successful.Madonna will be treated as\
    \ if she had been mentioned twice. Counting the number of occurrences in the document\
    \ is used as the prime criterion to assign a relevance measure to a name.Apart\
    \ from their importance as index terms for a document, people's names also have\
    \ a considerable effect on the classification process. Failing to recognize them\
    \ and treat them as one token in the text (rather than treating first and family\
    \ names as separate tokens) would mean that important information is ignored and\
    \ would therefore lead to poorer classification results. The following hypothetical\
    \ example might illustrate this point.(2) Helmut Schmidt (former German chancelor):\
    \ politics Martin Scorsese (US film director): film Martin Schmidt (German ski\
    \ athlete): sportsAssuming that we had no name recognition component and assuming\
    \ that the classifier has learnt that the tokens Helmut and Schmidt have some\
    \ significance for the topic politics and Martin and Scorsese have some relevance\
    \ for film, then the tokens Martin and Schmidt will falsely signal some influence\
    \ towards politics and film even if they occur as parts of another name, here\
    \ that of an athlete. The name recognition component on the other hand will treat\
    \ these names as tokens, exploiting the fact that in most domains, most names\
    \ tend to be mentioned with a limited set of topics and thus often serve as good\
    \ features for a classification component.DocCat currently runs in a specific\
    \ setting where documents that are assigned to one or several broad topics (TOP\
    \ = {sports, theatre, economics, ...}) have to be annotated with finer labels\
    \ from a fixed vocabulary of thesaurus terms (SUBTOP = {..., aids, demonstration,\
    \ university, theft, cancer, ... }). Of these, DocCat has learnt to assign some\
    \ 600 terms1 . The results on the different broad topics reflect the internal\
    \ homogeneity of the respective sub corpora: On a well-defined topic such as sports\
    \ (recall 60%, precision 70%), DocCat performed much better than in other areas\
    \ like society/celebrities (recall 51%, precision 40%) in assigning the correct\
    \ specific topic terms. The first line for instance in table 2, reflects the fact\
    \ that on the sub-corpus annotated with sports DocCat performed the task of selecting\
    \ the correct assignments with 60% recall and 70% precision from a set of some\
    \ 600 SUBTOP labels. It has to be noted that these results reflect the number\
    \ of cases where the DocCat result differs from the initial manual annotation\
    \ found in the test set, no matter how this difference should be judged. Besides\
    \ the cases, however, where we have to admit a clear DocCat error, there are also\
    \ many cases where one has to conclude that both results (the manual and the automatic)\
    \ are equally valid and even a considerable fraction, where the results of DocCat\
    \ can or have to be considered superior. While we have yet to evaluate this question\
    \ quantitatively, we are confident that such a study will prove that the quality\
    \ of DocCat is even better than that reflected in the tables above."
- header: "System specifications"
  content: "The DocCat system is implemented using C++ for the number crunching part\
    \ of the calculations and Perl for the text processing parts and as a glue language.\
    \ It currently runs on AIX and Windows (95 and NT). A minimum of 128MB of RAM\
    \ is necessary for the training of moderately large corpora (> 50,000 documents).\
    \ The resulting classifier then is able to process a document in less than a second\
    \ and annotate it in the way described above.DocCat runs as a separate process\
    \ that is called over a simple interface (file or TCP/IP) from the respective\
    \ text documentation environment and delivers its results back over the same interface.\
    \ Thus DocCat has minimal requirements concerning the target environment and can\
    \ be employed in any documentation system where external functionalities can be\
    \ integrated."
- header: "Conclusion and Outlook"
  content: "The DocCat automatic indexing system can be applied in a real world setting,\
    \ yielding results that enhance the efficiency of manual indexing. For the setting\
    \ up of a classifier for a new domain or a new classification system, nothing\
    \ but the annotated training corpus is neededno hand coding of grammar or correlation\
    \ rules is required. "
- header: "Introduction"
  content: "DocCat is a project that uses Insight Discoverer™ technologies (ID Extractor\
    \ and ID Categorizer), which is a system for the automatic categorization, indexing,\
    \ and archival of textual data for large commercial text archives. DocCat was\
    \ one of the first systems for automatic indexing that was put in production in\
    \ a real-world environment and demonstrates the usefulness of this approach through\
    \ its use on many hundreds of new documents every day. Moreover DocCat is an ongoing\
    \ project and the properties of the system are subject to continuous refinement\
    \ and adaptation to specific requirements."
- header: "Description"
  content: "The term automatic indexing here refers to the process of assigning terms\
    \ to a document that correspond to its content, and not to the process of setting\
    \ up an index for full text retrieval. While the latter typically contains virtually\
    \ all the words of a document, with their lemmatised forms and without the stop\
    \ words, the former means annotating a document with only a few terms from a thesaurus\
    \ or some other kind of controlled set of index terms. DocCat assigns index terms\
    \ (here, \"topics\") to each document."
- header: "The making of DocCat"
  content: "In the summer of 1998, a team at the IBM Institute for Logic and Linguistics\
    \ in Heidelberg, Germany, was engaged in studying the automatic indexing of speech\
    \ data, i.e. the annotation of audio data with phrases and keywords that allow\
    \ a text-based search in a sound archive. It became apparent that a large proportion\
    \ of the functionality of such a system might be of interest for the handling\
    \ of textual data, too. After all, a textual representation (possibly infected\
    \ with errors from the speech recognition component) was to be the input for the\
    \ indexing component. Consequently, our work aroused interest not only in places\
    \ where large audio archives had to be managed, but also in the text archive field,\
    \ especially at the Gruner+Jahr (G+J) press database in Hamburg, where a number\
    \ of projects involving automatic indexing had already been considered.In the\
    \ first pilot project we were asked to demonstrate the practicability of our approach\
    \ in a limited scenario with German documents. When the results of this pilot\
    \ study turned out to be encouraging, a continued cooperation was agreed upon\
    \ where DocCat was to be extended towards a system that could be applied to everyday\
    \ tasks in a commercial text archive.The idea in both the pilot study and the\
    \ second phase was that the G&J archive should form a training corpus of manually\
    \ annotated documents that would enable specific machine learning algorithms to\
    \ collect correlations between a document's textual data and the presence or absence\
    \ of a specific label. The following were the various treated labels:Broad topic:\
    \ The manual annotations assign up to four labels to a document, indicating the\
    \ broad topic of the document (Examples: Sports, Theatre, Economics, Science,\
    \ ...). A total of 44 such topics had to be learned. Specific topics: A finer\
    \ classification of a document's topics is achieved by assigning one or more of\
    \ a fixed vocabulary of some 2000 thesaurus terms to it (Examples: Doping, Trade\
    \ Unions, Market Research, ...). Keywords: While a document need not literally\
    \ contain a thesaurus term in order to receive this term as a label, some terms\
    \ that are not part of the fixed vocabulary (i.e. the thesaurus) may also serve\
    \ as an indication of the document's contents. These"
- header: "The Insight Discoverer™ components within DocCat"
  content: "A fundamental requirement in setting up the DocCat system was that the\
    \ amount of effort that had to be invested in building corpus-or domain-specific\
    \ knowledge bases such as dictionaries, grammars or thesauri should be kept as\
    \ low as possible. This not only reduces the workload necessary for the development\
    \ of the system, but, at the same time, ensures that new classifiers for new corpora,\
    \ domains or documentation requirements could be built (i.e. \"trained\") with\
    \ minimal effort. If no new types of labels need to be considered, building a\
    \ new classifier will require only a training corpus of annotated documents as\
    \ input."
