<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Hotho</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Nürnberger</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gerhard</forename><surname>Paaß</surname></persName>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-17T03:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The enormous amount of information stored in unstructured texts cannot simply be used for further processing by computers, which typically handle text as simple sequences of character strings. Therefore, specific (pre-)processing methods and algorithms are required in order to extract useful patterns. Text mining refers generally to the process of extracting interesting information and knowledge from unstructured text. In this article, we discuss text mining as a young and interdisciplinary field in the intersection of the related areas information retrieval, machine learning, statistics, computational linguistics and especially data mining. We describe the main analysis tasks preprocessing, classification, clustering, information extraction and visualization. In addition, we briefly discuss a number of successful applications of text mining.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As computer networks become the backbones of science and economy enormous quantities of machine readable documents become available. There are estimates that 85% of business information lives in the form of text <ref type="bibr">(TMS05 2005)</ref>. Unfortunately, the usual logic-based programming paradigm has great difficulties in capturing the fuzzy and often ambiguous relations in text documents. Text mining aims at disclosing the concealed information by means of methods which on the one hand are able to cope with the large number of words and structures in natural language and on the other hand allow to handle vagueness, uncertainty and fuzziness.</p><p>In this paper we describe text mining as a truly interdisciplinary method drawing on information retrieval, machine learning, statistics, computational linguistics and especially data mining. We first give a short sketch of these methods and then define text mining in relation to them. Later sections survey state of the art approaches for the main analysis tasks preprocessing, classification, clustering, information extraction and visualization. The last section exemplifies text mining in the context of a number of successful applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Knowledge Discovery</head><p>In literature we can find different definitions of the terms knowledge discovery or knowledge discovery in databases (KDD) and data mining. In order to distinguish data mining from KDD we define KDD according to Fayyad as follows <ref type="bibr" target="#b20">(Fayyad et al. 1996)</ref>:</p><p>Knowledge Discovery in Databases (KDD) is the non-trivial process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data.</p><p>The analysis of data in KDD aims at finding hidden patterns and connections in these data. By data we understand a quantity of facts, which can be, for instance, data in a database, but also data in a simple text file. Characteristics that can be used to measure the quality of the patterns found in the data are the comprehensibility for humans, validity in the context of given statistic measures, novelty and usefulness. Furthermore, different methods are able to discover not only new patterns but to produce at the same time generalized models which represent the found connections. In this context, the expression "potentially useful" means that the samples to be found for an application generate a benefit for the user. Thus the definition couples knowledge discovery with a specific application.</p><p>Knowledge discovery in databases is a process that is defined by several processing steps that have to be applied to a data set of interest in order to extract useful patterns. These steps have to be performed iteratively and several steps usually require interactive feedback from a user. As defined by the CRoss Industry Standard Process for Data Mining (Crisp DM 1 ) model (crispdm and CRISP99 1999) the main steps are: (1) business understanding 2 , (2) data understanding, (3) data preparation, (4) modelling, (5) evaluation, (6) deployment (cf. fig. <ref type="figure" target="#fig_0">1</ref> <ref type="foot" target="#foot_0">3</ref> ). Besides the initial problem of analyzing and understanding the overall task (first two steps) one of the most time consuming steps is data preparation. This is especially of interest for text mining which needs special preprocessing methods to convert textual data into a format 1 CRoss Industry Standard Process for Data Mining Homepage, http://www.crisp-dm.org/ [accessed <ref type="bibr">May 2005]</ref>. 2 Business understanding could be defined as understanding the problem we need to solve. In the context of text mining, for example, that we are looking for groups of similar documents in a given document collection.</p><p>which is suitable for data mining algorithms. The application of data mining algorithms in the modelling step, the evaluation of the obtained model and the deployment of the application (if necessary) are closing the process cycle. Here the modelling step is of main interest as text mining frequently requires the development of new or the adaptation of existing algorithms. Research in the area of data mining and knowledge discovery is still in a state of great flux. One indicator for this is the sometimes confusing use of terms. On the one side there is data mining as synonym for KDD, meaning that data mining contains all aspects of the knowledge discovery process. This definition is in particular common in practice and frequently leads to problems to distinguish the terms clearly. The second way of looking at it considers data mining as part of the KDD-Processes (see <ref type="bibr" target="#b20">Fayyad et al. (1996)</ref>) and describes the modelling phase, i.e. the application of algorithms and methods for the calculation of the searched patterns or models. Other authors like for instance <ref type="bibr" target="#b51">Kumar &amp; Joshi (2003)</ref> consider data mining in addition as the search for valuable information in large quantities of data. In this article, we equate data mining with the modelling phase of the KDD process.</p><p>The roots of data mining lie in most diverse areas of research, which underlines the interdisciplinary character of this field. In the following we briefly discuss the relations to three of the addressed research areas: Databases, machine learning and statistics.</p><p>Databases are necessary in order to analyze large quantities of data efficiently. In this connection, a database represents not only the medium for consistent storing and accessing, but moves in the closer interest of research, since the analysis of the data with data mining algorithms can be supported by databases and thus the use of database technology in the data mining process might be useful. An overview of data mining from the database perspective can be found in <ref type="bibr" target="#b10">Chen et al. (1996)</ref>.</p><p>Machine Learning (ML) is an area of artificial intelligence concerned with the development of techniques which allow computers to "learn" by the analysis of data sets. The focus of most machine learning methods is on symbolic data. ML is also concerned with the algorithmic complexity of computational implementations. Mitchell presents many of the commonly used ML methods in <ref type="bibr" target="#b67">Mitchell (1997)</ref>.</p><p>Statistics has its grounds in mathematics and deals with the science and practice for the analysis of empirical data. It is based on statistical theory which is a branch of applied mathematics. Within statistical theory, randomness and uncertainty are modelled by probability theory. Today many methods of statistics are used in the field of KDD. Good overviews are given in <ref type="bibr" target="#b31">Hastie et al. (2001)</ref>; <ref type="bibr">Berthold &amp; Hand (1999)</ref>; <ref type="bibr" target="#b60">Maitra (2002)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Definition of Text Mining</head><p>Text mining or knowledge discovery from text (KDT) -for the first time mentioned in <ref type="bibr" target="#b21">Feldman &amp; Dagan (1995)</ref> -deals with the machine supported analysis of text. It uses techniques from information retrieval, information extraction as well as natural language processing (NLP) and connects them with the algorithms and methods of KDD, data mining, machine learning and statistics. Thus, one selects a similar procedure as with the KDD process, whereby not data in general, but text documents are in focus of the analysis. From this, new questions for the used data mining methods arise. One problem is that we now have to deal with problems of -from the data modelling perspective -unstructured data sets.</p><p>If we try to define text mining, we can refer to related research areas. For each of them, we can give a different definition of text mining, which is motivated by the specific perspective of the area:</p><p>Text Mining = Information Extraction. The first approach assumes that text mining essentially corresponds to information extraction (cf. section 3.3)the extraction of facts from texts.</p><p>Text Mining = Text Data Mining. Text mining can be also defined -similar to data mining -as the application of algorithms and methods from the fields machine learning and statistics to texts with the goal of finding useful patterns. For this purpose it is necessary to pre-process the texts accordingly. Many authors use information extraction methods, natural language processing or some simple preprocessing steps in order to extract data from texts. To the extracted data then data mining algorithms can be applied (see <ref type="bibr" target="#b68">Nahm &amp; Mooney (2002)</ref>; <ref type="bibr" target="#b26">Gaizauskas (2003)</ref>).</p><p>Text Mining = KDD Process. Following the knowledge discovery process model (crispdm and CRISP99 1999), we frequently find in literature text mining as a process with a series of partial steps, among other things also information extraction as well as the use of data mining or statistical procedures. Hearst summarizes this in <ref type="bibr" target="#b33">Hearst (1999)</ref> in a general manner as the extraction of not yet discovered information in large collections of texts. Also <ref type="bibr" target="#b45">Kodratoff (1999)</ref> and Gomez in <ref type="bibr" target="#b36">Hidalgo (2002)</ref> consider text mining as process orientated approach on texts.</p><p>In this article, we consider text mining mainly as text data mining. Thus, our focus is on methods that extract useful patterns from texts in order to, e.g., categorize or structure text collections or to extract useful information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Related Research Areas</head><p>Current research in the area of text mining tackles problems of text representation, classification, clustering, information extraction or the search for and modelling of hidden patterns. In this context the selection of characteristics and also the influence of domain knowledge and domain-specific procedures plays an important role. Therefore, an adaptation of the known data mining algorithms to text data is usually necessary. In order to achieve this, one frequently relies on the experience and results of research in information retrieval, natural language processing and information extraction. In all of these areas we also apply data mining methods and statistics to handle their specific tasks:</p><p>Information Retrieval (IR). Information retrieval is the finding of documents which contain answers to questions and not the finding of answers itself <ref type="bibr">(Hearst Band 20 -2005</ref><ref type="bibr">1999)</ref>. In order to achieve this goal statistical measures and methods are used for the automatic processing of text data and comparison to the given question.</p><p>Information retrieval in the broader sense deals with the entire range of information processing, from data retrieval to knowledge retrieval (see <ref type="bibr">Sparck-Jones &amp; Willett (1997)</ref> for an overview). Although, information retrieval is a relatively old research area where first attempts for automatic indexing where made in 1975 <ref type="bibr" target="#b82">(Salton et al. 1975)</ref>, it gained increased attention with the rise of the World Wide Web and the need for sophisticated search engines.</p><p>Even though, the definition of information retrieval is based on the idea of questions and answers, systems that retrieve documents based on keywords, i.e. systems that perform document retrieval like most search engines, are frequently also called information retrieval systems.</p><p>Natural Language Processing (NLP). The general goal of NLP is to achieve a better understanding of natural language by use of computers <ref type="bibr" target="#b45">(Kodratoff 1999)</ref>. Others include also the employment of simple and durable techniques for the fast processing of text, as they are presented e.g. in <ref type="bibr" target="#b0">Abney (1991)</ref>. The range of the assigned techniques reaches from the simple manipulation of strings to the automatic processing of natural language inquiries. In addition, linguistic analysis techniques are used among other things for the processing of text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Extraction (IE).</head><p>The goal of information extraction methods is the extraction of specific information from text documents. These are stored in data base-like patterns (see <ref type="bibr" target="#b95">Wilks (1997)</ref>) and are then available for further use. For further details see section 3.3.</p><p>In the following, we will frequently refer to the above mentioned related areas of research. We will especially provide examples for the use of machine learning methods in information extraction and information retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Text Encoding</head><p>For mining large document collections it is necessary to pre-process the text documents and store the information in a data structure, which is more appropriate for further processing than a plain text file. Even though, meanwhile several methods exist that try to exploit also the syntactic structure and semantics of text, most text mining approaches are based on the idea that a text document can be represented by a set of words, i.e. a text document is described based on the set of words contained in it (bag-of-words representation). However, in order to be able to define at least the importance of a word within a given document, usually a vector representation is used, where for each word a numerical "importance" value is stored. The currently predominant approaches based on this idea are the vector space model <ref type="bibr" target="#b82">(Salton et al. 1975)</ref>, the probabilistic model <ref type="bibr" target="#b77">(Robertson 1977</ref>) and the logical model <ref type="bibr" target="#b94">(van Rijsbergen 1986)</ref>.</p><p>In the following we briefly describe, how a bag-of-words representation can be obtained. Furthermore, we describe the vector space model and corresponding similarity measures in more detail, since this model will be used by several text mining approaches discussed in this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text Preprocessing</head><p>In order to obtain all words that are used in a given text, a tokenization process is required, i.e. a text document is split into a stream of words by removing all punctuation marks and by replacing tabs and other non-text characters by single white spaces. This tokenized representation is then used for further processing. The set of different words obtained by merging all text documents of a collection is called the dictionary of a document collection.</p><p>In order to allow a more formal description of the algorithms, we define first some terms and variables that will be frequently used in the following: Let D be the set of documents and T = {t 1 , . . . , t m } be the dictionary, i.e. the set of all different terms occurring in D, then the absolute frequency of term t ∈ T in document d ∈ D is given by tf(d, t). We denote the term vectors t d = (tf(d, t 1 ), . . . , tf(d, t m )). Later on, we will also need the notion of the centroid of a set X of term vectors. It is defined as the mean value t X := 1 |X| ∑ t d ∈X t d of its term vectors. In the sequel, we will apply tf also on subsets of terms: For T ⊆ T, we let tf(d, T ) := ∑ t∈T tf(d, t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Filtering, Lemmatization and Stemming</head><p>In order to reduce the size of the dictionary and thus the dimensionality of the description of documents within the collection, the set of words describing the documents can be reduced by filtering and lemmatization or stemming methods.</p><p>Filtering methods remove words from the dictionary and thus from the documents. A standard filtering method is stop word filtering. The idea of stop word filtering is to remove words that bear little or no content information, like articles, conjunctions, prepositions, etc. Furthermore, words that occur extremely often can be said to be of little information content to distinguish between documents, and also words that occur very seldom are likely to be of no particular statistical relevance and can be removed from the dictionary <ref type="bibr" target="#b25">(Frakes &amp; Baeza-Yates 1992)</ref>. In order to further reduce the number of words in the dictionary, also (index) term selection methods can be used (see <ref type="bibr">Sect. 2.1.2)</ref>.</p><p>Lemmatization methods try to map verb forms to the infinite tense and nouns to the singular form. However, in order to achieve this, the word form has to be known, i.e. the part of speech of every word in the text document has to be assigned. Since this tagging process is usually quite time consuming and still error-prone, in practice frequently stemming methods are applied.</p><p>Stemming methods try to build the basic forms of words, i.e. strip the plural 's' from nouns, the 'ing' from verbs, or other affixes. A stem is a natural group of words with equal (or very similar) meaning. After the stemming process, every word is represented by its stem. A well-known rule based stemming algorithm has been originally proposed by Porter <ref type="bibr" target="#b73">(Porter 1980)</ref>. He defined a set of production rules to iteratively transform (English) words into their stems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Index Term Selection</head><p>To further decrease the number of words that should be used also indexing or keyword selection algorithms can be used (see, e.g. <ref type="bibr" target="#b14">Deerwester et al. (1990)</ref>; <ref type="bibr" target="#b97">Witten et al. (1999)</ref>). In this case, only the selected keywords are used to describe the documents. A simple method for keyword selection is to extract keywords based on their entropy. E.g. for each word t in the vocabulary the entropy as defined by <ref type="bibr" target="#b58">Lochbaum &amp; Streeter (1989)</ref> can be computed:</p><formula xml:id="formula_0">W(t) = 1 + 1 log 2 |D| ∑ d∈D P(d, t) log 2 P(d, t) with P(d, t) = tf(d, t) ∑ n l=1 tf(d l , t) (1)</formula><p>Here the entropy gives a measure how well a word is suited to separate documents by keyword search. For instance, words that occur in many documents will have low entropy. The entropy can be seen as a measure of the importance of a word in the given domain context. As index words a number of words that have a high entropy relative to their overall frequency can be chosen, i.e. of words occurring equally often those with the higher entropy can be preferred.</p><p>In order to obtain a fixed number of index terms that appropriately cover the documents, a simple greedy strategy can be applied: From the first document in the collection select the term with the highest relative entropy (or information gain as described in Sect. 3.1.1) as an index term. Then mark this document and all other documents containing this term. From the first of the remaining unmarked documents select again the term with the highest relative entropy as an index term. Then mark again this document and all other documents containing this term. Repeat this process until all documents are marked, then unmark them all and start again. The process can be terminated when the desired number of index terms have been selected. A more detailed discussion of the benefits of this approach for clustering -with respect to reduction of words required in order to obtain a good clustering performance -can be found in <ref type="bibr" target="#b7">Borgelt &amp; Nürnberger (2004)</ref>.</p><p>An index term selection methods that is more appropriate if we have to learn a classifier for documents is discussed in Sect. 3.1.1. This approach also considers the word distributions within the classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Vector Space Model</head><p>Despite of its simple data structure without using any explicit semantic information, the vector space model enables very efficient analysis of huge document collections. It was originally introduced for indexing and information retrieval <ref type="bibr" target="#b82">(Salton et al. 1975)</ref> but is now used also in several text mining approaches as well as in most of the currently available document retrieval systems.</p><p>The vector space model represents documents as vectors in m-dimensional space, i.e. each document d is described by a numerical feature vector w(d) = (x(d, t 1 ), . . . , x(d, t m )). Thus, documents can be compared by use of simple vector operations and even queries can be performed by encoding the query terms similar to the documents in a query vector. The query vector can then be compared to each document and a result list can be obtained by ordering the documents according to the computed similarity <ref type="bibr" target="#b80">(Salton et al. 1994)</ref>. The main task of the vector space representation of documents is to find an appropriate encoding of the feature vector.</p><p>Each element of the vector usually represents a word (or a group of words) of the document collection, i.e. the size of the vector is defined by the number of words (or groups of words) of the complete document collection. The simplest way of document encoding is to use binary term vectors, i.e. a vector element is set to one if the corresponding word is used in the document and to zero if the word is not. This encoding will result in a simple Boolean comparison or search if a query is encoded in a vector. Using Boolean encoding the importance of all terms for a specific query or comparison is considered as similar. To improve the performance usually term weighting schemes are used, where the weights reflect the importance of a word in a specific document of the considered collection. Large weights are assigned to terms that are used frequently in relevant documents but rarely in the whole document collection <ref type="bibr" target="#b81">(Salton &amp; Buckley 1988)</ref>. Thus a weight w(d, t) for a term t in document d is computed by term frequency tf(d, t) times inverse document frequency idf(t), which describes the term specificity within the document collection. In <ref type="bibr" target="#b80">Salton et al. (1994)</ref> a weighting scheme was proposed that has meanwhile proven its usability in practice. Besides term frequency and inverse document frequency -defined as id f (t) := log(N/n t ) -, a length normalization factor is used to ensure that all documents have equal chances of being retrieved independent of their lengths:</p><formula xml:id="formula_1">w(d, t) = tf(d, t) log(N/n t ) ∑ m j=1 t f (d, t j ) 2 (log(N/n t j )) 2 , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where N is the size of the document collection D and n t is the number of documents in D that contain term t.</p><p>Based on a weighting scheme a document d is defined by a vector of term weights w(d) = (w(d, t 1 ), . . . , w(d, t m )) and the similarity S of two documents d 1 and d 2 (or the similarity of a document and a query vector) can be computed based on the inner product of the vectors (by which -if we assume normalized vectors -the cosine between the two document vectors is computed), i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S(d</head><formula xml:id="formula_3">1 , d 2 ) = ∑ m k=1 w(d 1 , t k ) • w(d 2 , t k ).</formula><p>( 3)</p><p>A frequently used distance measure is the Euclidian distance. We calculate the distance between two text documents d 1 , d 2 ∈ D as follows:</p><formula xml:id="formula_4">dist(d 1 , d 2 ) = 2 ∑ m k=1 |w(d 1 , t k ) -w(d 2 , t k )| 2 . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>However, the Euclidean distance should only be used for normalized vectors, since otherwise the different lengths of documents can result in a smaller distance between documents that share less words than between documents that have more words in common and should be considered therefore as more similar.</p><p>Note that for normalized vectors the scalar product is not much different in behavior from the Euclidean distance, since for two vectors x and y it is</p><formula xml:id="formula_6">cos ϕ = x y | x| • | y| = 1 - 1 2 d 2 x | x| , y | y| .</formula><p>For a more detailed discussion of the vector space model and weighting schemes see, e.g. Baeza-Yates &amp; Ribeiro-Neto (1999); Greiff (1998); <ref type="bibr" target="#b81">Salton &amp; Buckley (1988)</ref>; <ref type="bibr" target="#b82">Salton et al. (1975)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Linguistic Preprocessing</head><p>Often text mining methods may be applied without further preprocessing. Sometimes, however, additional linguistic preprocessing (c.f. <ref type="bibr" target="#b62">Manning &amp; Schütze (2001)</ref>) may be used to enhance the available information about terms. For this, the following approaches are frequently applied:</p><p>Part-of-speech tagging (POS) determines the part of speech tag, e.g. noun, verb, adjective, etc. for each term.</p><p>Text chunking aims at grouping adjacent words in a sentence. An example of a chunk is the noun phrase "the current account deficit".</p><p>Word Sense Disambiguation (WSD) tries to resolve the ambiguity in the meaning of single words or phrases. An example is 'bank' which may have -among others -the senses 'financial institution' or the 'border of a river or lake'. Thus, instead of terms the specific meanings could be stored in the vector space representation. This leads to a bigger dictionary but considers the semantic of a term in the representation.</p><p>Parsing produces a full parse tree of a sentence. From the parse, we can find the relation of each word in the sentence to all the others, and typically also its function in the sentence (e.g. subject, object, etc.).</p><p>Linguistic processing either uses lexica and other resources as well as handcrafted rules. If a set of examples is available machine learning methods as described in section 3, especially in section 3.3, may be employed to learn the desired tags.</p><p>It turned out, however, that for many text mining tasks linguistic preprocessing is of limited value compared to the simple bag-of-words approach with basic preprocessing. The reason is that the co-occurrence of terms in the vector representation serves as an automatic disambiguation, e.g. for classification <ref type="bibr" target="#b54">(Leopold &amp; Kindermann 2002)</ref>. Recently some progress was made by enhancing bag of words with linguistic feature for text clustering and classification <ref type="bibr" target="#b40">(Hotho et al. 2003;</ref><ref type="bibr" target="#b6">Bloehdorn &amp; Hotho 2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Mining Methods for Text</head><p>One main reason for applying data mining methods to text document collections is to structure them. A structure can significantly simplify the access to a document collection for a user. Well known access structures are library catalogues or book indexes. However, the problem of manual designed indexes is the time required to maintain them. Therefore, they are very often not up-to-date and thus not usable for recent publications or frequently changing information sources like the World Wide Web. The existing methods for structuring collections either try to assign keywords to documents based on a given keyword set (classification or categorization methods) or automatically structure document collections to find groups of similar documents (clustering methods). In the following we first describe both of these approaches. Furthermore, we discuss in Sect. 3.3 methods to automatically extract useful information patterns from text document collections. In Sect. 3.4 we review methods for visual text mining. These methods allow in combination with structuring methods the development of powerful tools for the interactive exploration of document collections. We conclude this section with a brief discussion of further application areas for text mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Classification</head><p>Text classification aims at assigning pre-defined classes to text documents <ref type="bibr" target="#b67">(Mitchell 1997)</ref>. An example would be to automatically label each incoming news story with a topic like "sports", "politics", or "art". Whatever the specific method employed, a data mining classification task starts with a training set D = (d 1 , . . . , d n ) of documents that are already labelled with a class L ∈ L (e.g. sport, politics). The task is then to determine a classification model</p><formula xml:id="formula_7">f : D → L f (d) = L (5)</formula><p>which is able to assign the correct class to a new document d of the domain.</p><p>To measure the performance of a classification model a random fraction of the labelled documents is set aside and not used for training. We may classify the documents of this test set with the classification model and compare the estimated labels with the true labels. The fraction of correctly classified documents in relation to the total number of documents is called accuracy and is a first performance measure.</p><p>Often, however, the target class covers only a small percentage of the documents. Then we get a high accuracy if we assign each document to the alternative class. To avoid this effect different measures of classification success are often used. Precision quantifies the fraction of retrieved documents that are in fact relevant, i.e. belong to the target class. Recall indicates which fraction of the relevant documents is retrieved.</p><formula xml:id="formula_8">precision = #{relevant ∩ retrieved} #retrieved recall = #{relevant ∩ retrieved} #relevant (6)</formula><p>Obviously there is a trade off between precision and recall. Most classifiers internally determine some "degree of membership" in the target class. If only documents of high degree are assigned to the target class, the precision is high. However, many relevant documents might have been overlooked, which corresponds to a low recall. When on the other hand the search is more exhaustive, recall increases and precision goes down. The F-score is a compromise of both for measuring the overall performance of classifiers.</p><formula xml:id="formula_9">F = 2 1/recall + 1/precision (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Index Term Selection</head><p>As document collections often contain more than 100,000 different words we may select the most informative ones for a specific classification task to reduce the number of words and thus the complexity of the classification problem at hand. One commonly used ranking score is the information gain which for a term t j is defined as</p><formula xml:id="formula_10">IG(t j ) = 2 ∑ c=1 p(L c ) log 2 1 p(L c ) - 1 ∑ m=0 p(t j =m) 2 ∑ c=1 p(L c |t j =m) log 2 1 p(L c |t j =m) (8)</formula><p>Here p(L c ) is the fraction of training documents with classes L 1 and L 2 , p(t j =1) and p(t j =0) is the number of documents with / without term t j and p(L c |t j =m) is the conditional probability of classes L 1 and L 2 if term t j is contained in the document or is missing. It measures how useful t j is for predicting L 1 from an information-theoretic point of view. We may determine IG(t j ) for all terms and remove those with very low information gain from the dictionary.</p><p>In the following sections we describe the most frequently used data mining methods for text categorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Naïve Bayes Classifier</head><p>Probabilistic classifiers start with the assumption that the words of a document d i have been generated by a probabilistic mechanism. It is supposed that the class L(d i ) of document d i has some relation to the words which appear in the document. This may be described by the conditional distribution p(t 1 , . . . , t n i |L(d i )) of the n i words given the class. Then the Bayesian formula yields the probability of a class given the words of a document <ref type="bibr" target="#b67">(Mitchell 1997</ref>)</p><formula xml:id="formula_11">p(L c |t 1 , . . . , t n i ) = p(t 1 , . . . , t n i |L c )p(L c ) ∑ L∈L p(t 1 , . . . , t n i |L)p(L)</formula><p>Note that each document is assumed to belong to exactly one of the k classes in L. The prior probability p(L) denotes the probability that an arbitrary document belongs to class L before its words are known. Often the prior probabilities of all classes may be taken to be equal. The conditional probability on the left is the desired posterior probability that the document with words t 1 , . . . , t n i belongs to class L c . We may assign the class with highest posterior probability to our document.</p><p>For document classification it turned out that the specific order of the words in a document is not very important. Even more we may assume that for documents of a given class a word appears in the document irrespective of the presence of other words. This leads to a simple formula for the conditional probability of words given a class L c p(t 1 , . . . ,</p><formula xml:id="formula_12">t n i |L c ) = n i ∏ j=1 p(t j |L c )</formula><p>Combining this "naïve" independence assumption with the Bayes formula defines the Naïve Bayes classifier <ref type="bibr" target="#b28">(Good 1965)</ref>. Simplifications of this sort are required as many thousand different words occur in a corpus.</p><p>The naïve Bayes classifier involves a learning step which simply requires the estimation of the probabilities of words p(t j |L c ) in each class by its relative frequencies in the documents of a training set which are labelled with L c . In the classification step the estimated probabilities are used to classify a new instance according to the Bayes rule. In order to reduce the number of probabilities p(t j |L m ) to be estimated, we can use index term selection methods as discussed above in Sect. 3.1.1.</p><p>Although this model is unrealistic due to its restrictive independence assumption it yields surprisingly good classifications <ref type="bibr" target="#b19">(Dumais et al. 1998;</ref><ref type="bibr" target="#b41">Joachims 1998)</ref>. It may be extended into several directions <ref type="bibr" target="#b85">(Sebastiani 2002)</ref>.</p><p>As the effort for manually labeling the documents of the training set is high, some authors use unlabeled documents for training. Assume that from a small training set it has been established that word t i is highly correlated with class L c . If from unlabeled documents it may be determined that word t j is highly correlated with t i , then also t j is a good predictor for class L c . In this way unlabeled documents may improve classification performance. In <ref type="bibr" target="#b69">Nigam et al. (2000)</ref> the authors used a combination of Expectation-Maximization (EM, <ref type="bibr" target="#b15">Dempster et al. (1977)</ref>) and a naïve Bayes classifier and were able to reduce the classification error by up to 30%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Nearest Neighbor Classifier</head><p>Instead of building explicit models for the different classes we may select documents from the training set which are "similar" to the target document. The class of the target document subsequently may be inferred from the class labels of these similar documents. If k similar documents are considered, the approach is also known as k-nearest neighbor classification.</p><p>There is a large number of similarity measures used in text mining. One possibility is simply to count the number of common words in two documents. Obviously this has to be normalized to account for documents of different lengths. On the other hand words have greatly varying information content. A standard way to measure the latter is the cosine similarity as defined in (3). Note that only a small fraction of all possible terms appear in this sums as w(d, t) = 0 if the term t is not present in the document d. Other similarity measures are discussed in <ref type="bibr" target="#b2">Baeza-Yates &amp; Ribeiro-Neto (1999)</ref>.</p><p>For deciding whether document d i belongs to class L m , the similarity S(d i , d j ) to all documents d j in the training set is determined. The k most similar training documents (neighbors) are selected. The proportion of neighbors having the same class may be taken as an estimator for the probability of that class, and the class with the largest proportion is assigned to document d i . The optimal number k of neighbors may be estimated from additional training data by cross-validation.</p><p>Nearest neighbor classification is a nonparametric method and it can be shown that for large data sets the error rate of the 1-nearest neighbor classifier is never larger than twice the optimal error rate <ref type="bibr" target="#b31">(Hastie et al. 2001)</ref>. Several studies have shown that k-nearest neighbor methods have very good performance in practice <ref type="bibr" target="#b41">(Joachims 1998)</ref>. Their drawback is the computational effort during classification, where basically the similarity of a document with respect to all other documents of a training set has to be determined. Some extensions are discussed in <ref type="bibr" target="#b85">Sebastiani (2002)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Decision Trees</head><p>Decision trees are classifiers which consist of a set of rules which are applied in a sequential way and finally yield a decision. They can be best explained by observing the training process, which starts with a comprehensive training set. It uses a divide and conquer strategy: For a training set M with labelled documents the word t i is selected, which can predict the class of the documents in the best way, e.g. by the information gain (8). Then M is partitioned into two subsets, the subset M + i with the documents containing t i , and the subset M - i with the documents without t i . This procedure is recursively applied to M + i and M - i . It stops if all documents in a subset belong to the same class L c . It generates a tree of rules with an assignment to actual classes in the leaves.</p><p>Decision trees are a standard tool in data mining <ref type="bibr" target="#b74">(Quinlan 1986;</ref><ref type="bibr" target="#b67">Mitchell 1997)</ref>. They are fast and scalable both in the number of variables and the size of the training set. For text mining, however, they have the drawback that the final decision depends only on relatively few terms. A decisive improvement may be achieved by boosting decision trees <ref type="bibr" target="#b83">(Schapire &amp; Singer 1999)</ref>, i.e. determining a set of complementary decision trees constructed in such a way that the overall error is reduced. <ref type="bibr" target="#b84">Schapire &amp; Singer (2000)</ref> use even simpler one step decision trees containing only one rule and get impressive results for text classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Support Vector Machines and Kernel Methods</head><p>A Support Vector Machine (SVM) is a supervised classification algorithm that recently has been applied successfully to text classification tasks <ref type="bibr" target="#b41">(Joachims 1998;</ref><ref type="bibr" target="#b19">Dumais et al. 1998;</ref><ref type="bibr" target="#b54">Leopold &amp; Kindermann 2002)</ref>. As usual a document d is represented by a -possibly weighted -vector (t d1 , . . . , t dN ) of the counts of its words. A single SVM can only separate two classes -a positive class L 1 (indicated by y = +1) and a negative class L 2 (indicated by y = -1). In the space of input vectors a hyperplane may be defined by setting y = 0 in the following linear equation.</p><formula xml:id="formula_13">y = f ( t d ) = b 0 + N ∑ j=1 b j t dj</formula><p>The SVM algorithm determines a hyperplane which is located between the positive and negative examples of the training set. The parameters b j are adapted in such a way that the distance ξ -called margin -between the hyperplane and the closest positive and negative example documents is maximized, as shown in Fig. <ref type="figure" target="#fig_3">3</ref>.1.5. This amounts to a constrained quadratic optimization problem which can be solved efficiently for a large number of input vectors. The documents having distance ξ from the hyperplane are called support vectors and determine the actual location of the hyperplane. Usually only a small fraction of documents are support vectors. A new document with term vector t d is classified in L 1 if the value f ( t d ) &gt; 0 and into L 2 otherwise. In case that the document vectors of the two classes are not linearly separable a hyperplane is selected such that as few as possible document vectors are located on the "wrong" side.</p><p>SVMs can be used with non-linear predictors by transforming the usual input features in a non-linear way, e.g. by defining a feature map</p><formula xml:id="formula_14">φ(t 1 , . . . , t N ) = t 1 , . . . , t N , t 2 1 , t 1 t 2 , . . . , t N t N-1 , t 2 N</formula><p>Subsequently a hyperplane may be defined in the expanded input space. Obviously such non-linear transformations may be defined in a large number of ways.</p><p>The most important property of SVMs is that learning is nearly independent of the dimensionality of the feature space. It rarely requires feature selection as it inherently selects data points (the support vectors) required for a good classification. This allows good generalization even in the presence of a large number of features and makes SVM especially suitable for the classification of texts <ref type="bibr" target="#b41">(Joachims 1998</ref>). In the case of textual data the choice of the kernel function has a minimal effect on the accuracy of classification: Kernels that imply a high dimensional feature space show slightly better results in terms of precision and recall, but they are subject to overfitting <ref type="bibr" target="#b54">(Leopold &amp; Kindermann 2002)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.6">Classifier Evaluations</head><p>During the last years text classifiers have been evaluated on a number of benchmark document collections. It turns out that the level of performance of course depends on the document collection. Table <ref type="table" target="#tab_0">1</ref> gives some representative results achieved for the Reuters 20 newsgroups collection <ref type="bibr">(Sebastiani 2002, p.38)</ref>. Concerning the relative quality of classifiers boosted trees, SVMs, and k-nearest neighbors usually deliver top-notch performance, while naïve Bayes and decision trees are less reliable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Clustering</head><p>Clustering method can be used in order to find groups of documents with similar content. The result of clustering is typically a partition (also called) clustering P, a set of clusters P. Each cluster consists of a number of documents d. Objects -in our case documents -of a cluster should be similar and dissimilar to documents of other clusters. Usually the quality of clusterings is considered better if the contents of the documents within one cluster are more similar and between the clusters more dissimilar. Clustering methods group the documents only by considering their distribution in document space (for example, a n-dimensional space if we use the vector space model for text documents).</p><p>Clustering algorithms compute the clusters based on the attributes of the data and measures of (dis)similarity. However, the idea of what an ideal clustering result should look like varies between applications and might be even different between users. One can exert influence on the results of a clustering algorithm by using only subsets of attributes or by adapting the used similarity measures and thus control the clustering process. To which extent the result of the cluster algorithm coincides with the ideas of the user can be assessed by evaluation measures. A survey of different kinds of clustering algorithms and the resulting cluster types can be found in <ref type="bibr" target="#b90">Steinbach et al. (2003)</ref>.</p><p>In the following, we first introduce standard evaluation methods and present then details for hierarchical clustering approaches, k-means, bi-section-k-means, self-organizing maps and the EM-algorithm. We will finish the clustering section with a short overview of other clustering approaches used for text clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Evaluation of Clustering Results</head><p>In general, there are two ways to evaluate clustering results. One the one hand statistical measures can be used to describe the properties of a clustering result. On the other hand some given classification can be seen as a kind of gold standard which is then typically used to compare the clustering results with the given classification. We discuss both aspects in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Measures</head><p>In the following, we first discuss measures which cannot make use of a given classification L of the documents. They are called indices in statistical literature and evaluate the quality of a clustering on the basis of statistic connections. One finds a large number of indices in literature (see <ref type="bibr" target="#b22">Fickel (1997)</ref>; <ref type="bibr" target="#b18">Duda &amp; Hart (1973)</ref>). One of the most well-known measures is the mean square error. It permits to make statements on quality of the found clusters dependent on the number of clusters. Unfortunately, the computed quality is always better if the number of cluster is higher. In <ref type="bibr" target="#b42">Kaufman &amp; Rousseeuw (1990)</ref> an alternative measure, the silhouette coefficient, is presented which is independent of the number of clusters. We introduce both measures in the following.</p><p>Mean square error If one keeps the number of dimensions and the number of clusters constant the mean square error (Mean Square error, MSE) can be used Band 20 -2005 likewise for the evaluation of the quality of clustering. The mean square error is a measure for the compactness of the clustering and is defined as follows:</p><p>Definition 1 (MSE) The means square error (MSE) for a given clustering P is defined as</p><formula xml:id="formula_15">MSE(P) = ∑ P∈P MSE(P), (<label>9</label></formula><formula xml:id="formula_16">)</formula><p>whereas the means square error for a cluster P is given by:</p><formula xml:id="formula_17">MSE(P) = ∑ d∈P dist(d, µ P ) 2 , (<label>10</label></formula><formula xml:id="formula_18">)</formula><p>and µ P = 1 |P| ∑ d∈P t d is the centroid of the clusters P and dist is a distance measure.</p><p>Silhouette Coefficient One clustering measure that is independent from the number of clusters is the silhouette coefficient SC(P) (cf. <ref type="bibr" target="#b42">Kaufman &amp; Rousseeuw (1990)</ref>). The main idea of the coefficient is to find out the location of a document in the space with respect to the cluster of the document and the next similar cluster. For a good clustering the considered document is nearby the own cluster whereas for a bad clustering the document is closer to the next cluster. With the help of the silhouette coefficient one is able to judge the quality of a cluster or the entire clustering (details can be found in <ref type="bibr" target="#b42">Kaufman &amp; Rousseeuw (1990)</ref>). <ref type="bibr" target="#b42">Kaufman &amp; Rousseeuw (1990)</ref> gives characteristic values of the silhouette coefficient for the evaluation of the cluster quality. A value for SC(P) between 0.7 and 1.0 signals excellent separation between the found clusters, i.e. the objects within a cluster are very close to each other and are far away from other clusters.</p><p>The structure was very well identified by the cluster algorithm. For the range from 0.5 to 0.7 the objects are clearly assigned to the appropriate clusters. A larger level of noise exists in the data set if the silhouette coefficient is within the range of 0.25 to 0.5 whereby also here still clusters are identifiable. Many objects could not be assigned clearly to one cluster in this case due to the cluster algorithm. At values under 0.25 it is practically impossible to identify a cluster structure and to calculate meaningful (from the view of application) cluster centers. The cluster algorithm more or less "guessed" the clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparative Measures</head><p>The purity measure is based on the well-known precision measure for information retrieval (cf. <ref type="bibr" target="#b72">Pantel &amp; Lin (2002)</ref>). Each resulting cluster P from a partitioning P of the overall document set D is treated as if it were the result of a query. Each set L of documents of a partitioning L, which is obtained by manual labelling, is treated as if it is the desired set of documents for a query which leads to the same definitions for precision, recall and f-score as defined in Equations 6 and 7. The two partitions P and L are then compared as follows.</p><p>The precision of a cluster P ∈ P for a given category L ∈ L is given by</p><formula xml:id="formula_19">Precision(P, L) := |P ∩ L| |P| . (<label>11</label></formula><formula xml:id="formula_20">)</formula><p>The overall value for purity is computed by taking the weighted average of maximal precision values:</p><formula xml:id="formula_21">Purity(P, L) := ∑ P∈P |P| |D| max L∈L Precision(P, L). (<label>12</label></formula><formula xml:id="formula_22">)</formula><p>The counterpart of purity is:</p><formula xml:id="formula_23">InversePurity(P, L) := ∑ L∈L |L| |D| max P∈P Recall(P, L), (<label>13</label></formula><formula xml:id="formula_24">)</formula><p>where Recall(P, L) := Precision(L, P) and the well known </p><p>which is based on the F-score as defined in Eq. 7.</p><p>The three measures return values in the interval [0, 1], with 1 indicating optimal agreement. Purity measures the homogeneity of the resulting clusters when evaluated against a pre-categorization, while inverse purity measures how stable the pre-defined categories are when split up into clusters. Thus, purity achieves an "optimal" value of 1 when the number of clusters k equals |D|, whereas inverse purity achieves an "optimal" value of 1 when k equals 1. Another name in the literature for inverse purity is microaveraged precision. The reader may note that, in the evaluation of clustering results, microaveraged precision is identical to microaveraged recall (cf. e.g. <ref type="bibr" target="#b85">Sebastiani (2002)</ref>). The Fmeasure works similar as inverse purity, but it depreciates overly large clusters, as it includes the individual precision of these clusters into the evaluation.</p><p>While (inverse) purity and F-measure only consider 'best' matches between 'queries' and manually defined categories, the entropy indicates how large the information content uncertainty of a clustering result with respect to the given classification is</p><formula xml:id="formula_26">E(P, L) = ∑ P∈P prob(P) • E(P)</formula><p>, where ( <ref type="formula">15</ref>)</p><formula xml:id="formula_27">E(P) = -∑ L∈L prob(L|P) log(prob(L|P)) (<label>16</label></formula><formula xml:id="formula_28">)</formula><p>where prob(L|P) = Precision(P, L) and prob(P) = |P| |D| . The entropy has the range [0, log(|L|)], with 0 indicating optimality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Partitional Clustering</head><p>Hierarchical Clustering Algorithms <ref type="bibr" target="#b62">Manning &amp; Schütze (2001)</ref>; <ref type="bibr" target="#b91">Steinbach et al. (2000)</ref> got their name since they form a sequence of groupings or clusters that can be represented in a hierarchy of clusters. This hierarchy can be obtained either in a top-down or bottom-up fashion. Top-down means that we start with one cluster that contains all documents. This cluster is stepwise refined by splitting it iteratively into sub-clusters. One speaks in this case also of the so called "divisive" algorithm. The bottom-up or "agglomerative" procedures start by considering every document as individual cluster. Then the most similar clusters are iteratively merged, until all documents are contained in one single cluster. In practice the divisive procedure is almost of no importance due to its generally bad results. Therefore, only the agglomerative algorithm is outlined in the following.</p><p>The agglomerative procedure considers initially each document d of the the whole document set D as an individual cluster. It is the first cluster solution. It is assumed that each document is member of exactly one cluster. One determines the similarity between the clusters on the basis of this first clustering and selects the two clusters p, q of the clustering P with the minimum distance dist(p, q). Both cluster are merged and one receives a new clustering. One continues this procedure and re-calculates the distances between the new clusters in order to join again the two clusters with the minimum distance dist(p, q). The algorithm stops if only one cluster is remaining.</p><p>The distance can be computed according to Eq. 4. It is also possible to derive the clusters directly on the basis of the similarity relationship given by a matrix. For the computation of the similarity between clusters that contain more than one element different distance measures for clusters can be used, e.g. based on the outer cluster shape or the cluster center. Common linkage procedures that make use of different cluster distance measures are single linkage, average linkage or Ward's procedure. The obtained clustering depends on the used measure. Details can be found, for example, in <ref type="bibr" target="#b18">Duda &amp; Hart (1973)</ref>.</p><p>By means of so-called dendrograms one can represent the hierarchy of the clusters obtained as a result of the repeated merging of clusters as described above. The dendrograms allows to estimate the number of clusters based on the distances of the merged clusters. Unfortunately, the selection of the appropriate linkage method depends on the desired cluster structure, which is usually unknown in advance. For example, single linkage tends to follow chain-like clusters in the data, while complete linkage tends to create ellipsoid clusters. Thus prior knowledge about the expected distribution and cluster form is usually necessary for the selection of the appropriate method (see also <ref type="bibr" target="#b18">Duda &amp; Hart (1973)</ref>). However, substantially more problematic for the use of the algorithm for large data sets is the memory required to store the similarity matrix, which consists of n(n -1)/2 elements where n is the number of documents. Also the runtime behavior with O(n 2 ) is worse compared to the linear behavior of KMeans as discussed in the following.</p><p>k-means is one of the most frequently used clustering algorithms in practice in the field of data mining and statistics (see <ref type="bibr" target="#b18">Duda &amp; Hart (1973)</ref>; <ref type="bibr" target="#b30">Hartigan (1975)</ref>). The procedure which originally comes from statistics is simple to implement and can also be applied to large data sets. It turned out that especially in the field of text clustering k-means obtains good results. Proceeding from a starting solution in which all documents are distributed on a given number of clusters one tries to improve the solution by a specific change of the allocation of documents to the clusters. Meanwhile, a set of variants exists whereas the basic principle goes back to <ref type="bibr" target="#b23">Forgy (1965)</ref> or <ref type="bibr" target="#b59">MacQueen (1967)</ref>. In literature for vector quantization KMeans is also known under the name LloydMaxAlgorithm <ref type="bibr" target="#b27">(Gersho &amp; Gray 1992)</ref>. The basic principle is shown in the following algorithm:</p><p>k-means essentially consists of the steps three and four in the algorithm, whereby the number of clusters k must be given. In step three the documents are assigned to the nearest of the k centroids (also called cluster prototype).</p><p>Step four calculates a new centroids on the basis of the new allocations. We repeat the two steps in a loop (step five) until the cluster centroids do not change any more. The algorithm 5.1 corresponds to a simple hill climbing procedure which typically gets stuck in a local optimum (the finding of the global optimum is a NP complete problem). Apart from a suitable method to determine the starting solution (step one), we require a measure for calculating the distance or Algorithm 1 The KMeans algorithm Input: set D, distance measure dist, number k of cluster Output: A partitioning P of the set D of documents (i. e., a set P of k disjoint subsets of D with P∈P P = D).</p><p>1: Choose randomly k data points from D as starting centroids t P 1 . . . t P k .</p><p>2: repeat 3:</p><p>Assign each point of P to the closest centroid with respect to dist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>(Re-)calculate the cluster centroids t P 1 . . . t P k of clusters P 1 . . . P k . 5: until cluster centroids t P 1 . . . t P k are stable 6: return set P := {P 1 , . . . , P k }, of clusters. similarity in step three (cf. section 2.1). Furthermore the abort criterion of the loop in step five can be chosen differently e.g. by stopping after a fix number of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-Section-k-means</head><p>One fast text clustering algorithm, which is also able to deal with the large size of the textual data is the Bi-Section-KMeans algorithm.</p><p>In <ref type="bibr" target="#b91">Steinbach et al. (2000)</ref> it was shown that Bi-Section-KMeans is a fast and high-quality clustering algorithm for text documents which is frequently outperforming standard KMeans as well as agglomerative clustering techniques.</p><p>Bi-Section-KMeans is based on the KMeans algorithm. It repeatedly splits the largest cluster (using KMeans) until the desired number of clusters is obtained. Another way of choosing the next cluster to be split is picking the one with the largest variance. <ref type="bibr" target="#b91">Steinbach et al. (2000)</ref> showed neither of these two has a significant advantage.</p><p>Self Organizing Map (SOM, cf. <ref type="bibr" target="#b46">Kohonen (1982)</ref>) are a special architecture of neural networks that cluster high-dimensional data vectors according to a similarity measure. The clusters are arranged in a low-dimensional topology that preserves the neighborhood relations in the high dimensional data. Thus, not only objects that are assigned to one cluster are similar to each other (as in every cluster analysis), but also objects of nearby clusters are expected to be more similar than objects in more distant clusters. Usually, two-dimensional grids of squares or hexagons are used (cf. Fig. <ref type="figure" target="#fig_3">3</ref>).</p><p>The network structure of a self-organizing map has two layers (see Fig. <ref type="figure" target="#fig_3">3</ref>). The neurons in the input layer correspond to the input dimensions, here the words of the document vector. The output layer (map) contains as many neurons as clusters needed. All neurons in the input layer are connected with all neurons in the output layer. The weights of the connection between input and output layer of the neural network encode positions in the high-dimensional data space (similar to the cluster prototypes in k-means). Thus, every unit in the output layer represents a cluster center. Before the learning phase of the network, the two-dimensional structure of the output units is fixed and the weights are initialized randomly. During learning, the sample vectors (defining the documents) are repeatedly propagated through the network. The weights of the most similar prototype w s (winner neuron) are modified such that the prototype moves toward the input vector w i , which is defined by the currently considered document d, i.e. w i := t d (competitive learning). As similarity measure usually the Euclidean distance is used. However, for text documents the scalar product (see Eq. 3) can be applied. The weights w s of the winner neuron are modified according to the following equation:</p><formula xml:id="formula_29">w s = w s + σ • ( w s -w i ),</formula><p>where σ is a learning rate.</p><p>To preserve the neighborhood relations, prototypes that are close to the winner neuron in the two-dimensional structure are also moved in the same direction. The weight change decreases with the distance from the winner neuron. Therefore, the adaption method is extended by a neighborhood function v (see also Fig. <ref type="figure" target="#fig_3">3</ref>):</p><formula xml:id="formula_30">w s = w s + v(i, s) • σ • ( w s -w i ),</formula><p>where σ is a learning rate. By this learning procedure, the structure in the highdimensional sample data is non-linearly projected to the lower-dimensional topology. After learning, arbitrary vectors (i.e. vectors from the sample set or prior 'unknown' vectors) can be propagated through the network and are mapped to the output units. For further details on self-organizing maps see <ref type="bibr" target="#b47">Kohonen (1984)</ref>. Examples for the application of SOMs for text mining can be found in <ref type="bibr" target="#b56">Lin et al. (1991)</ref>; <ref type="bibr" target="#b39">Honkela et al. (1996)</ref>; <ref type="bibr" target="#b48">Kohonen et al. (2000)</ref>; Nürnberger d i with probability q ic to P c (soft clustering), where q i = (q i1 , . . . , q ik ) is a probability vector ∑ k c=1 q ic = 1. The underlying statistical assumption is that a document was created in two stages: First we pick a cluster P c from {1, . . . , k} with fixed probability q c ; then we generate the words t of the document according to a cluster-specific probability distribution p(t|P c ). This corresponds to a mixture model where the probability of an observed document (t 1 , . . . , t n i ) is</p><formula xml:id="formula_31">p(t 1 , . . . , t n i ) = k ∑ c=1 q c p(t 1 , . . . , t n i |P c )<label>(17)</label></formula><p>Each cluster P c is a mixture component. The mixture probabilities q c describe an unobservable "cluster variable" z which may take the values from {1, . . . , k}. A well established method for estimating models involving unobserved variables is the EM-algorithm <ref type="bibr" target="#b31">(Hastie et al. 2001)</ref>, which basically replaces the unknown value with its current probability estimate and then proceeds as if it has been observed. Clustering methods for documents based on mixture models have been proposed by <ref type="bibr" target="#b9">Cheeseman &amp; Stutz (1996)</ref> and yield excellent results. Hofmann ( <ref type="formula">2001</ref>) formulates a variant that is able to cluster terms occurring together instead of documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Alternative Clustering Approaches</head><p>Co-clustering algorithm designate the simultaneous clustering of documents and terms <ref type="bibr" target="#b16">(Dhillon et al. 2003)</ref>. They follow thereby another paradigm than the "classical" cluster algorithm as KMeans which only clusters elements of the one dimension on the basis of their similarity to the second one, e.g. documents based on terms.</p><p>Fuzzy Clustering While most classical clustering algorithms assign each datum to exactly one cluster, thus forming a crisp partition of the given data, fuzzy clustering allows for degrees of membership, to which a datum belongs to different clusters <ref type="bibr" target="#b4">(Bezdek 1981)</ref>. These approaches are frequently more stable. Applications to text are described in, e.g., <ref type="bibr" target="#b64">Mendes &amp; Sacks (2001)</ref>; <ref type="bibr" target="#b7">Borgelt &amp; Nürnberger (2004)</ref>.</p><p>The Utility of Clustering We have described the most important types of clustering approaches, but we had to leave out many other. Obviously there are many ways to define clusters and because of this we cannot expect to obtain something like the 'true' clustering. Still clustering can be insightful. In contrast to classification, which relies on a prespecified grouping, cluster procedures label documents in a new way. By studying the words and phrases that characterize a cluster, for example, a company could learn new insights about its customers and their typical properties. A comparison of some clustering methods is given in <ref type="bibr" target="#b91">Steinbach et al. (2000)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Information Extraction</head><p>Natural language text contains much information that is not directly suitable for automatic analysis by a computer. However, computers can be used to sift through large amounts of text and extract useful information from single words, phrases or passages. Therefore information extraction can be regarded as a restricted form of full natural language understanding, where we know in advance what kind of semantic information we are looking for. The main task is to extract parts of text and assign specific attributes to it.</p><p>As an example consider the task to extract executive position changes from news stories: "Robert L. James, chairman and chief executive officer of McCann-Erickson, is going to retire on July 1st. He will be replaced by John J. Donner, Jr., the agencies chief operating officer." In this case we have to identify the following information: Organization (McCann-Erickson), position (chief executive officer), date (July 1), outgoing person name (Robert L. James), and incoming person name (John J. Donner, Jr.).</p><p>The task of information extraction naturally decomposes into a series of processing steps, typically including tokenization, sentence segmentation, part-of-speech assignment, and the identification of named entities, i.e. person names, location names and names of organizations. At a higher level phrases and sentences have to be parsed, semantically interpreted and integrated. Finally the required pieces of information like "position" and "incoming person name" are entered into the database. Although the most accurate information extraction systems often involve handcrafted language-processing modules, substantial progress has been made in applying data mining techniques to a number of these steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Classification for Information Extraction</head><p>Entity extraction was originally formulated in the Message Understanding Conference <ref type="bibr" target="#b11">(Chinchor 1997)</ref>. One can regard it as a word-based tagging problem: The word, where the entity starts, get tag "B", continuation words get tag "I" and words outside the entity get tag "O". This is done for each type of entity of interest. For the example above we have for instance the person-words "by (O) John (B) J. (I) Donner (I) Jr. (I) the (O)".</p><p>Hence we have a sequential classification problem for the labels of each word, with the surrounding words as input feature vector. A frequent way of forming the feature vector is a binary encoding scheme. Each feature component can be considered as a test that asserts whether a certain pattern occurs at a specific position or not. For example, a feature component takes the value 1 if the previous word is the word "John" and 0 otherwise. Of course we may not only test the presence of specific words but also whether the words starts with a capital letter, has a specific suffix or is a specific part-of-speech. In this way results of previous analysis may be used. Now we may employ any efficient classification method to classify the word labels using the input feature vector. A good candidate is the Support Vector Machine because of its ability to handle large sparse feature vectors efficiently. <ref type="bibr" target="#b92">Takeuchi &amp; Collier (2002)</ref> used it to extract entities in the molecular biology domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Hidden Markov Models</head><p>One problem of standard classification approaches is that they do not take into account the predicted labels of the surrounding words. This can be done using probabilistic models of sequences of labels and features. Frequently used is the hidden Markov model (HMM), which is based on the conditional distributions of current labels L (j) given the previous label L (j-1) and the distribution of the current word t (j) given the current and the previous labels L (j) , L (j-1) . L (j) ∼ p(L (j) |L (j-1) ) t (j) ∼ p(t (j) |L (j) , L (j-1) ) (18)</p><p>A training set of words and their correct labels is required. For the observed words the algorithm takes into account all possible sequences of labels and computes their probabilities. An efficient learning method that exploits the sequential structure is the Viterbi algorithm <ref type="bibr" target="#b75">(Rabiner 1989)</ref>. Hidden Markov models were successfully used for named entity extraction, e.g. in the Identifinder system <ref type="bibr" target="#b5">(Bikel et al. 1999)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Conditional Random Fields</head><p>Hidden Markov models require the conditional independence of features of different words given the labels. This is quite restrictive as we would like to include features which correspond to several words simultaneously. A recent approach for modelling this type of data is called conditional random field (CRF, cf. <ref type="bibr" target="#b52">Lafferty et al. (2001)</ref>). Again we consider the observed vector of words t and the corresponding vector of labels L. The labels have a graph structure. For a label L c let N(c) be the indices of neighboring labels. Then (t, L) is a conditional random field when conditioned on the vector t of all terms the random variables obey the Markov property</p><formula xml:id="formula_32">p(L c |t, L d ; d = c) = p(L c |t, L d ; d ∈ N(c))<label>(19)</label></formula><p>i.e. the whole vector t of observed terms and the labels of neighbors may influence the distribution of the label L c . Note that we do not model the distribution p(t) of the observed words, which may exhibit arbitrary dependencies.</p><p>We consider the simple case that the words t = (t 1 , t 2 , . . . , t n ) and the corresponding labels L 1 , L 2 , . . . , L n have a chain structure and that L c depends only on the preceding and succeeding labels L c-1 and L c+1 . Then the conditional distribution p(L|t) has the form</p><formula xml:id="formula_33">p(L|t) = 1 const exp ⎛ ⎝ n ∑ j=1 k j ∑ r=1 λ jr f jr (L j , t) + n-1 ∑ j=1 m j ∑ r=1 µ jr g jr (L j , L j-1 , t) ⎞ ⎠ (<label>20</label></formula><formula xml:id="formula_34">)</formula><p>where f jr (L j , t) and g jr (L j , L j-1 , t) are different features functions related to L j and the pair L j , L j-1 respectively. CRF models encompass hidden Markov models, but they are much more expressive because they allow arbitrary dependencies in the observation sequence and more complex neighborhood structures of labels. As for most machine learning algorithms a training sample of words and the correct labels is required. In addition to the identity of words arbitrary properties of the words, like part-of-speech tags, capitalization, prefixes and suffixes, etc. may be used leading to sometimes more than a million features. The unknown parameter values λ jr and µ jr are usually estimated using conjugate gradient optimization routines <ref type="bibr" target="#b63">(McCallum 2003)</ref>. <ref type="bibr" target="#b63">McCallum (2003)</ref> applies CRFs with feature selection to named entity recognition and reports the following F1-measures for the CoNLL corpus: person names 93%, location names 92%, organization names 84%, miscellaneous names 80%. CRFs also have been successfully applied to noun phrase identification <ref type="bibr" target="#b63">(McCallum 2003)</ref>, part-of-speech tagging <ref type="bibr" target="#b52">(Lafferty et al. 2001)</ref>, shallow parsing <ref type="bibr" target="#b86">(Sha &amp; Pereira 2003)</ref>, and biological entity recognition <ref type="bibr" target="#b44">(Kim et al. 2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Explorative Text Mining: Visualization Methods</head><p>Graphical visualization of information frequently provides more comprehensive and better and faster understandable information than it is possible by pure text based descriptions and thus helps to mine large document collections. Many of the approaches developed for text mining purposes are motivated by methods that had been proposed in the areas of explorative data analysis, information visualization and visual data mining. For an overview of these areas of research see, e.g., U. <ref type="bibr" target="#b93">Fayyad (2001)</ref>; <ref type="bibr" target="#b43">Keim (2002)</ref>. In the following we will focus on methods that have been specifically designed for text mining oras a subgroup of text mining methods and a typical application of visualization methods -information retrieval.</p><p>In text mining or information retrieval systems visualization methods can improve and simplify the discovery or extraction of relevant patterns or information. Information that allow a visual representation comprises aspects of the document collection or result sets, keyword relations, ontologies or -if retrieval systems are considered -aspects of the search process itself, e.g. the search or navigation path in hyperlinked collections.</p><p>However, especially for text collections we have the problem of finding an appropriate visualization for abstract textual information. Furthermore, an interactive visual data exploration interface is usually desirable, e.g. to zoom in local areas or to select or mark parts for further processing. This results in great demands on the user interface and the hardware. In the following we give a brief overview of visualization methods that have been realized for text mining and information retrieval systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Visualizing Relations and Result Sets</head><p>Interesting approaches to visualize keyword-document relations are, e.g., the Cat-a-Cone model <ref type="bibr" target="#b34">(Hearst &amp; Karadi 1997)</ref>, which visualizes in a three dimensional representation hierarchies of categories that can be interactively used to refine a search. The InfoCrystal <ref type="bibr" target="#b89">(Spoerri 1995)</ref> visualizes a (weighted) boolean query and the belonging result set in a crystal structure. The Lyberworld model <ref type="bibr" target="#b35">(Hemmje et al. 1994</ref>) and the components of the SENTINEL Model <ref type="bibr" target="#b24">(Fox et al. 1999)</ref> are representing documents in an abstract keyword space.</p><p>An approach to visualize the results of a set of queries was presented in <ref type="bibr" target="#b32">Havre et al. (2001)</ref>. Here, retrieved documents are arranged according to their similarity to a query on straight lines. These lines are arranged in a circle around a common center, i.e. every query is represented by a single line. If several documents are placed on the same (discrete) position, they are arranged in the same distance to the circle, but with a slight offset. Thus, clusters occur that represent the distribution of documents for the belonging query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Visualizing Document Collections</head><p>For the visualization of document collections usually two-dimensional projections are used, i.e. the high dimensional document space is mapped on a two-dimensional surface. In order to depict individual documents or groups of documents usually text flags are used, which represent either a keyword or the document category. Colors are frequently used to visualize the density, e.g. the number of documents in this area, or the difference to neighboring documents, e.g. in order to emphasize borders between different categories. If three-dimensional projections are used, for example, the number of documents assigned to a specific area can be represented by the z-coordinate.</p><p>An Example: Visualization Using Self-Organizing Maps Visualization of document collections requires methods that are able to group documents based on their similarity and furthermore that visualize the similarity between discovered groups of documents. Clustering approaches that are frequently used to find groups of documents with similar content <ref type="bibr" target="#b91">(Steinbach et al. 2000)</ref> -see also section 3.2 -usually do not consider the neighborhood relations between the obtained cluster centers. Self-organizing maps, as discussed above, are an alternative approach which is frequently used in data analysis to cluster high dimensional data. The resulting clusters are arranged in a low-dimensional topology that preserves the neighborhood relations of the corresponding high dimensional data vectors and thus not only objects that are assigned to one cluster are similar to each other, but also objects of nearby clusters are expected to be more similar than objects in more distant clusters.</p><p>Usually, two-dimensional arrangements of squares or hexagons are used for the definition of the neighborhood relations. Although other topologies are possible for self-organizing maps, two-dimensional maps have the advantage of intuitive visualization and thus good exploration possibilities. In document retrieval, self-organizing maps can be used to arrange documents based on their similarity. This approach opens up several appealing navigation possibilities. Most important, the surrounding grid cells of documents known to be interesting can be scanned for further similar documents. Furthermore, the distribution of keyword search results can be visualized by coloring the grid cells of the map with respect to the number of hits. This allows a user to judge e.g. whether the search results are assigned to a small number of (neighboring) grid cells of the map, or whether the search hits are spread widely over the map and thus the search was -most likely -too unspecific.</p><p>A first application of self-organizing maps in information retrieval was presented in <ref type="bibr" target="#b56">Lin et al. (1991)</ref>. It provided a simple two-dimensional cluster representation (categorization) of a small document collection. A refined model, the WEBSOM approach, extended this idea to a web based interface applied to newsgroup data that provides simple zooming techniques and coloring methods <ref type="bibr" target="#b39">(Honkela et al. 1996;</ref><ref type="bibr" target="#b38">Honkela 1997;</ref><ref type="bibr" target="#b48">Kohonen et al. 2000)</ref>. Further extensions introduced hierarchies <ref type="bibr" target="#b65">(Merkl 1998)</ref>, supported the visualization of search results <ref type="bibr" target="#b79">(Roussinov &amp; Chen 2001</ref>) and combined search, navigation and visualization techniques in an integrated tool <ref type="bibr" target="#b70">(Nürnberger 2001)</ref>. A screenshot of the prototype discussed in Nürnberger ( <ref type="formula">2001</ref>) is depicted in Fig. <ref type="figure" target="#fig_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Other Techniques</head><p>Besides methods based on self-organizing maps several other techniques have been successfully applied to visualize document collections. For example, the tool VxInsight <ref type="bibr" target="#b8">(Boyack et al. 2002)</ref> realizes a partially interactive mapping by an energy minimization approach similar to simulated annealing to construct a three dimensional landscape of the document collection. As input either a vector space description of the documents or a list of directional edges, e.g. defined based on citations of links, can be used. The tool SPIRE <ref type="bibr" target="#b96">(Wise et al. 1995)</ref>  applies a three step approach: It first clusters documents in document space, than projects the discovered cluster centers onto a two dimensional surface and finally maps the documents relative to the projected cluster centers. SPIRE offers a scatter plot like projection as well as a three dimensional visualization. The visualization tool SCI-Map <ref type="bibr" target="#b87">(Small 1999)</ref> applies an iterative clustering approach to create a network using, e.g., references of scientific publications. The tools visualizes the structure by a map hierarchy with an increasing number of details.</p><p>One major problem of most existing visualization approaches is that they create their output only by use of data inherent information, i.e. the distribution of the documents in document space. User specific information can not be integrated in order to obtain, e.g., an improved separation of the documents with respect to user defined criteria like keywords or phrases. Furthermore, the possibilities for a user to interact with the system in order to navigate or search are usually very limited, e.g., to boolean keyword searches and simple result lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Further Application Areas</head><p>Further major applications of text mining methods consider the detection of topics in text streams and text summarization.</p><p>Topic detection studies the problem of detecting new and upcoming topics in time-ordered document collections. The methods are frequently used in order to detect and monitor (topic tracking) news tickers or news broadcasts. An introduction and overview of current approaches can be found in <ref type="bibr" target="#b1">Allan (2002)</ref>.</p><p>Text summarization aims at the creation of a condensed version of a document or a document collection (multidocument summarization) that should contain its most important topics. Most approaches still focus on the idea to extract individual informative sentences from a text. The summary consists then simply of a collection of these sentences. However, recently refined approaches try to extract semantic information from documents and create summaries based on this information (cf. <ref type="bibr" target="#b55">Leskovec et al. (2004)</ref>). For an overview see <ref type="bibr">Mani &amp; Maybury (1999)</ref> and <ref type="bibr" target="#b76">Radev et al. (2002)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Applications</head><p>In this section we briefly discuss successful applications of text mining methods in quite diverse areas as patent analysis, text classification in news agencies, bioinformatics and spam filtering. Each of the applications has specific char-acteristics that had to be considered while selecting appropriate text mining methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Patent Analysis</head><p>In recent years the analysis of patents developed to a large application area. The reasons for this are on the one hand the increased number of patent applications and on the other hand the progress that had been made in text classification, which allows to use these techniques in this due to the commercial impact quite sensitive area. Meanwhile, supervised and unsupervised techniques are applied to analyze patent documents and to support companies and also the European patent office in their work. The challenges in patent analysis consists of the length of the documents, which are larger then documents usually used in text classification, and the large number of available documents in a corpus <ref type="bibr" target="#b49">(Koster et al. 2001)</ref>. Usually every document consist of 5,000 words in average. More than 140,000 documents have to be handled by the European patent office (EPO) per year. They are processed by 2,500 patent examiners in three locations.</p><p>In several studies the classification quality of state-of-the-art methods was analyzed. <ref type="bibr" target="#b49">Koster et al. (2001)</ref> reported very good result with an 3% error rate for 16,000 full text documents to be classified in 16 classes (mono-classification) and a 6% error rate in the same setting for abstracts only by using the Winnow <ref type="bibr" target="#b57">(Littlestone 1988</ref>) and the Rocchio algorithm <ref type="bibr" target="#b78">(Rocchio 1971)</ref>. These results are possible due to the large amount of available training documents. Good results are also reported in <ref type="bibr" target="#b50">(Krier &amp; Zacca 2002)</ref> for an internal EPO text classification application with a precision of 81 % and an recall of 78 %.</p><p>Text clustering techniques for patent analysis are often applied to support the analysis of patents in large companies by structuring and visualizing the investigated corpus. Thus, these methods find their way in a lot of commercial products but are still also of interest for research, since there is still a need for improved performance. Companies like IBM offer products to support the analysis of patent text documents. Dorre describes in <ref type="bibr" target="#b17">(Dörre et al. 1999)</ref> the IBM Intelligent Miner for text in a scenario applied to patent text and compares it also to data mining and text mining. <ref type="bibr" target="#b12">Coupet &amp; Hehenberger (1998)</ref> do not only apply clustering but also give some nice visualization. A similar scenario on the basis of SOM is given in <ref type="bibr" target="#b53">(Lamirel et al. 2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Text Classification for News Agencies</head><p>In publishing houses a large number of news stories arrive each day. The users like to have these stories tagged with categories and the names of important persons, organizations and places. To automate this process the Deutsche Presse-Agentur (dpa) and a group of leading German broadcasters (PAN) wanted to select a commercial text classification system to support the annotation of news articles. Seven systems were tested with a two given test corpora of about half a million news stories and different categorical hierarchies of about 800 and 2,300 categories <ref type="bibr" target="#b71">(Paaß &amp; deVries 2005)</ref>. Due to confidentiality the results can be published only in anonymized form.</p><p>For the corpus with 2,300 categories the best system achieved at an F1-value of 39%, while for the corpus with 800 categories an F1-value of 79% was reached.</p><p>In the latter case a partially automatic assignment based on the reliability score was possible for about half the documents, while otherwise the systems could only deliver proposals for human categorizers. Especially good are the results for recovering persons and geographic locations with about 80% F1-value. In general there were great variations between the performances of the systems.</p><p>In a usability experiment with human annotators the formal evaluation results were confirmed leading to faster and more consistent annotation. It turned out, that with respect to categories the human annotators exhibit a relative large disagreement and a lower consistency than text mining systems. Hence the support of human annotators by text mining systems offers more consistent annotations in addition to faster annotation. The Deutsche Presse-Agentur now is routinely using a text mining system in its news production workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Bioinformatics</head><p>Bio-entity recognition aims to identify and classify technical terms in the domain of molecular biology that correspond to instances of concepts that are of interest to biologists. Examples of such entities include the names of proteins, genes and their locations of activity such as cells or organism names. Entity recognition is becoming increasingly important with the massive increase in reported results due to high throughput experimental methods. It can be used in several higher level information access tasks such as relation extraction, summarization and question answering.</p><p>Recently the GENIA corpus was provided as a benchmark data set to compare different entity extraction approaches <ref type="bibr" target="#b44">(Kim et al. 2004</ref>). It contains 2,000 abstracts from the MEDLINE database which were hand annotated with 36 types of biological entities. The following sentence is an example: "We have shown that &lt;protein&gt; interleukin-1 &lt;/protein&gt; (&lt;protein&gt; IL-1 &lt;/protein&gt;) and &lt;protein&gt; IL-2 &lt;/protein&gt; control &lt;DNA&gt; IL-2 receptor alpha (IL-2R alpha) gene &lt;/DNA&gt; transcription in &lt;cell_line&gt; CD4-CD8-murine T lymphocyte precursors &lt;/cell_line&gt;".</p><p>In the 2004 evaluation four types of extraction models were used: Support Vector Machines (SVMs), Hidden Markov Models (HMMs), Conditional Random Fields (CRFs) and the related Maximum Entropy Markov Models (MEMMs). Varying types of input features were employed: lexical features (words), n-grams, orthographic information, word lists, part-of-speech tags, noun phrase tags, etc. The evaluation shows that the best five systems yield an F1-value of about 70% <ref type="bibr" target="#b44">(Kim et al. 2004)</ref>. They use SVMs in combination with Markov models (72.6%), MEMMs (70.1%), CRFs (69.8%), CRFs together with SVMs (66.3%), and HMMs (64.8%). For practical applications the current accuracy levels are not yet satisfactory and research currently aims at including a sophisticated mix of external resources such as keyword lists and ontologies which provide terminological resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Anti-Spam Filtering of Emails</head><p>The explosive growth of unsolicited e-mail, more commonly known as spam, over the last years has been undermining constantly the usability of e-mail. One solution is offered by anti-spam filters. Most commercially available filters use black-lists and hand-crafted rules. On the other hand, the success of machine learning methods in text classification offers the possibility to arrive at anti-spam filters that quickly may be adapted to new types of spam.</p><p>There is a growing number of learning spam filters mostly using naive Bayes classifiers. A prominent example is Mozilla's e-mail client. <ref type="bibr" target="#b66">Michelakis et al. (2004)</ref> compare different classifier methods and investigate different costs of classifying a proper mail as spam. They find that for their benchmark corpora the SVM nearly always yields best results.</p><p>To explore how well a learning-based filter performs in real life, they used an SVM-based procedure for seven months without retraining. They achieved a precision of 96.5% and a recall of 89.3%. They conclude that these good results may be improved by careful preprocessing and the extension of filtering to different languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this article, we tried to give a brief introduction to the broad field of text mining. Therefore, we motivated this field of research, gave a more formal definition of the terms used herein and presented a brief overview of currently available text mining methods, their properties and their application to specific problems. Even though, it was impossible to describe all algorithms and applications in detail within the (size) limits of an article, we think that the ideas discussed and the provided references should give the interested reader a rough overview of this field and several starting points for further studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Phases of Crisp DM</figDesc><graphic coords="3,181.69,193.39,239.60,240.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hyperplane with maximal distance (margin) to examples of positive and negative classes constructed by the support vector machine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>F 2 •</head><label>2</label><figDesc>Recall(P, L) • Precision(P, L) Recall(P, L) + Precision(P, L) ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Network architecture of self-organizing maps (left) and possible neighborhood function v for increasing distances from s (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A Prototypical Retrieval System Based on Self-Organizing Maps</figDesc><graphic coords="33,471.62,89.57,56.71,634.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of Different Classifiers for the Reuters collection</figDesc><table><row><cell cols="2">Method F 1 -value</cell></row><row><cell>naïve Bayes</cell><cell>0.795</cell></row><row><cell>decision tree C4.5</cell><cell>0.794</cell></row><row><cell>k-nearest neighbor</cell><cell>0.856</cell></row><row><cell>SVM</cell><cell>0.870</cell></row><row><cell>boosted tree</cell><cell>0.878</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>The figure is taken from the Crisp-DM homepage, http://www.crisp-dm.org/Process/index. htm [accessed May 2005].20LDV-FORUM</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_1"><p>LDV-FORUM</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_2"><p>LDV-FORUM</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>Band 20 -2005</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>LDV-FORUM</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="32" xml:id="foot_5"><p>LDV-FORUM</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="52" xml:id="foot_6"><p>LDV-FORUM</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Parsing by chunks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Abney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principle-Based Parsing: Computation and Psycholinguistics</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Berwick</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Abney</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Tenny</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="257" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Topic Detection and Tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Norwell, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Modern Information Retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Addison Wesley Longman</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m">Intelligent data analysis</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Berthold</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, Inc</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bezdek</surname></persName>
		</author>
		<title level="m">Pattern Recognition with Fuzzy Objective Function Algorithms</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Plenum Press</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An algorithm that learns what&apos;s in a name</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Text classification by boosting weak learners based on terms and concepts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bloehdorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hotho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Data Mining (ICDM 04)</title>
		<meeting>IEEE Int. Conf. on Data Mining (ICDM 04)</meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="331" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast fuzzy clustering of web page collections</title>
		<author>
			<persName><forename type="first">C</forename><surname>Borgelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nürnberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of PKDD Workshop on Statistical Approaches for Web Mining (SAWM)</title>
		<meeting>of PKDD Workshop on Statistical Approaches for Web Mining (SAWM)<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain visualization using vxinsight for science and technology management</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Boyack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Wylie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technologie</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="764" to="774" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bayesian classification (AutoClass): Theory and results</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cheeseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stutz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Piatetsky-Shapiro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Uthurusamy</surname></persName>
		</editor>
		<imprint>
			<publisher>AAAI/MIT Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="153" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Data mining: an overview from a database perspective</title>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="866" to="883" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Muc-7 named entity task definition version 3.5</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chinchor</surname></persName>
		</author>
		<ptr target="ftp.muc.saic.com/pub/MUC/MUC7-guidelines" />
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>NIST</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Text mining applied to patent analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Coupet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hehenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of American Intellectual Property Law Association</title>
		<imprint>
			<publisher>AIPLA) Airlington</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cross industry standard process for data mining</title>
		<author>
			<persName><forename type="first">Crisp</forename></persName>
		</author>
		<ptr target="http://www.crisp-dm.org/" />
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page">99</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Landauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Sciences</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistic Society, Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Information-theoretic co-clustering</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ninth ACM SIGKDD int. conf. on Knowledge Discovery and Data Mining</title>
		<meeting>of the ninth ACM SIGKDD int. conf. on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Text mining: finding nuggets in mountains of textual data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dörre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gerstl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Seiffert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th ACM Int. Conf. on Knowledge Discovery and Data Mining (KDD-99)</title>
		<meeting>5th ACM Int. Conf. on Knowledge Discovery and Data Mining (KDD-99)<address><addrLine>San Diego, US; New York, US</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="398" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<title level="m">Pattern Classification and Scene Analysis</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>J. Wiley &amp; Sons</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inductive learning algorithms and representations for text categorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th Int. Conf. on Information and Knowledge Managment</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Knowledge discovery and data mining: Towards a unifying framework</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Piatetsky-Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="82" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kdt -knowledge discovery in texts</title>
		<author>
			<persName><forename type="first">R</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the First Int. Conf. on Knowledge Discovery (KDD)</title>
		<meeting>of the First Int. Conf. on Knowledge Discovery (KDD)</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="112" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Clusteranalyse mit gemischt-skalierten merkmalen: Abstrahierung vom skalenniveau</title>
		<author>
			<persName><forename type="first">N</forename><surname>Fickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Allg. Statistisches Archiv</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="265" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cluster analysis of multivariate data: Efficiency versus interpretability of classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Forgy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="768" to="769" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sentinel: A multiple engine information retrieval and visualization system</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Knepper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Snowberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="616" to="625" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Frakes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<title level="m">Information Retrieval: Data Structures &amp; Algorithms</title>
		<meeting><address><addrLine>New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An information extraction perspective on text mining: Tasks, technologies and prototype applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<ptr target="http://www.itri.bton.ac.uk/projects/euromap/TextMiningEvent/Rob_Gaizauskas.pdf" />
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="20" to="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Vector quantization and signal compression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The Estimation of Probabilities: An Essay on Modern Bayesian Methods</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Good</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965">1965</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A theory of term weighting based on exploratory data analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Greiff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Clustering Algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hartigan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>John Wiley and Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Interactive visualization of multiple query result</title>
		<author>
			<persName><forename type="first">S</forename><surname>Havre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hetzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Perrine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jurrus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Symposium on Information Visualization</title>
		<meeting>of IEEE Symposium on Information Visualization</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Untangling text data mining</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL&apos;99 the 37th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>of ACL&apos;99 the 37th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cat-a-cone: An interactive interface for specifying searches and viewing retrieval results using a large category hierarchie</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Karadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 20th Annual Int. ACM SIGIR Conference</title>
		<meeting>of the 20th Annual Int. ACM SIGIR Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="246" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lyberworld -a visualization user interface supporting fulltext retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hemmje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kunkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Willett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGIR</title>
		<meeting>of ACM SIGIR</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="254" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tutorial on text mining and internet content filtering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hidalgo</surname></persName>
		</author>
		<ptr target="http://ecmlpkdd.cs.helsinki./pdf/hidalgo.pdf" />
	</analytic>
	<monogr>
		<title level="s">Tutorial Notes Online</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised learning by probabilistic latent semantic analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Journal</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="196" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Self-Organizing Maps in Natural Language Processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Honkela</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<pubPlace>Espoo, Finland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Helsinki Univ. of Technology, Neural Networks Research Center</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Newsgroup exploration with the websom method and browsing interface, technical report</title>
		<author>
			<persName><forename type="first">T</forename><surname>Honkela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lagus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>Espoo, Finland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Helsinki University of Technology, Neural Networks Research Center</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ontologies improve text document clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hotho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Staab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stumme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Data Mining (ICDM 03)</title>
		<meeting>IEEE Int. Conf. on Data Mining (ICDM 03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="541" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Machine Learning (ECML)</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Nedellec</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Rouveirol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Finding groups in data: an introduction to cluster analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rousseeuw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Information visualization and visual data mining</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="100" to="107" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Introduction to the bioentity task at jnlpba</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on Natural Language Processing in Biomedicine and its Applications</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Collier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Ruch</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Nazarenko</surname></persName>
		</editor>
		<meeting>Workshop on Natural Language essing in Biomedicine and its Applications</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="70" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Knowledge discovery in texts: A definition and applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kodratoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">1609</biblScope>
			<biblScope unit="page" from="16" to="29" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Self-organized formation of topologically correct feature maps</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="59" to="69" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Self-Organization and Associative Memory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Self organization of a massive document collection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lagus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salojärvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Honkela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Paattero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saarela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="574" to="585" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Classifying patent applications with winnow</title>
		<author>
			<persName><forename type="first">C</forename><surname>Koster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Benelearn</title>
		<meeting>Benelearn<address><addrLine>Antwerpen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Automatic categorisation applications at the european patent office</title>
		<author>
			<persName><forename type="first">M</forename><surname>Krier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zacca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Patent Information</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="187" to="196" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">What is data mining?</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<ptr target="http://www-users.cs.umn.edu/~mjoshi/hpdmtut/sld004.htm" />
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Intelligent patent analysis through the use of a neural network: Experiment of multi-viewpoint analysis with the multisom model</title>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Lamirel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Al Shehabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Francois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-2003 Workshop on Patent Corpus Processing</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Text categorization with support vector machines. How to represent texts in input space?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Leopold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kindermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="423" to="444" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning sub-structures of document semantic graphs for document summarization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grobelnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Milic-Frayling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD 2004 Workshop on Link Analysis and Group Detection (LinkKDD)</title>
		<meeting><address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A selforganizing semantic map for information retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Marchionini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soergel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 14th International ACM/SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>of the 14th International ACM/SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="262" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm</title>
		<author>
			<persName><forename type="first">N</forename><surname>Littlestone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="285" to="318" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Combining and comparing the effectiveness of latent semantic indexing and the ordinary vector space model for information retrieval</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Lochbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Streeter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="665" to="676" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the fifth Berkeley Symposium Band 20 -2005 on Mathematical Statistics and Probability</title>
		<editor>
			<persName><forename type="first">Le</forename><surname>Cam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Neyman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename></persName>
		</editor>
		<meeting>of the fifth Berkeley Symposium Band 20 -2005 on Mathematical Statistics and Probability</meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A statistical perspective on data mining</title>
		<author>
			<persName><forename type="first">R</forename><surname>Maitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Ind. Soc. Prob. Statist</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Advances in Automatic Text Summarization</title>
		<editor>Mani, I. &amp; Maybury, M. T.</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<title level="m">Foundations of Statistical Natural Language Processing</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Efficiently inducing features of conditional random fields</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. on Uncertainty in Articifical Intelligence (UAI)</title>
		<meeting>Conf. on Uncertainty in Articifical Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Dynamic knowledge representation for e-learning applications</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sacks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of BISC International Workshop on Fuzzy Logic and the Internet (FLINT 2001)</title>
		<meeting>of BISC International Workshop on Fuzzy Logic and the Internet (FLINT 2001)<address><addrLine>Berkeley, USA. ERL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="176" to="181" />
		</imprint>
		<respStmt>
			<orgName>College of Engineering, University of California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Text classification with self-organizing maps: Some lessons learned</title>
		<author>
			<persName><forename type="first">D</forename><surname>Merkl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="61" to="77" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Filtron: A learning-based anti-spam filter</title>
		<author>
			<persName><forename type="first">E</forename><surname>Michelakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sakkis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stamatopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Conf. on Email and Anti-Spam (CEAS 2004)</title>
		<meeting>1st Conf. on Email and Anti-Spam (CEAS 2004)<address><addrLine>Mountain View, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m">Machine Learning</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Text mining with information extraction</title>
		<author>
			<persName><forename type="first">U</forename><surname>Nahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI 2002 Spring Symposium on Mining Answers from Texts and Knowledge Bases</title>
		<meeting>the AAAI 2002 Spring Symposium on Mining Answers from Texts and Knowledge Bases</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Text classification from labeled and unlabeled documents using em</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="103" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Interactive text retrieval supported by growing self-organizing maps</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nürnberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Workshop on Information Retrieval (IR 2001)</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</editor>
		<meeting>of the International Workshop on Information Retrieval (IR 2001)<address><addrLine>Oulu, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
	<note>Infotech</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Evaluating the performance of text mining systems on real-world press archives</title>
		<author>
			<persName><forename type="first">G</forename><surname>Paaß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Devries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th Annual Conference of the German Classification Society</title>
		<meeting>29th Annual Conference of the German Classification Society</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005. GfKl 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Document clustering with committees</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR&apos;02</title>
		<meeting>of SIGIR&apos;02<address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">An algorithm for suffix stripping</title>
		<author>
			<persName><forename type="first">M</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Program</publisher>
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A tutorial on hidden markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE</title>
		<meeting>of IEEE</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Introduction to the special issue on summarization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="408" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The probability ranking principle</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="294" to="304" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Relevance feedback in information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The SMART Retrieval System</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</editor>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Information navigation on the web by clustering and summarizing query results</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Roussinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="789" to="816" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Automatic structuring and retrieval of large text files</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="108" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Term weighting approaches in automatic text retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A vector space model for automatic indexing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
		<idno>also TR74-218</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975">1975</date>
			<pubPlace>NY, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Improved boosting using confidence-rated predictions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="336" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">BoosTexter: A boosting-based system for text categorization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<biblScope unit="page" from="135" to="168" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Machine learning in automated text categorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Shallow parsing with conditional random fields</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Human Language Technology NAACL</title>
		<meeting>Human Language Technology NAACL</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Visualizing science by citation mapping</title>
		<author>
			<persName><forename type="first">H</forename><surname>Small</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="799" to="813" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Readings in Information Retrieval</title>
		<editor>Sparck-Jones, K. &amp; Willett, P.</editor>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">InfoCrystal: A Visual Tool for Information Retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Spoerri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Challenges of clustering high dimensional data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ertoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New Vistas in Statistical Physics -Applications in Econophysics, Bioinformatics, and Pattern Recognition</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Wille</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A comparison of document clustering techniques</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumara</surname></persName>
		</author>
		<idno>also TR 00-034</idno>
	</analytic>
	<monogr>
		<title level="m">KDD Workshop on Text Mining</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>University of Minnesota, MN</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Use of support vector machines in extended named entity recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Collier</surname></persName>
		</author>
		<ptr target="http://www.textminingnews.com/" />
	</analytic>
	<monogr>
		<title level="m">6th Conf. on Natural Language Learning (CoNLL-02)</title>
		<imprint>
			<date type="published" when="2002">2002. 2005</date>
			<biblScope unit="page" from="20" to="2005" />
		</imprint>
	</monogr>
	<note>Text mining summit conference brochure</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Information Visualization in Data Mining and Knowledge Discovery</title>
		<author>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A non-classical logic for information retrieval</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="481" to="485" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Information extraction as a core language technology</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Extraction</title>
		<editor>
			<persName><forename type="first">M.-T</forename><surname>Pazienza</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Visualizing the non-visual: Spatial analysis and interaction with information from text documents</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Wise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pennock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lantrip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pottier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Crow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Symposium on Information Visualization &apos;95</title>
		<meeting>of IEEE Symposium on Information Visualization &apos;95</meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Managing Gigabytes: Compressing and Indexing Documents and Images</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
