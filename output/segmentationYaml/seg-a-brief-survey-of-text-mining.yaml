---
abstractSeg: "The enormous amount of information stored in unstructured texts cannot\
  \ simply be used for further processing by computers, which typically handle text\
  \ as simple sequences of character strings. Therefore, specific (pre-)processing\
  \ methods and algorithms are required in order to extract useful patterns. Text\
  \ mining refers generally to the process of extracting interesting information and\
  \ knowledge from unstructured text. In this article, we discuss text mining as a\
  \ young and interdisciplinary field in the intersection of the related areas information\
  \ retrieval, machine learning, statistics, computational linguistics and especially\
  \ data mining. We describe the main analysis tasks preprocessing, classification,\
  \ clustering, information extraction and visualization. In addition, we briefly\
  \ discuss a number of successful applications of text mining."
sectionList:
- header: "Introduction"
  content: "As computer networks become the backbones of science and economy enormous\
    \ quantities of machine readable documents become available. There are estimates\
    \ that 85% of business information lives in the form of text (TMS05 2005). Unfortunately,\
    \ the usual logic-based programming paradigm has great difficulties in capturing\
    \ the fuzzy and often ambiguous relations in text documents. Text mining aims\
    \ at disclosing the concealed information by means of methods which on the one\
    \ hand are able to cope with the large number of words and structures in natural\
    \ language and on the other hand allow to handle vagueness, uncertainty and fuzziness.In\
    \ this paper we describe text mining as a truly interdisciplinary method drawing\
    \ on information retrieval, machine learning, statistics, computational linguistics\
    \ and especially data mining. We first give a short sketch of these methods and\
    \ then define text mining in relation to them. Later sections survey state of\
    \ the art approaches for the main analysis tasks preprocessing, classification,\
    \ clustering, information extraction and visualization. The last section exemplifies\
    \ text mining in the context of a number of successful applications."
- header: "Text Encoding"
  content: "For mining large document collections it is necessary to pre-process the\
    \ text documents and store the information in a data structure, which is more\
    \ appropriate for further processing than a plain text file. Even though, meanwhile\
    \ several methods exist that try to exploit also the syntactic structure and semantics\
    \ of text, most text mining approaches are based on the idea that a text document\
    \ can be represented by a set of words, i.e. a text document is described based\
    \ on the set of words contained in it (bag-of-words representation). However,\
    \ in order to be able to define at least the importance of a word within a given\
    \ document, usually a vector representation is used, where for each word a numerical\
    \ \"importance\" value is stored. The currently predominant approaches based on\
    \ this idea are the vector space model (Salton et al. 1975), the probabilistic\
    \ model (Robertson 1977) and the logical model (van Rijsbergen 1986).In the following\
    \ we briefly describe, how a bag-of-words representation can be obtained. Furthermore,\
    \ we describe the vector space model and corresponding similarity measures in\
    \ more detail, since this model will be used by several text mining approaches\
    \ discussed in this article."
- header: "Data Mining Methods for Text"
  content: "Often text mining methods may be applied without further preprocessing.\
    \ Sometimes, however, additional linguistic preprocessing (c.f. Manning & Schü\
    tze (2001)) may be used to enhance the available information about terms. For\
    \ this, the following approaches are frequently applied:Part-of-speech tagging\
    \ (POS) determines the part of speech tag, e.g. noun, verb, adjective, etc. for\
    \ each term.Text chunking aims at grouping adjacent words in a sentence. An example\
    \ of a chunk is the noun phrase \"the current account deficit\".Word Sense Disambiguation\
    \ (WSD) tries to resolve the ambiguity in the meaning of single words or phrases.\
    \ An example is 'bank' which may have -among others -the senses 'financial institution'\
    \ or the 'border of a river or lake'. Thus, instead of terms the specific meanings\
    \ could be stored in the vector space representation. This leads to a bigger dictionary\
    \ but considers the semantic of a term in the representation.Parsing produces\
    \ a full parse tree of a sentence. From the parse, we can find the relation of\
    \ each word in the sentence to all the others, and typically also its function\
    \ in the sentence (e.g. subject, object, etc.).Linguistic processing either uses\
    \ lexica and other resources as well as handcrafted rules. If a set of examples\
    \ is available machine learning methods as described in section 3, especially\
    \ in section 3.3, may be employed to learn the desired tags.It turned out, however,\
    \ that for many text mining tasks linguistic preprocessing is of limited value\
    \ compared to the simple bag-of-words approach with basic preprocessing. The reason\
    \ is that the co-occurrence of terms in the vector representation serves as an\
    \ automatic disambiguation, e.g. for classification (Leopold & Kindermann 2002).\
    \ Recently some progress was made by enhancing bag of words with linguistic feature\
    \ for text clustering and classification (Hotho et al. 2003;Bloehdorn & Hotho\
    \ 2004).One main reason for applying data mining methods to text document collections\
    \ is to structure them. A structure can significantly simplify the access to a\
    \ document collection for a user. Well known access structures are library catalogues\
    \ or book indexes. However, the problem of manual designed indexes is the time\
    \ required to maintain them. Therefore, they are very often not up-to-date and\
    \ thus not usable for recent publications or frequently changing information sources\
    \ like the World Wide Web. The existing methods for structuring collections either\
    \ try to assign keywords to documents based on a given keyword set (classification\
    \ or categorization methods) or automatically structure document collections to\
    \ find groups of similar documents (clustering methods). In the following we first\
    \ describe both of these approaches. Furthermore, we discuss in Sect. 3.3 methods\
    \ to automatically extract useful information patterns from text document collections.\
    \ In Sect. 3.4 we review methods for visual text mining. These methods allow in\
    \ combination with structuring methods the development of powerful tools for the\
    \ interactive exploration of document collections. We conclude this section with\
    \ a brief discussion of further application areas for text mining."
- header: "Applications"
  content: "Co-clustering algorithm designate the simultaneous clustering of documents\
    \ and terms (Dhillon et al. 2003). They follow thereby another paradigm than the\
    \ \"classical\" cluster algorithm as KMeans which only clusters elements of the\
    \ one dimension on the basis of their similarity to the second one, e.g. documents\
    \ based on terms.Fuzzy Clustering While most classical clustering algorithms assign\
    \ each datum to exactly one cluster, thus forming a crisp partition of the given\
    \ data, fuzzy clustering allows for degrees of membership, to which a datum belongs\
    \ to different clusters (Bezdek 1981). These approaches are frequently more stable.\
    \ Applications to text are described in, e.g., Mendes & Sacks (2001); Borgelt\
    \ & Nürnberger (2004).The Utility of Clustering We have described the most important\
    \ types of clustering approaches, but we had to leave out many other. Obviously\
    \ there are many ways to define clusters and because of this we cannot expect\
    \ to obtain something like the 'true' clustering. Still clustering can be insightful.\
    \ In contrast to classification, which relies on a prespecified grouping, cluster\
    \ procedures label documents in a new way. By studying the words and phrases that\
    \ characterize a cluster, for example, a company could learn new insights about\
    \ its customers and their typical properties. A comparison of some clustering\
    \ methods is given in Steinbach et al. (2000).Natural language text contains much\
    \ information that is not directly suitable for automatic analysis by a computer.\
    \ However, computers can be used to sift through large amounts of text and extract\
    \ useful information from single words, phrases or passages. Therefore information\
    \ extraction can be regarded as a restricted form of full natural language understanding,\
    \ where we know in advance what kind of semantic information we are looking for.\
    \ The main task is to extract parts of text and assign specific attributes to\
    \ it.As an example consider the task to extract executive position changes from\
    \ news stories: \"Robert L. James, chairman and chief executive officer of McCann-Erickson,\
    \ is going to retire on July 1st. He will be replaced by John J. Donner, Jr.,\
    \ the agencies chief operating officer.\" In this case we have to identify the\
    \ following information: Organization (McCann-Erickson), position (chief executive\
    \ officer), date (July 1), outgoing person name (Robert L. James), and incoming\
    \ person name (John J. Donner, Jr.).The task of information extraction naturally\
    \ decomposes into a series of processing steps, typically including tokenization,\
    \ sentence segmentation, part-of-speech assignment, and the identification of\
    \ named entities, i.e. person names, location names and names of organizations.\
    \ At a higher level phrases and sentences have to be parsed, semantically interpreted\
    \ and integrated. Finally the required pieces of information like \"position\"\
    \ and \"incoming person name\" are entered into the database. Although the most\
    \ accurate information extraction systems often involve handcrafted language-processing\
    \ modules, substantial progress has been made in applying data mining techniques\
    \ to a number of these steps.Entity extraction was originally formulated in the\
    \ Message Understanding Conference (Chinchor 1997). One can regard it as a word-based\
    \ tagging problem: The word, where the entity starts, get tag \"B\", continuation\
    \ words get tag \"I\" and words outside the entity get tag \"O\". This is done\
    \ for each type of entity of interest. For the example above we have for instance\
    \ the person-words \"by (O) John (B) J. (I) Donner (I) Jr. (I) the (O)\".Hence\
    \ we have a sequential classification problem for the labels of each word, with\
    \ the surrounding words as input feature vector. A frequent way of forming the\
    \ feature vector is a binary encoding scheme. Each feature component can be considered\
    \ as a test that asserts whether a certain pattern occurs at a specific position\
    \ or not. For example, a feature component takes the value 1 if the previous word\
    \ is the word \"John\" and 0 otherwise. Of course we may not only test the presence\
    \ of specific words but also whether the words starts with a capital letter, has\
    \ a specific suffix or is a specific part-of-speech. In this way results of previous\
    \ analysis may be used. Now we may employ any efficient classification method\
    \ to classify the word labels using the input feature vector. A good candidate\
    \ is the Support Vector Machine because of its ability to handle large sparse\
    \ feature vectors efficiently. Takeuchi & Collier (2002) used it to extract entities\
    \ in the molecular biology domain.One problem of standard classification approaches\
    \ is that they do not take into account the predicted labels of the surrounding\
    \ words. This can be done using probabilistic models of sequences of labels and\
    \ features. Frequently used is the hidden Markov model (HMM), which is based on\
    \ the conditional distributions of current labels L (j) given the previous label\
    \ L (j-1) and the distribution of the current word t (j) given the current and\
    \ the previous labels L (j) , L (j-1) . L (j) ∼ p(L (j) |L (j-1) ) t (j) ∼ p(t\
    \ (j) |L (j) , L (j-1) ) (18)A training set of words and their correct labels\
    \ is required. For the observed words the algorithm takes into account all possible\
    \ sequences of labels and computes their probabilities. An efficient learning\
    \ method that exploits the sequential structure is the Viterbi algorithm (Rabiner\
    \ 1989). Hidden Markov models were successfully used for named entity extraction,\
    \ e.g. in the Identifinder system (Bikel et al. 1999).Hidden Markov models require\
    \ the conditional independence of features of different words given the labels.\
    \ This is quite restrictive as we would like to include features which correspond\
    \ to several words simultaneously. A recent approach for modelling this type of\
    \ data is called conditional random field (CRF, cf. Lafferty et al. (2001)). Again\
    \ we consider the observed vector of words t and the corresponding vector of labels\
    \ L. The labels have a graph structure. For a label L c let N(c) be the indices\
    \ of neighboring labels. Then (t, L) is a conditional random field when conditioned\
    \ on the vector t of all terms the random variables obey the Markov propertyi.e.\
    \ the whole vector t of observed terms and the labels of neighbors may influence\
    \ the distribution of the label L c . Note that we do not model the distribution\
    \ p(t) of the observed words, which may exhibit arbitrary dependencies.We consider\
    \ the simple case that the words t = (t 1 , t 2 , . . . , t n ) and the corresponding\
    \ labels L 1 , L 2 , . . . , L n have a chain structure and that L c depends only\
    \ on the preceding and succeeding labels L c-1 and L c+1 . Then the conditional\
    \ distribution p(L|t) has the formwhere f jr (L j , t) and g jr (L j , L j-1 ,\
    \ t) are different features functions related to L j and the pair L j , L j-1\
    \ respectively. CRF models encompass hidden Markov models, but they are much more\
    \ expressive because they allow arbitrary dependencies in the observation sequence\
    \ and more complex neighborhood structures of labels. As for most machine learning\
    \ algorithms a training sample of words and the correct labels is required. In\
    \ addition to the identity of words arbitrary properties of the words, like part-of-speech\
    \ tags, capitalization, prefixes and suffixes, etc. may be used leading to sometimes\
    \ more than a million features. The unknown parameter values λ jr and µ jr are\
    \ usually estimated using conjugate gradient optimization routines (McCallum 2003).\
    \ McCallum (2003) applies CRFs with feature selection to named entity recognition\
    \ and reports the following F1-measures for the CoNLL corpus: person names 93%,\
    \ location names 92%, organization names 84%, miscellaneous names 80%. CRFs also\
    \ have been successfully applied to noun phrase identification (McCallum 2003),\
    \ part-of-speech tagging (Lafferty et al. 2001), shallow parsing (Sha & Pereira\
    \ 2003), and biological entity recognition (Kim et al. 2004).Graphical visualization\
    \ of information frequently provides more comprehensive and better and faster\
    \ understandable information than it is possible by pure text based descriptions\
    \ and thus helps to mine large document collections. Many of the approaches developed\
    \ for text mining purposes are motivated by methods that had been proposed in\
    \ the areas of explorative data analysis, information visualization and visual\
    \ data mining. For an overview of these areas of research see, e.g., U. Fayyad\
    \ (2001); Keim (2002). In the following we will focus on methods that have been\
    \ specifically designed for text mining oras a subgroup of text mining methods\
    \ and a typical application of visualization methods -information retrieval.In\
    \ text mining or information retrieval systems visualization methods can improve\
    \ and simplify the discovery or extraction of relevant patterns or information.\
    \ Information that allow a visual representation comprises aspects of the document\
    \ collection or result sets, keyword relations, ontologies or -if retrieval systems\
    \ are considered -aspects of the search process itself, e.g. the search or navigation\
    \ path in hyperlinked collections.However, especially for text collections we\
    \ have the problem of finding an appropriate visualization for abstract textual\
    \ information. Furthermore, an interactive visual data exploration interface is\
    \ usually desirable, e.g. to zoom in local areas or to select or mark parts for\
    \ further processing. This results in great demands on the user interface and\
    \ the hardware. In the following we give a brief overview of visualization methods\
    \ that have been realized for text mining and information retrieval systems.Interesting\
    \ approaches to visualize keyword-document relations are, e.g., the Cat-a-Cone\
    \ model (Hearst & Karadi 1997), which visualizes in a three dimensional representation\
    \ hierarchies of categories that can be interactively used to refine a search.\
    \ The InfoCrystal (Spoerri 1995) visualizes a (weighted) boolean query and the\
    \ belonging result set in a crystal structure. The Lyberworld model (Hemmje et\
    \ al. 1994) and the components of the SENTINEL Model (Fox et al. 1999) are representing\
    \ documents in an abstract keyword space.An approach to visualize the results\
    \ of a set of queries was presented in Havre et al. (2001). Here, retrieved documents\
    \ are arranged according to their similarity to a query on straight lines. These\
    \ lines are arranged in a circle around a common center, i.e. every query is represented\
    \ by a single line. If several documents are placed on the same (discrete) position,\
    \ they are arranged in the same distance to the circle, but with a slight offset.\
    \ Thus, clusters occur that represent the distribution of documents for the belonging\
    \ query.For the visualization of document collections usually two-dimensional\
    \ projections are used, i.e. the high dimensional document space is mapped on\
    \ a two-dimensional surface. In order to depict individual documents or groups\
    \ of documents usually text flags are used, which represent either a keyword or\
    \ the document category. Colors are frequently used to visualize the density,\
    \ e.g. the number of documents in this area, or the difference to neighboring\
    \ documents, e.g. in order to emphasize borders between different categories.\
    \ If three-dimensional projections are used, for example, the number of documents\
    \ assigned to a specific area can be represented by the z-coordinate.An Example:\
    \ Visualization Using Self-Organizing Maps Visualization of document collections\
    \ requires methods that are able to group documents based on their similarity\
    \ and furthermore that visualize the similarity between discovered groups of documents.\
    \ Clustering approaches that are frequently used to find groups of documents with\
    \ similar content (Steinbach et al. 2000) -see also section 3.2 -usually do not\
    \ consider the neighborhood relations between the obtained cluster centers. Self-organizing\
    \ maps, as discussed above, are an alternative approach which is frequently used\
    \ in data analysis to cluster high dimensional data. The resulting clusters are\
    \ arranged in a low-dimensional topology that preserves the neighborhood relations\
    \ of the corresponding high dimensional data vectors and thus not only objects\
    \ that are assigned to one cluster are similar to each other, but also objects\
    \ of nearby clusters are expected to be more similar than objects in more distant\
    \ clusters.Usually, two-dimensional arrangements of squares or hexagons are used\
    \ for the definition of the neighborhood relations. Although other topologies\
    \ are possible for self-organizing maps, two-dimensional maps have the advantage\
    \ of intuitive visualization and thus good exploration possibilities. In document\
    \ retrieval, self-organizing maps can be used to arrange documents based on their\
    \ similarity. This approach opens up several appealing navigation possibilities.\
    \ Most important, the surrounding grid cells of documents known to be interesting\
    \ can be scanned for further similar documents. Furthermore, the distribution\
    \ of keyword search results can be visualized by coloring the grid cells of the\
    \ map with respect to the number of hits. This allows a user to judge e.g. whether\
    \ the search results are assigned to a small number of (neighboring) grid cells\
    \ of the map, or whether the search hits are spread widely over the map and thus\
    \ the search was -most likely -too unspecific.A first application of self-organizing\
    \ maps in information retrieval was presented in Lin et al. (1991). It provided\
    \ a simple two-dimensional cluster representation (categorization) of a small\
    \ document collection. A refined model, the WEBSOM approach, extended this idea\
    \ to a web based interface applied to newsgroup data that provides simple zooming\
    \ techniques and coloring methods (Honkela et al. 1996;Honkela 1997;Kohonen et\
    \ al. 2000). Further extensions introduced hierarchies (Merkl 1998), supported\
    \ the visualization of search results (Roussinov & Chen 2001) and combined search,\
    \ navigation and visualization techniques in an integrated tool (Nürnberger 2001).\
    \ A screenshot of the prototype discussed in Nürnberger ( 2001) is depicted in\
    \ Fig. 4.Besides methods based on self-organizing maps several other techniques\
    \ have been successfully applied to visualize document collections. For example,\
    \ the tool VxInsight (Boyack et al. 2002) realizes a partially interactive mapping\
    \ by an energy minimization approach similar to simulated annealing to construct\
    \ a three dimensional landscape of the document collection. As input either a\
    \ vector space description of the documents or a list of directional edges, e.g.\
    \ defined based on citations of links, can be used. The tool SPIRE (Wise et al.\
    \ 1995)  applies a three step approach: It first clusters documents in document\
    \ space, than projects the discovered cluster centers onto a two dimensional surface\
    \ and finally maps the documents relative to the projected cluster centers. SPIRE\
    \ offers a scatter plot like projection as well as a three dimensional visualization.\
    \ The visualization tool SCI-Map (Small 1999) applies an iterative clustering\
    \ approach to create a network using, e.g., references of scientific publications.\
    \ The tools visualizes the structure by a map hierarchy with an increasing number\
    \ of details.One major problem of most existing visualization approaches is that\
    \ they create their output only by use of data inherent information, i.e. the\
    \ distribution of the documents in document space. User specific information can\
    \ not be integrated in order to obtain, e.g., an improved separation of the documents\
    \ with respect to user defined criteria like keywords or phrases. Furthermore,\
    \ the possibilities for a user to interact with the system in order to navigate\
    \ or search are usually very limited, e.g., to boolean keyword searches and simple\
    \ result lists.Further major applications of text mining methods consider the\
    \ detection of topics in text streams and text summarization.Topic detection studies\
    \ the problem of detecting new and upcoming topics in time-ordered document collections.\
    \ The methods are frequently used in order to detect and monitor (topic tracking)\
    \ news tickers or news broadcasts. An introduction and overview of current approaches\
    \ can be found in Allan (2002).Text summarization aims at the creation of a condensed\
    \ version of a document or a document collection (multidocument summarization)\
    \ that should contain its most important topics. Most approaches still focus on\
    \ the idea to extract individual informative sentences from a text. The summary\
    \ consists then simply of a collection of these sentences. However, recently refined\
    \ approaches try to extract semantic information from documents and create summaries\
    \ based on this information (cf. Leskovec et al. (2004)). For an overview see\
    \ Mani & Maybury (1999) and Radev et al. (2002).In this section we briefly discuss\
    \ successful applications of text mining methods in quite diverse areas as patent\
    \ analysis, text classification in news agencies, bioinformatics and spam filtering.\
    \ Each of the applications has specific char-acteristics that had to be considered\
    \ while selecting appropriate text mining methods."
- header: "Conclusion"
  content: "In recent years the analysis of patents developed to a large application\
    \ area. The reasons for this are on the one hand the increased number of patent\
    \ applications and on the other hand the progress that had been made in text classification,\
    \ which allows to use these techniques in this due to the commercial impact quite\
    \ sensitive area. Meanwhile, supervised and unsupervised techniques are applied\
    \ to analyze patent documents and to support companies and also the European patent\
    \ office in their work. The challenges in patent analysis consists of the length\
    \ of the documents, which are larger then documents usually used in text classification,\
    \ and the large number of available documents in a corpus (Koster et al. 2001).\
    \ Usually every document consist of 5,000 words in average. More than 140,000\
    \ documents have to be handled by the European patent office (EPO) per year. They\
    \ are processed by 2,500 patent examiners in three locations.In several studies\
    \ the classification quality of state-of-the-art methods was analyzed. Koster\
    \ et al. (2001) reported very good result with an 3% error rate for 16,000 full\
    \ text documents to be classified in 16 classes (mono-classification) and a 6%\
    \ error rate in the same setting for abstracts only by using the Winnow (Littlestone\
    \ 1988) and the Rocchio algorithm (Rocchio 1971). These results are possible due\
    \ to the large amount of available training documents. Good results are also reported\
    \ in (Krier & Zacca 2002) for an internal EPO text classification application\
    \ with a precision of 81 % and an recall of 78 %.Text clustering techniques for\
    \ patent analysis are often applied to support the analysis of patents in large\
    \ companies by structuring and visualizing the investigated corpus. Thus, these\
    \ methods find their way in a lot of commercial products but are still also of\
    \ interest for research, since there is still a need for improved performance.\
    \ Companies like IBM offer products to support the analysis of patent text documents.\
    \ Dorre describes in (Dörre et al. 1999) the IBM Intelligent Miner for text in\
    \ a scenario applied to patent text and compares it also to data mining and text\
    \ mining. Coupet & Hehenberger (1998) do not only apply clustering but also give\
    \ some nice visualization. A similar scenario on the basis of SOM is given in\
    \ (Lamirel et al. 2003).In publishing houses a large number of news stories arrive\
    \ each day. The users like to have these stories tagged with categories and the\
    \ names of important persons, organizations and places. To automate this process\
    \ the Deutsche Presse-Agentur (dpa) and a group of leading German broadcasters\
    \ (PAN) wanted to select a commercial text classification system to support the\
    \ annotation of news articles. Seven systems were tested with a two given test\
    \ corpora of about half a million news stories and different categorical hierarchies\
    \ of about 800 and 2,300 categories (Paaß & deVries 2005). Due to confidentiality\
    \ the results can be published only in anonymized form.For the corpus with 2,300\
    \ categories the best system achieved at an F1-value of 39%, while for the corpus\
    \ with 800 categories an F1-value of 79% was reached.In the latter case a partially\
    \ automatic assignment based on the reliability score was possible for about half\
    \ the documents, while otherwise the systems could only deliver proposals for\
    \ human categorizers. Especially good are the results for recovering persons and\
    \ geographic locations with about 80% F1-value. In general there were great variations\
    \ between the performances of the systems.In a usability experiment with human\
    \ annotators the formal evaluation results were confirmed leading to faster and\
    \ more consistent annotation. It turned out, that with respect to categories the\
    \ human annotators exhibit a relative large disagreement and a lower consistency\
    \ than text mining systems. Hence the support of human annotators by text mining\
    \ systems offers more consistent annotations in addition to faster annotation.\
    \ The Deutsche Presse-Agentur now is routinely using a text mining system in its\
    \ news production workflow.Bio-entity recognition aims to identify and classify\
    \ technical terms in the domain of molecular biology that correspond to instances\
    \ of concepts that are of interest to biologists. Examples of such entities include\
    \ the names of proteins, genes and their locations of activity such as cells or\
    \ organism names. Entity recognition is becoming increasingly important with the\
    \ massive increase in reported results due to high throughput experimental methods.\
    \ It can be used in several higher level information access tasks such as relation\
    \ extraction, summarization and question answering.Recently the GENIA corpus was\
    \ provided as a benchmark data set to compare different entity extraction approaches\
    \ (Kim et al. 2004). It contains 2,000 abstracts from the MEDLINE database which\
    \ were hand annotated with 36 types of biological entities. The following sentence\
    \ is an example: \"We have shown that <protein> interleukin-1 </protein> (<protein>\
    \ IL-1 </protein>) and <protein> IL-2 </protein> control <DNA> IL-2 receptor alpha\
    \ (IL-2R alpha) gene </DNA> transcription in <cell_line> CD4-CD8-murine T lymphocyte\
    \ precursors </cell_line>\".In the 2004 evaluation four types of extraction models\
    \ were used: Support Vector Machines (SVMs), Hidden Markov Models (HMMs), Conditional\
    \ Random Fields (CRFs) and the related Maximum Entropy Markov Models (MEMMs).\
    \ Varying types of input features were employed: lexical features (words), n-grams,\
    \ orthographic information, word lists, part-of-speech tags, noun phrase tags,\
    \ etc. The evaluation shows that the best five systems yield an F1-value of about\
    \ 70% (Kim et al. 2004). They use SVMs in combination with Markov models (72.6%),\
    \ MEMMs (70.1%), CRFs (69.8%), CRFs together with SVMs (66.3%), and HMMs (64.8%).\
    \ For practical applications the current accuracy levels are not yet satisfactory\
    \ and research currently aims at including a sophisticated mix of external resources\
    \ such as keyword lists and ontologies which provide terminological resources.The\
    \ explosive growth of unsolicited e-mail, more commonly known as spam, over the\
    \ last years has been undermining constantly the usability of e-mail. One solution\
    \ is offered by anti-spam filters. Most commercially available filters use black-lists\
    \ and hand-crafted rules. On the other hand, the success of machine learning methods\
    \ in text classification offers the possibility to arrive at anti-spam filters\
    \ that quickly may be adapted to new types of spam.There is a growing number of\
    \ learning spam filters mostly using naive Bayes classifiers. A prominent example\
    \ is Mozilla's e-mail client. Michelakis et al. (2004) compare different classifier\
    \ methods and investigate different costs of classifying a proper mail as spam.\
    \ They find that for their benchmark corpora the SVM nearly always yields best\
    \ results.To explore how well a learning-based filter performs in real life, they\
    \ used an SVM-based procedure for seven months without retraining. They achieved\
    \ a precision of 96.5% and a recall of 89.3%. They conclude that these good results\
    \ may be improved by careful preprocessing and the extension of filtering to different\
    \ languages.In this article, we tried to give a brief introduction to the broad\
    \ field of text mining. Therefore, we motivated this field of research, gave a\
    \ more formal definition of the terms used herein and presented a brief overview\
    \ of currently available text mining methods, their properties and their application\
    \ to specific problems. Even though, it was impossible to describe all algorithms\
    \ and applications in detail within the (size) limits of an article, we think\
    \ that the ideas discussed and the provided references should give the interested\
    \ reader a rough overview of this field and several starting points for further\
    \ studies."
