---
sectionList:
- header: "Ats systems classifications and applications"
  content: "There are many classifications for ATS systems as illustrated in Fig.\
    \ 2. ATS systems can be classified based on any of the criteria below.Classification\
    \ based on the Input Size: Single-document or Multi-document. Input size refers\
    \ to the number of source documents used to generate the target summary. As shown\
    \ in Fig. 1, Single-Document Summarization (SDS) uses a single text document to\
    \ generate a summary and the target is to shorten the input document while keeping\
    \ the important information (Joshi, Wang, & McClean, 2018). In Multi-Document\
    \ Summarization (MDS), the summary is generated based on a set of input documents\
    \ and the target is to remove repetitive content in the input documents (Joshi\
    \ et al., 2018). MDS is more complex than SDS and has some prominent issues like\
    \ redundancy, coverage, temporal relatedness, compression ratio, etc. (Gupta &\
    \ Siddiqui, 2012).Classification based on the Text Summarization Approach: Extractive,\
    \ Abstractive, or Hybrid. The extractive text summarization approach selects the\
    \ most important sentences in the input document(s) and the output summary is\
    \ composed by concatenating the selected sentences. The abstractive text summarization\
    \ approach represents the input document(s) in an intermediate representation\
    \ and the output summary is generated from this representation. Unlike extractive\
    \ summaries, abstractive summaries consist of sentences that are different than\
    \ the original document (s) sentences. The hybrid text summarization approach\
    \ merges between both the extractive and abstractive approaches. These approaches\
    \ will be explained in more detail in section 3.Classification based on Nature\
    \ of the Output Summary: Generic or Query-Based. A generic text summarizer extracts\
    \ important information from one or more input documents to provide a general\
    \ sense of its contents (Gambhir & Gupta, 2017;Sahoo, Balabantaray, Phukon, &\
    \ Saikia, 2016). A query-based summarization means that the multi-document summarizer\
    \ deals with a group of homogeneous documents, taken out from a large corpus as\
    \ a result of a query (Sahoo et al., 2016). Then, the generated summary includes\
    \ a query-related content. A query-based summary presents the information that\
    \ is most relevant to the initial search query, while a generic summary gives\
    \ an overall sense of the document content (Mohan, Sunitha, Ganesh, & Jaya, 2016).\
    \ The querybased summary is sometimes referred to as a like query-focused, topic-focused,\
    \ or user-focused summary (Gambhir & Gupta, 2017).Classification based on the\
    \ Summary Language: Monolingual, Multilingual, or Cross-Lingual. A summarization\
    \ system is monolingual when the language of source and target documents is the\
    \ same. A summarization system is multi-lingual when the source text is written\
    \ in a number of languages (e.g. English, Arabic, and French) and the summary\
    \ is also generated in these languages. A summarization system is cross-lingual\
    \ when the source text is written in one language (e.g. English) and the summary\
    \ is generated in another one (e.g. Arabic or French) (Gambhir & Gupta, 2017).\
    \ Classification based on the Summarization Algorithm: Supervised, or Unsupervised.\
    \ The supervised algorithm needs a training phase which requires an annotated\
    \ training data. Human efforts are required to manually annotate the training\
    \ data, so the latter is difficult to create and expensive. On the other side,\
    \ the unsupervised algorithm does not require a training phase nor training data\
    \ (Mohd, Jan, & Shah, 2020).Classification based on the Summary Content: Indicative\
    \ or Informative. An indicative summary contains only the general idea or information\
    \ about the source text (Bhat, Mohd, & Hashmy, 2018). So it is used to determine\
    \ what the input text is about (i.e. what topics are addressed) to alert the user\
    \ about the source content (Takeuchi, 2002). The purpose of an indicative summary\
    \ is to inform users about the scope of the input text to help them decide whether\
    \ or not to read the original text. On the other side, an informative summary\
    \ contains the important information and ideas in the original text (Bhat et al.,\
    \ 2018) such that it covers all topics of the text (Gupta & Lehal, 2010). The\
    \ purpose of an informative summary is to cover the main contents of the original\
    \ text without details (Takeuchi, 2002).Classification based on the Summary Type:\
    \ Headline, Sentence-Level, Highlights, or Full Summary. The length of the generated\
    \ summaries differs based on the purpose of the ATS system. Headline generation\
    \ produces a headline which is usually shorter than a sentence (Dernoncourt, Ghassemi,\
    \ & Chang, 2018). A sentence-level summarization generates a single sentence from\
    \ the input text which is usually an abstractive sentence (Dernoncourt et al.,\
    \ 2018). A highlights summarization produces a telegraphic style and highly compressed\
    \ summary which is usually in the form of bullet points (Woodsend & Lapata, 2010).\
    \ The highlights summary provides the reader with a brief overview about the main\
    \ information in the input document(s) (Woodsend & Lapata, 2010). Finally, a full\
    \ summary generation is usually guided by the required summary length or a compression\
    \ ratio.Classification based on the Summarization Domain: General, or Domain-Specific.\
    \ The general or domain-independent ATS system summarizes documents that belong\
    \ to different domains. On the other side, the domain-specific ATS system is specialized\
    \ to summarize documents from a certain domain (e.g. medical documents or legal\
    \ documents).ATS is widely used in text mining and analytics applications such\
    \ as information retrieval, information extraction, question answering, etc. ATS\
    \ is used with the information retrieval techniques to enhance the capabilities\
    \ of the search engines. In Tuarob, Bhatia, Mitra, and Giles (2016), Tuarob et\
    \ al. propose a search engine to search for algorithms and pseudo-codes. First,\
    \ a dataset is constructed by extracting algorithms from scientific papers. Then,\
    \ ATS is used to add additional textual metadata to the extracted algorithms from\
    \ the scientific documents. In Yulianti, Chen, Scholer, Croft, and Sanderson (2018),\
    \ Yulianti et al. use text summarization to extract answers for non-factoid queries.\
    \ In Li, Zhu, Ma, Zhang, and Zong (2018), Li et al. propose an extractive multi-modal\
    \ summarization (MMS) system for asynchronous collections of text, image, audio,\
    \ and video. The proposed system generates a textual summary from the aforementioned\
    \ sources.There are different text genres like news articles, novels, books, forums,\
    \ blogs, emails, opinion reviews, spoken dialogues which combine text summarization\
    \ with speech recognition, medical documents, legal documents, etc. (Vodolazova,\
    \ Lloret, Mu√±oz, & Palomar, 2013a). Each ATS system supports one or more text\
    \ genres as inputs; hence the ATS systems are used for different applications\
    \ like news summarization, email summarization, domain-specific summarization\
    \ (e.g. legal or biomedical documents summarization), etc. Examples of various\
    \ ATS applications are explored below.News Summarization: Newsblaster (McKeown\
    \ et al., 2002) is a text summarizer that helps users find the most desirable\
    \ news to them. On a daily basis, the system automatically collects, clusters,\
    \ categorizes, and summarizes news from several news sites on the Internet (e.g.\
    \ CNN and Reuters) (Gupta & Lehal, 2010). Sahni and Palwe (2017), Sethi, Sonawane,\
    \ Khanwalker, and Keskar (2017) are other examples of ATS systems for news summarization.Opinion/Sentiment\
    \ Summarization: Sentiment Analysis (SA) is the study of people's opinions, emotions,\
    \ and judgments about events and products. ASFuL is an aspect-based sentiment\
    \ summarization system (Mary & Arockiam, 2017). It uses fuzzy logic to classify\
    \ sentiments or opinions polarity from the product reviews as ''Strong Positive\"\
    , ''Positive\", ''Negative\", and ''Strong Negative\". It also integrates the\
    \ non-opinionated sentences using the Imputation of Missing Sentiment (IMS) technique.\
    \ IMS is used to reduce the neutral score in sentiment polarity. E-commerce websites\
    \ are growing rapidly hence there are hundreds of reviews for any product on average.\
    \ ATS is very useful in this case to summarize the user reviews. Bhargava andSharma\
    \ (2017), Lovinger, Valova, andClough (2019), Mirani and Sasi (2017), Roul and\
    \ Arora (2019), Yadav and Chatterjee (2016), Zhou, Wan, and Xiao (2016) are additional\
    \ examples of ATS systems for opinion/sentiment or user reviews summarization.Microblog/Tweet\
    \ Summarization: Social networking sites like Facebook, Twitter, etc. include\
    \ millions of messages (Vijay Kumar & Janga Reddy, 2019). During emergency events,\
    \ microblogging sites such as ''Twitter\" are important sources of real-time information\
    \ because hundreds, thousands or even more of microblogs (tweets) are posted on\
    \ Twitter (Dutta et al., 2019). Therefore, microblog summarization became very\
    \ important in recent years. Examples of ATS systems for microblog or tweet summarization\
    \ can be found in Chakraborty, Bhavsar, Dandapat, and Chandra (2019), Rudra, Goyal,\
    \ Ganguly, Imran, and Mitra (2019), Vijay Kumar and Janga Reddy (2019).Books Summarization:\
    \ most research focus on short documents summarization. In Mihalcea and Ceylan\
    \ (2007), Mihalcea and Ceylan address the problems of book summarization and introduce\
    \ a specific benchmark for book summarization. ATS systems developed for short\
    \ documents are not suitable for summarizing long documents such as books because:\
    \ 1) the selected sentences may not cover all the book topics, 2) considering\
    \ the length of documents is essential for better performance, and 3) using the\
    \ sentence position feature differs in books than short documents (i.e. it may\
    \ be unuseful to include the sentences located at the beginning of the book in\
    \ the generated summary).Story/Novel Summarization: In Kazantseva and Szpakowicz\
    \ (2010), Kazantseva and Szpakowicz present an approach for automatic extractive\
    \ summarization that uses syntactic information and shallow semantics (provided\
    \ by a GATE gazetteer (Cunningham, Maynard, Bontcheva, & Tablan, 2002)) to produce\
    \ indicative summaries focusing on three types of entities: people, locations,\
    \ and timestamps of short stories. The generated summary helps the reader to decide\
    \ whether to read the complete story, but it does not attempt to retell the plot\
    \ for two reasons: 1) many people do not want to know what happens in a story\
    \ before reading it, and 2) the summarization problem would be too complex if\
    \ summarizing the full plot was required.Email Summarization: Email messages are\
    \ domain-general text, they are unstructured and not always syntactically wellformed\
    \ (Muresan, Tzoukermann, & Klavans, 2001). In Muresan et al. (2001), Muresan et\
    \ al. propose an ATS system that combines linguistic techniques with machine learning\
    \ algorithms to extract noun phrases to generate a summary of Email messages.\
    \ More examples of ATS systems for email summarization can be found in Carenini,\
    \ Ng, and Zhou (2007), Ulrich, Carenini, Murray, and Ng (2009).Biomedical Documents\
    \ Summarization: In Men√©ndez, Plaza, and Camacho (2014), Men√©ndez et al. propose\
    \ an ATS system that combines genetic clustering and graph connectivity information\
    \ to improve the graph-based summarization process. Genetic clustering identifies\
    \ the different topics in a document, and connectivity information (i.e. degree\
    \ centrality) shows the importance of the different topics. Morales, Esteban,\
    \ and Gerv (2008), Nasr Azadani, Ghadiri, and Davoodijam (2018), Reeve, Han, and\
    \ Brooks (2006), Reeve, Han, and Brooks (2007) are other examples of ATS systems\
    \ for biomedical text summarization.Legal Documents Summarization: In Kavila,\
    \ Puli, Prasada Raju, and Bandaru (2013), Kavila et al. propose an ATS system\
    \ and an automatic search system for legal documents to save the time of legal\
    \ experts. The summarization task identifies the rhetorical roles presenting the\
    \ sentences of a legal judgment document. The search task identifies the related\
    \ past cases based on the given legal query. The hybrid system uses different\
    \ techniques such as keyword or key phrase matching technique and the case-based\
    \ technique. Anand and Wagh (2019), Kavila et al. (2013), Merchant and Pande (2018)\
    \ are among examples of ATS systems for legal documents summarization.Scientific\
    \ Papers Summarization: Scientific papers are wellstructured documents that have\
    \ some common characteristics like the predictable locations of typical items\
    \ in a document, cue words, and a template-like structure (Kazantseva & Szpakowicz,\
    \ 2010). In Mohammad et al. (2009), Mohammad et al. propose a multi-document ATS\
    \ framework that generates a technical survey on a given topic from multiple research\
    \ papers by merging two methods: 1) mining the structure of citations and relations\
    \ among citations to get citation information, and 2) summarization techniques\
    \ that identify the content of the material in both the citing and cited papers.\
    \ (Alampalli Ramu, Bandarupalli, Nekkanti, & Ramesh, 2020) propose a summarizer\
    \ to extract the problem statement from one research paper then uses it to find\
    \ the related papers. Jiang et al. (2019) present an abstractive deep-learningbased\
    \ summarizer used for automatic survey generation. Cohan and Goharian (2018),\
    \ Lloret, Rom√°-Ferri, and Palomar (2011, 2013), Marques, Cozman, and Santos (2019),\
    \ Mohammad et al. (2009), Teufel and Moens (2002), Zhang, Li, and Yao (2018) represent\
    \ various examples of ATS systems for scientific papers."
- header: "Ats approaches"
  content: "Fig. 3 shows various extractive text summarization methods like statistical-based,\
    \ concept-based, etc. Table 1 illustrates the pros and cons of each method along\
    \ with examples of ATS systems that apply it. In the rest of this subsection,\
    \ we list and briefly describe each of these methods.Statistical-Based Methods:\
    \ These methods extract important sentences and words from the source text based\
    \ on the statistical analysis of a set of features. The ''most important\" sentence\
    \ is defined as the ''most favorably positioned\", the ''most frequent\", etc.\
    \ (Gupta & Lehal, 2010). The sentence scoring steps of a statistical-based extractive\
    \ summarizer include (Gambhir & Gupta, 2017): 1) selecting and calculating some\
    \ statistical and/or linguistic features then assigning weights to them (Gupta\
    \ & Lehal, 2010), and 2) assigning a final score to each sentence in the document\
    \ (Gambhir & Gupta, 2017) that is determined using a feature-weight equation (Gupta\
    \ & Lehal, 2010) (i.e. all the selected features' scores are computed and summed\
    \ to obtain the score of each sentence).Concept-Based Methods: These methods extract\
    \ concepts from a piece of text from external knowledge bases like WordNet (Miller,\
    \ 1995;WordNet: An Electronic Lexical Database, 1998), HowNet, Wikipedia, etc.\
    \ The importance of sentences is then calculated based on the concepts retrieved\
    \ from the external knowledge base HowNet instead of words. The sentence scoring\
    \ steps of a concept-based extractive summarizer include (Moratanch & Chitrakala,\
    \ 2017): 1) retrieving concepts of a text from the external knowledge base, 2)\
    \ building a conceptual vector or graph model to show the relationship between\
    \ concept and sentences, and 3) applying a ranking algorithm to score the sentences.Topic-Based\
    \ Methods: These methods rely on identifying a document's topic which is its main\
    \ subject (i.e. what the document is about). Some of the most common methods for\
    \ topic representations are Term Frequency, Term Frequency-Inverse Document Frequency\
    \ (TF-IDF), lexical chains, topic word approaches in which the topic representation\
    \ consists of a simple table and their corresponding weights (Nenkova & McKeown,\
    \ 2012), etc. The processing steps of a topic-based extractive summarizer include\
    \ (Nenkova & McKeown, 2012): 1) converting the input text to an intermediate representation\
    \ that captures the topics discussed in the input text, and 2) assigning an importance\
    \ score to each of the sentences in the input document according to this representation.Sentence\
    \ Centrality or Clustering-Based Methods: In these methods, a multi-document extractive\
    \ summarizer identifies the most central and important sentences in a cluster\
    \ such that they cover the important information related to the cluster main subject.\
    \ The sentence centrality is defined using the centrality of its words. The common\
    \ way to measure the word centrality is to look at the centroid of the document\
    \ cluster in a vector space. The centroid of a cluster is a pseudo-document that\
    \ consists of words having TF-IDF scores above a predefined threshold (Erkan &\
    \ Radev, 2004). The sentence scoring steps in a centroid-based summarizer include:\
    \ 1) building a representative centroid by computing the TF-IDF representations\
    \ of each sentence in the document (Mehta & Majumder, 2018), and 2) considering\
    \ a sentence as central when it contains more words from the centroid of the cluster\
    \ as a measure of its closeness to the cluster centroid (Erkan & Radev, 2004).\
    \ The closer a sentence is to the cluster centroid the more important it is (Mehta\
    \ & Majumder, 2018). Clustering-based summarization considers both relevance and\
    \ redundancy removal in the generated summary. The clustering algorithms are used\
    \ for ATS and the sentence selection steps include (Nazari & Mahdavi, 2019): 1)\
    \ using a clustering algorithm to cluster the input sentences, 2) ranking and\
    \ ordering clusters such that a cluster gets a high rank when it has more important\
    \ words, and 3) from these clusters, selecting representative sentences for the\
    \ summary.Graph-Based Methods: These methods use sentence-based graphs to represent\
    \ a document or document cluster. Using such representation has been commonly\
    \ used for extractive summarization (e.g. LexRank (Erkan & Radev, 2004) and TextRank\
    \ (Mihalcea & Tarau)). In Erkan and Radev (2004), Erkan and Radev discuss how\
    \ random walks on sentence-based graphs can help in text summarization. For example,\
    \ LexRank's sentence scoring steps include (Wang et al., 2017): 1) representing\
    \ the document sentences using an undirected graph such that each node in the\
    \ graph represents a sentence from the input text and for each pair of sentences\
    \ the weight of the connecting edge is the semantic similarity between the two\
    \ corresponding sentences using cosine similarity, and 2) using a ranking algorithm\
    \ to determine the importance of each sentence. The sentences are ranked based\
    \ on their LexRank scores in a similar way to the PageRank (Brin & Page, 1998)\
    \ algorithm except that the LexRank graph is undirected (Moratanch & Chitrakala,\
    \ 2017).Semantic-Based Methods: Latent Semantic Analysis (LSA) is a commonly used\
    \ semantic-based extractive ATS method. LSA is an unsupervised technique that\
    \ represents text semantics based on the observed co-occurrence of words (Nenkova\
    \ & McKeown, 2012). The sentence scoring steps of any LSA-based extractive summarizer\
    \ include (Al-Sabahi, Zhang, Long, & Alwesabi, 2018): 1) creating the input matrix\
    \ (term-to-sentence matrix), and 2) applying Singular Value Decomposition (SVD)\
    \ to the input matrix to identify the relationships between terms and sentences.\
    \ Many other semantic-based techniques are used for ATS like Semantic Role Labelling\
    \ (SRL) and Explicit Semantic Analysis (ESA). In Mohamed and Oussalah (2019),\
    \ Mohamed and Oussalah propose a semantic-based extractive ATS system based on\
    \ the SRL and ESA along with the Wikipedia knowledge base.Machine-Learning-Based\
    \ Methods: These methods convert the summarization problem to a supervised classification\
    \ problem at the sentence level. The system learns by examples to classify each\
    \ sentence of the test document either as ''summary\" or ''non-summary\" class\
    \ using a training set of documents (i.e. a collection of documents and their\
    \ respective human-generated summaries). For the machine-learning-based summarizer,\
    \ the sentence scoring steps include (Moratanch & Chitrakala, 2017): 1) extracting\
    \ features from the preprocessed document (i.e. based on multiple features of\
    \ sentences and words), and 2) feeding the extracted features to a neural network\
    \ which produces a single value as an output score.Deep-Learning-Based Methods:\
    \ In Kobayashi, Noguchi, and Yatsuka (2015), Kobayashi et al. propose a summarization\
    \ system using document level similarity based on embeddings (i.e. distributed\
    \ representations of words). An embedding of a word represents its meaning. A\
    \ document is considered as a bag-of-sentences and a sentence as a bag-of-words.\
    \ The task is formalized as the problem of maximizing a sub-modular function defined\
    \ by the negative summation of the nearest neighbors distances on embedding distributions\
    \ (i.e. a set of word embeddings in a document). Kobayashi et al. conclude that\
    \ the document-level similarity can determine more complex meanings than sentence-level\
    \ similarity. Chen and Nguyen (2019) propose an ATS system for singledocument\
    \ summarization using a reinforcement learning algorithm and a Recurrent Neural\
    \ Network (RNN) sequence model of encoder-extractor network architecture. The\
    \ important features  (Gambhir & Gupta, 2017). It does not require any extra linguistic\
    \ knowledge or complex linguistic processing and it is language independent (Ko\
    \ & Seo, 2008).Some important sentences may be not included in the summary because\
    \ they have less score than others. Similar sentences may be included in the summary\
    \ because they have high scores. (Afsharizadeh, Ebrahimpour-Komleh, & Bagheri,\
    \ 2018) Concept-Based The summary sentences cover several concepts.It requires\
    \ to use similarity measures in order to reduce redundancy (Moratanch & Chitrakala,\
    \ 2017). The selected concepts (i.e. they are based on the knowledge base) affect\
    \ the quality of the summary. (Sankarasubramaniam, Ramanathan, & Ghosh, 2014)\
    \ Topic-Based The summary sentences concentrate on the various topics in the input\
    \ document(s).Sentences that do not have the highest score will not appear in\
    \ the summary even if they are very related to the main topics (Wang & Ma, 2013).\
    \ The selected topics affect the quality of the generated summary. (Sahni & Palwe,\
    \ 2017) Sentence Centrality or Clustering-BasedIt may avoid including repeated\
    \ sentences in the summary. It is suitable for multidocument summarization because\
    \ it groups different sentences about the same topic in the documents (Nazari\
    \ & Mahdavi, 2019).It requires prior specification of the number of clusters.\
    \ The highly scored sentences may be similar, therefore redundancy removal techniques\
    \ are required (Mehta & Majumder, 2018). Some sentences may express more than\
    \ one topic but each sentence has to be assigned to only one cluster (Nazari &\
    \ Mahdavi, 2019). (Radev, Jing, Stys ¬¥, & Tam, 2004;Roul & Arora, 2019) Graph-Based\
    \ It enhances coherency and detects redundant information (Moratanch & Chitrakala,\
    \ 2017). It is language-independent (Nasar, Jaffry, & Malik, 2019), and domain-independent\
    \ (Nallapati, Zhai, & Zhou, 2017).It assumes that the weights of the words are\
    \ equal, so it does not consider the importance of words in the document (Fang,\
    \ Mu, Deng, & Wu, 2017). It does not focus on issues like the dangling anaphora\
    \ problem (Moratanch & Chitrakala, 2017). Graphs that represent sentences as Bag\
    \ of Words and use similarity measure; may fail to identify semantically equivalent\
    \ sentences (Khan et al., 2018). The selected sentences are affected by the accuracy\
    \ of similarity computation (Wang et al., 2017). (Baralis, Cagliero, Mahoto, &\
    \ Fiori, 2013;Dutta, Das, Mallick, Sarkar, & Das, 2019;Mallick, Das, Dutta, Das,\
    \ & Sarkar, 2019;Mihalcea, 2004;Sarrac√©n & Rosso, 2018) Semantic-Based LSA is\
    \ a language-independent technique and generates semantically related sentences\
    \ in the summary (Gambhir & Gupta, 2017).The generated summary depends on the\
    \ quality of the semantic representation of the input text. Computing SVD consumes\
    \ a large time (Gambhir & Gupta, 2017). (Mashechkin, Petrovskiy, Popov, & Tsarev,\
    \ 2011;Mohamed & Oussalah, 2019) Machine-Learning-Based To improve the sentence\
    \ selection for the summary, a large set of training data is required (Moratanch\
    \ & Chitrakala, 2017).Relatively-simple regression models can achieve better results\
    \ than other classifiers (Gambhir & Gupta, 2017).It requires a large data set\
    \ of manually created extractive summaries such that each sentence in the original\
    \ training documents can be labeled as either ''summary\" or ''non-summary\" (Moratanch\
    \ & Chitrakala, 2017). (Alguliyev et al., 2019;John & Wilscy, 2013;Shetty & Kallimani,\
    \ 2017) Deep-Learning-Based The network can be trained based on the human reader's\
    \ style. The features set can be changed based on the user's requirements (Moratanch\
    \ & Chitrakala, 2017).It requires human efforts to manually build the training\
    \ data. Neural networks are slow in the training and testing phases. It is hard\
    \ to define how the network generates a decision (Moratanch & Chitrakala, 2017).\
    \ (Cheng & Lapata, 2016;Kobayashi et al., 2015;Nallapati et al., 2017;Warule,\
    \ Sawarkar, & Gulati, 2019;Yao, Zhang, Luo, & Wu, 2018;Yousefi-Azar & Hamey, 2017)\
    \ Optimization-Based Using the strength of genetic algorithms to find the optimal\
    \ weights (Meena & Gopalani, 2015).High computational time and cost. It is required\
    \ to define the number of iterations for the optimization algorithms. (Lovinger\
    \ et al., 2019Fuzzy logic is compatible with the real world which is not a two-value\
    \ (0 and 1) world (Nazari & Mahdavi, 2019). It handles the uncertainties in the\
    \ input as the fuzzy inference systems can produce a logical assessment in an\
    \ uncertain and ambiguous environment (Kumar & Sharma, 2019).The redundancy of\
    \ the selected sentences in the summary is a negative factor that may occur and\
    \ affect the quality of summary (Patel et al., 2019). So, a redundancy removal\
    \ technique is required in the postprocessing phase to improve the quality of\
    \ the final summary. (Mutlu, Sezer, & Akcayol, 2019;Patel et al., 2019;Qassem,\
    \ Wang, Barada, Al-Rubaie, & Almoosa, 2019) are selected by a sentence-level selective\
    \ encoding technique then the summary sentences are extracted.Optimization-Based\
    \ Methods: These methods convert the summarization problem to an optimization\
    \ problem. For example, a generic extractive multi-document ATS system is formulated\
    \ as a multi-objective problem in Sanchez-Gomez, Vega-Rodr√≠guez, and P√©rez (2020b).\
    \ The sentence scoring steps of an optimization-based extractive summarizer include:\
    \ 1) creating a suitable representation to the input text such as the commonly-used\
    \ vector representation (i.e. each sentence in the input text is represented as\
    \ a vector of words), and 2) using an optimization algorithm (e.g. A Multi-Objective\
    \ Artificial Bee Colony (MOABC) algorithm) to select the summary sentences based\
    \ on the required summary length limit besides one or more optimization criteria:\
    \ content coverage, redundancy reduction, relevance and coherence (Sanchez-Gomez\
    \ et al., 2020b). In addition, the strength of genetic algorithms in adjusting\
    \ weights could be used for ATS. In Meena and Gopalani (2015), the sentence scoring\
    \ steps for an extractive genetic-algorithm-based summarizer include: 1) identifying\
    \ the text features from the input text like sentence location, sentence length,\
    \ etc. and, 2) using the genetic algorithm to adjust weights of these features\
    \ then calculating the sentences scores.Fuzzy-Logic-Based Methods: These methods\
    \ use the fuzzy logic concept for ATS. Fuzzy logic resembles the human reasoning\
    \ system and provides an efficient way to represent the feature values of sentences\
    \ because not everything in the world can be defined as zero and one (Kumar &\
    \ Sharma, 2019). The sentence scoring steps include (Nazari & Mahdavi, 2019):\
    \ 1) selecting a set of features for each sentence like sentence length, term\
    \ weight, etc., and 2) using the fuzzy logic system (i.e. after inserting the\
    \ required rules in the knowledge base of this system) to provide a score for\
    \ each sentence that reflects the sentence importance. So, each sentence in the\
    \ output gets a score value from 0 to 1 based on the sentence features and the\
    \ predefined rules in the knowledge base.In conclusion, using different methods\
    \ together produce better summaries because this combination uses their strength\
    \ and eliminates their shortcomings. Besides, using a combination of different\
    \ features most probably produces better results when calculating the weights\
    \ of the input sentences (Nazari & Mahdavi, 2019). Many ATS systems combine multiple\
    \ methods to benefit from the advantages of each method like Alami, Meknassi,\
    \ and En-nahnahi (2019), Mao, Yang, Huang, Liu, and Li (2019), Mohd et al. (2020),\
    \ Moratanch and Chitrakala (2017), Rahman, Rafiq, Saha, Rafian, and Arif (2019).\
    \ In Moratanch and Chitrakala (2017), Moratanch and Chitrakala combine both graph-based\
    \ and concept-based methods to build a summarization system. In Rahman et al.\
    \ (2019), Rahman et al. propose an extractive ATS system to summarize Bengali\
    \ text using TextRank, Fuzzy C-Means, and aggregate sentence scoring methods.\
    \ Mohd et al. (2020) propose an extractive summarizer which uses a Distributional\
    \ Semantic Model to capture the semantics of text, the K-means clustering algorithm\
    \ to cluster the semantically similar sentences, and a ranking algorithm to rank\
    \ sentences in each cluster. Mohd et al. conclude that capturing semantics to\
    \ be used for summarization improves the precision of the generated summaries.\
    \ Alami et al. (2019) propose an extractive ATS system relying on an ensemble\
    \ model with a majority voting technique that combines two models: Bag-of-Words\
    \ and Sentence2Vec (i.e. it represents each sentence in the input text as a vector).\
    \ Alami et al. conclude that the ensemble model usually achieves more precise\
    \ results than a single model because the data of each vector is complementary\
    \ to each other. Mao et al. (2019) use three different methods of combining unsupervised\
    \ learning with supervised learning to produce single documents summaries.Abstractive\
    \ summarization needs a deeper analysis of the input text (Mohan et al., 2016).\
    \ Abstractive text summarizers generate a summary by understanding the main concepts\
    \ in the input document using NLP methods, then paraphrasing the text to express\
    \ those concepts in fewer words and using a clear language (Al-Abdallah & Al-Taani,\
    \ 2017; Krishnakumari & Sivasankar, 2018). This approach does not copy sentences\
    \ from the original text for the summary generation (Bhat et al., 2018) instead\
    \ it requires the ability to make new sentences. Fig. 5 shows architecture of\
    \ an abstractive text summarizer. It consists of the pre-processing, post-processing,\
    \ and the processing tasks which include: 1) building an internal semantic representation\
    \ (Chitrakala, Moratanch, Ramya, Revanth Raaj, & Divya, 2018), and 2) generating\
    \ a summary using natural language generation techniques to create a summary that\
    \ is closer to the human-generated summaries (Chitrakala et al., 2018).Advantages:\
    \ It generates better summaries with different words that do not belong to the\
    \ original text by using more flexible expressions based on paraphrasing, compression,\
    \ or fusion (Hou et al., 2017). The generated summary is closer to the manual\
    \ summary (Sun, Wang, Ren, & Ji, 2016). Abstractive methods can further reduce\
    \ the text when compared to the extractive ones (Wang et al., 2017). This higher\
    \ condensation is because the production of new sentences can further reduce any\
    \ redundancy eventually achieving a good compression rate (Sakhare, Kumar, & Janmeda,\
    \ 2018).Disadvantages: In practice, generating a high-quality abstractive summary\
    \ is very difficult (Hou et al., 2017). Good abstractive summarizers are very\
    \ hard to develop as they require the use of natural language generation technology\
    \ which itself is still a growing field (Wang et al., 2017). The abstractive approach\
    \ needs a full interpretation of the input text in order to generate new sentences.\
    \ Most abstractive summarizers suffer from the generation of repeated words and\
    \ are not able to deal with out-of-vocabulary words appropriately (Hou et al.,\
    \ 2017). Capabilities of the abstractive summarizers are constrained by the richness\
    \ of their representations. Systems cannot summarize what their representations\
    \ cannot capture. In limited domains, it may be feasible to devise appropriate\
    \ structures, but a general-purpose solution depends on an open-domain semantic\
    \ analysis (Gupta & Lehal, 2010).The abstractive ATS methods can be categorized\
    \ into three categories (Gupta & Gupta, 2019): 1) structure-based: using predefined\
    \ structures (e.g. graphs, trees, rules, templates, and ontologies), 2) semantic-based:\
    \ using the text semantic representation and the natural language generation systems\
    \ (e.g. based on information items, predicate arguments, and semantic graphs),\
    \ and 3) deep-learning-based methods. Lin and Ng (2019) provide another categorization\
    \ to the abstractive methods as neural-based or classical which broadly refers\
    \ to any method that is not neural-based. Structure-based methods identify the\
    \ most important data in the input text then use graphs, trees, rules, templates,\
    \ or ontology to produce the abstractive summaries (Gupta & Gupta, 2019). Semantic-based\
    \ methods build the semantic representation of the input text by using the information-items,\
    \ predicatearguments, or semantic-graphs then use a natural language generation\
    \ system to produce the abstractive summaries (Gupta & Gupta, 2019). Various abstractive\
    \ text summarization methods are shown in Fig. 3 categorized as structure-based,\
    \ semanticbased, and deep-learning-based. Table 2 illustrates the pros and cons\
    \ of each method along with examples of summarizers that applying them. We use\
    \ the rest of this subsection to list and briefly describe each of these methods.Graph-Based\
    \ Methods: In Ganesan, Zhai, and Han (2010), Ganesan et al. propose an abstractive\
    \ summarizer ''Opinosis\" that uses a graph model. Each node represents a word\
    \ and positional information is linked to nodes. Directed edges represent the\
    \ structure of sentences. The processing steps of the graph-based method in Ganesan\
    \ et al. (2010) include: 1) graph generation: constructing a textual graph to\
    \ represent the source text, and 2) summary generation: generating the target\
    \ abstractive summary. To achieve this, various sub-paths in the graph are explored\
    \ and scored as follows:1. Rank all the paths then sort their scores in descending\
    \ order.The ranking also includes the collapsed paths.2. Eliminate duplicated\
    \ (or extremely similar) paths by using a similarity measure (e.g. Jaccard). 3.\
    \ Select the top few remaining paths as the generated summary, with the number\
    \ of paths to be chosen controlled by a parameter, which represents summary size.Tree-Based\
    \ Methods: These methods identify similar sentences that share mutual information\
    \ then accumulate these sentences to produce the abstractive summary (Gupta et\
    \ al., 2019). The similar sentences are represented into a tree-like structure.\
    \ Parsers are used to construct the dependency trees which are the most used tree-form\
    \ representations for the text. To create the final summaries, some tasks are\
    \ performed to process the trees like pruning, linearization (i.e. converting\
    \ trees to strings), etc. (Gupta & Gupta, 2019). Kurisinkel, Zhang, and Varma\
    \ (2017) propose a multidocument abstractive summarizer as follows: 1) parse the\
    \ input texts in the corpus to build a set of all syntactic dependency trees,\
    \ 2) extract a set of partial dependency trees (with variable sizes) from the\
    \ syntactic dependency trees, 3) cluster the extracted partial dependency trees\
    \ to guarantee topical diversity, and 4) use the partial trees in each cluster\
    \ to form a single sentence that represents the cluster in the abstractive summary.Rule-Based\
    \ Methods: These methods require defining the rules and categories to discover\
    \ the important concepts in the input text then use these concepts to produce\
    \ the summary. The steps of using this method include: 1) categorize the input\
    \ text based on  (Khan, Salim, & Farman, 2016;Ranjitha & Kallimani, 2017). A new\
    \ sentence is created by linking all words in a word graph path (Le & Le, 2013).Word\
    \ graphs do not reflect the word/phrase meaning. Diverse words/phrases that refer\
    \ to the same subject are represented as different nodes, so sentences consisting\
    \ of these nodes cannot be merged (Le & Le, 2013). (Khan et al., 2016;Lloret et\
    \ al., 2011;Ranjitha & Kallimani, 2017) Tree-Based The generated summaries have\
    \ improved quality when using language generators because they produce less-redundant\
    \ and fluent summaries (Gupta & Gupta, 2019).It cannot identify the relations\
    \ between sentences without discovering the shared phrases between these sentences.\
    \ It does not consider the context so it misses many significant phrases in the\
    \ text. This method performance depends on the available parsers which limit its\
    \ efficiency. It focuses on the syntax more than the semantics (Gupta & Gupta,\
    \ 2019).(Kurisinke, Zhang, & Varma, 2017)"
- header: "Rule-Based"
  content: "The hybrid approach combines both the abstractive and extractive approaches.\
    \ The typical architecture of a hybrid text summarizer is shown in Fig. 6. It\
    \ commonly consists of the following phases (Bhat et al., 2018;Lloret et al.,\
    \ 2011): 1) Pre-Processing, 2) sentence extraction (extractive ATS phase): extract\
    \ the key sentences from the input text (Wang et al., 2017), 3) summary generation\
    \ (abstractive ATS phase): generate the final abstractive summary by applying\
    \ the abstractive methods and techniques on the extracted sentences from the first\
    \ phase, and 4) Post-Processing: to ensure that the generated sentences are valid,\
    \ some general rules need to be defined like (Lloret, Rom√°-Ferri, & Palomar, 2013):1.\
    \ The minimal length for a sentence must be 3 words (i.e.subject + verb + object).\
    \ 2. Every sentence must contain a verb. 3. The sentence should not end with an\
    \ article (e.g. ''a\", and ''the\"), a preposition (e.g. ''of\"), a conjunction\
    \ (e.g. ''and\"), nor an interrogative word (e.g. ''who\").Advantages: Combining\
    \ the advantages of both extractive and abstractive approaches. The two approaches\
    \ are complementary and the overall performance of summarization is improved (Wang\
    \ et al., 2017).Disadvantages: Generating a less quality abstractive summary than\
    \ the pure abstractive approach because the generated summary depends on the extracts\
    \ instead of the original text.The research community is focusing more on the\
    \ extractive ATS approach using different methods and techniques, trying to Fig.\
    \ 6. The architecture of a hybrid text summarization system. achieve more coherent\
    \ and meaningful summaries (Gambhir & Gupta, 2017) because the abstractive approach\
    \ is highly complex and needs extensive NLP.Fig. 3 shows various hybrid text summarization\
    \ methods. Table 3 illustrates the pros, cons, and examples of summarizers for\
    \ each method. Up to our knowledge, there are mainly two methods that have been\
    \ used in the hybrid text summarization: extractive to abstractive and extractive\
    \ to shallow abstractive methods. Both methods are briefly discussed next.Extractive\
    \ to Abstractive Methods: These methods start by using one of the extractive ATS\
    \ methods then they use one of the abstractive text summarization methods which\
    \ is applied to the extracted sentences. In Wang et al. (2017), Wang et al. propose\
    \ a hybrid system for long text summarization ''EA-LTS\". The system consists\
    \ of two phases: 1) the extraction phase that uses a graph model to extract the\
    \ key sentences, and 2) the abstraction phase that constructs an RNN based encoder-decoder\
    \ and uses a pointer and attention mechanisms to generate summaries.Extractive\
    \ to Shallow Abstractive Methods: These methods start by using one of the extractive\
    \ ATS methods then they use a shallow abstractive text summarization method that\
    \ applies one or more of the following techniques to the extracted sentences:\
    \ information compression techniques, information fusion techniques (Lloret et\
    \ al., 2013), synonym replacement techniques (Patil, Dalmia, Ansari, Aul, & Bhatnagar,\
    \ 2014), etc. In Bhat et al. (2018), Bhat et al. propose a single-document hybrid\
    \ ATS system called ''SumItUp\". The hybrid system consists of two phases as follows:1.\
    \ Extractive Sentence Selection: uses some statistical features (sentence length,\
    \ sentence position, TF-IDF, noun phrase and verb phrase, proper noun, aggregate\
    \ cosine similarity, and cue-phrases) and a semantic feature (emotion described\
    \ by text) to generate the summary. Cosine similarity is used to remove the redundant\
    \ sentences in the extractive summary. 2. Abstractive Summary Generation: the\
    \ extracted sentences are fed to a language generator (i.e. a combination of WordNet,\
    \ Lesk algorithm and part-of-speech tagger) to convert the extractive summary\
    \ to the abstractive summary. To retain the original sequence, sentences are reordered\
    \ based on their initial index.In conclusion, the hybrid summarization approach\
    \ is a promising research direction. Mahajani et al. recommend researchers to\
    \ propose hybrid ATS systems in order to benefit from the advantages of both extractive\
    \ and abstractive approaches (Mahajani et al., 2019)."
- header: "Techniques and building blocks to implement the ATS systems"
  content: "Text summarization operations can be classified into two categories (Belkebir\
    \ & Guessoum, 2018): 1) single-sentence operations, and 2) multi-sentence operations.\
    \ A single-sentence operation is applied to a single sentence and a multi-sentence\
    \ operation is applied to two sentences at least. Text summarization operations\
    \ can also be classified as either (Hasler, 2007): 1) atomic operation which cannot\
    \ be further divided into other operations (e.g. insertion and deletion of words),\
    \ and 2) complex operation which can be divided into other operations (e.g. replacement\
    \ and reordering of words and merging of sentences). Fig. 7 shows the text summarization\
    \ operations classified as either single-sentence or multisentence operations.\
    \ Some operations can be used alone, in sequence, or in parallel to convert a\
    \ source document into a summary document (Jing, 2002). Based on analyzing the\
    \ operations of the human experts in (Jing, 2002), Jing defines the first 7 of\
    \ the following operations:1. Sentence Compression/Reduction: removal of unimportant\
    \ parts (e.g. phrases) to shorten the original sentence. 2. Syntactic Transformation:\
    \ transformation of a sentence by changing its syntactic structure (e.g. the position\
    \ of the subject in a sentence may be moved from the end to the front). This operation\
    \ may be used in both sentence compression and sentence combination operations.\
    \ 3. Lexical Paraphrasing: replacement of phrases by their paraphrases. 4. Generalization:\
    \ replacement of phrases or clauses by more general descriptions. 5. Specification:\
    \ replacement of phrases or clauses by more specific descriptions. 6. Sentence\
    \ Combination/Fusion: merge of two or more original sentences into a single summary\
    \ sentence. 7. Sentence Reordering: change of the order of summary sentences (e.g.\
    \ an ending sentence in an original text is placed at the beginning of the summary).\
    \ 8. Sentence Selection: selection of one sentence from two or more similar sentences.\
    \ 9. Sentence Clustering: grouping of the sentences into different clusters. This\
    \ operation is very useful in multi-document summarization (e.g. identify the\
    \ subject and cluster the sentences by subject (Zhong et al., 2017)).Each ATS\
    \ system uses one or more of the above operations. In the literature, there are\
    \ many proposed techniques and algorithms that perform these operations automatically\
    \ like Linhares Pontes, Huet, Torres-Moreno, and Linhares (2020), Vanetik, Litvak,\
    \ Churkin, and Last (2020) for sentence compression.The statistical and linguistic\
    \ features are mainly used to identify the important sentences and phrases in\
    \ the input document(s). Both the word level and sentence level features are used\
    \ in the text summarization literature (Moratanch & Chitrakala, 2017). Many ATS\
    \ systems in the literature are based on human-engineered features. Table 4 lists\
    \ the most common features that are clues to include a sentence or phrase in the\
    \ final summary. The simplest automatic text summarizer is based on the sentence\
    \ selection such that sentences are ranked for potential inclusion in the summary\
    \ using a weighted combination of statistical and linguistic features (Goldstein,\
    \ Kantrowitz, Mittal, & Carbonell, 1999). This paradigm transforms the problem\
    \ of summarization, which generally requires the ability to understand, interpret,\
    \ abstract and generate a new summary document, into a simpler problem that is\
    \ solved by the following steps (Goldstein et al., 1999): 1) assigning a score\
    \ to each sentence based on the score of their features, and 2) concatenating\
    \ the top-ranked sentences to form a summary (Wang et al., 2017). The sentence\
    \ score is computed using Eq. (1)."
- header: "Normalized Score√∞S"
  content: "Many text representation models have been used to represent the input\
    \ document(s) in the processing phase of the ATS systems. These text representation\
    \ models are depicted in the left-hand side of Fig. 8. In the following, we briefly\
    \ describe the most relevant text representation models.Graph theory has been\
    \ successfully applied to represent the semantic contents of a document (Vodolazova\
    \ et al., 2013b) or the document structure, so graph modeling is widely used in\
    \ document summarization. In a graph, text elements (words or sentences) are represented\
    \ by nodes and edges connect the related text elements together (e.g. semantic\
    \ relation) (Gambhir & Gupta, 2017). There are two types of graphs to represent\
    \ text: lexical graph and semantic graph.Lexical Graph: uses the lexical features\
    \ of the text to create a graph. TextRank (Mihalcea & Tarau) and LexRank (Erkan\
    \ & Radev, 2004) are lexical graph-based systems. They create graphs by representing\
    \ sentences as nodes and the edges connecting two sentences represent the degree\
    \ of content similarity between them (Joshi et al., 2018). TextRank and LexRank\
    \ are very similar. The key difference between them is that TextRank computes\
    \ similarity as the number of similar words between two sentences, while Lexrank\
    \ uses the cosine similarity between sentences and it is adjusted for multi-document\
    \ summarization (Alami, El Adlouni, En-nahnahi, & Meknassi, 2018).Semantic Graph:\
    \ uses the semantic properties of the text. Semantic properties are the ontological\
    \ relationship between two words (such as synonymy and hyponymy) and the relationship\
    \ among a set of words representing the syntactic structure of sentences (such\
    \ as the dependency tree and syntactic trees). The semantic relations between\
    \ words are very important because a set of words and their way of arrangement\
    \ provide a meaning. The same set of words arranged in different ways has a different\
    \ meaning (Joshi et al., 2018).There are many models of vector representation\
    \ like Bag-of-Words, Vector Space Model, and Word Embedding.Bag-of-Words Model\
    \ (BoW) (Harris, 1954): each sentence in the input text is represented as an N-dimensional\
    \ vector, where (N-1) is the number of all possible words in the text language\
    \ (Erkan & Radev, 2004). An entry ''UNK\" is added to the vector to represent\
    \ any out-of-vocabulary words (Finegan-Dollak, 2018). For each word in the sentence,\
    \ its entry value in the BoW vector is the number of occurrences of the word in\
    \ the sentence multiplied by the IDF of the word (Erkan & Radev, 2004). The BoW\
    \ representation has some weaknesses such as (Finegan-Dollak, 2018): 1) it does\
    \ not include term dependency and has a high dimensionality (dos Santos et al.,\
    \ 2018), 2) it is very sparse because most of the values in the vector are zeros,\
    \ 3) it ignores syntax (e.g. ''The dog chased the cat\" and ''The cat chased the\
    \ dog\" have the same vectors), 4) it ignores the meaning of words (e.g. the dogs\
    \ and cats are animals and the chasing is a type of movement), and 5) inaccurate\
    \ semantic representation of text because word order and arrangement are ignored\
    \ (Kim, Park, Lu, & Zhai, 2012).Vector Space Model (VSM) (Salton, Wong, & Yang,\
    \ 1975) or Term Vector Model: it is a classical form of document representation\
    \ that represents text documents as vectors (Ibrahim, Elghazaly, & Gheith, 2013).\
    \ VSM is commonly used in Information Retrieval (IR) by representing documents\
    \ and queries as vectors of weights. A document d consists of a set of terms (t\
    \ 1 , t 2 . . .t n ) where each term is a word that exists in the document, and\
    \ n is the total number of different words. Each term t has a corresponding weight\
    \ w (Mingzhen & Yu, 2009) such that each weight measures the significance of the\
    \ term in the document or query respectively. The weights are computed based on\
    \ the frequency of the terms in the query or the document (e.g. TF-IDF weights)\
    \ (Melucci, 2009).The great advantage of VSM is the simplicity to search for documents\
    \ or compare documents by using one of the similarity or distance measures like\
    \ the Spearman distance, cosine measure, Euclidean distance, Vector inner product,\
    \ Hamming distance, Correlation distance, or Jaccard distance (Mingzhen & Yu,\
    \ 2009). At retrieval time, the documents are ranked based on the cosine similarity\
    \ values between the document vectors and the query vector and returned to the\
    \ user from the highest to the lowest values (Melucci, 2009). The same concept\
    \ is used for query-based text summarization and clustering the similar sentences\
    \ where text parts (e.g. title, query, and sentences) are represented as vectors.\
    \ The bag-of-words model is the most popular VSM approach due to its simplicity\
    \ and general applicability (dos Santos et al., 2018).Word Vector or Word Embedding:\
    \ neural networks are used for learning vector representations of words. For example,\
    \ different methods are used to get the word embedding (Mogren, Kageback, "
- header: "Concept Similarity of a Sentence"
  content: "N-gram is perfect for multi-language operations because it requires no\
    \ linguistic preparations (e.g. stop word removal or stemming). N-gram is a set\
    \ of words or characters that contains N elements. Word N-grams are a sequence\
    \ of one word (unigrams), two words (bi-grams), three-word (tri-grams) or any\
    \ other N-grams (Abdolahi & Zahedh, 2017). Each word can be represented as a set\
    \ of character N-grams. For example, the word ''TEST\" could be represented with\
    \ the following N-grams (Cavnar, 1994):1. bi-grams: _T, TE, ES, ST, T_ 2. tri-grams:\
    \ _TE, TES, EST, ST_ 3. quad-grams: _TES, TEST, EST_ Where the underscore character\
    \ represents a leading or tailing space. Similar words will share a large number\
    \ of character Ngrams. For example, the words ''RETRIEVE\", ''RETRIEVING\", and\
    \ ''RETRIEVAL\" share the bi-grams: _R, RE, ET, TR, RI, IE, and EV.A document\
    \ is represented as a combination of topics and each topic is a probability distribution\
    \ over words (Singh, Devi, & Mahanta, 2017). A topic model is a generative probabilistic\
    \ model that can be used to identify topics of the input text represented as word\
    \ distributions. A word distribution represents a topic by appointing high probabilities\
    \ to words that portray a topic (e.g. in reviews of iPhone, a topic about battery\
    \ life may have high prob-abilities for words like ''hour\", ''battery\", and\
    \ ''life\") (Kim et al., 2012). The two basic representative topic modeling approaches\
    \ are Latent Dirichlet Analysis (LDA) (Blei, Ng, & Jordan, 2003;Kim et al., 2012)\
    \ and Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999).Commonly used\
    \ general-purpose meaning representations include Lambda Calculus and Abstract\
    \ Meaning Representation.Lambda Calculus: represents meanings as functions applied\
    \ to variables and constants. For example: the sentence ''what states border Texas\"\
    \ may be represented as kx: state(x) ^borders(x; Texas), x is a variable, state()\
    \ is a function that applies to a single entity, and borders() is a function that\
    \ defines a relationship between two entities. Lambda calculus can use higher-order\
    \ functions, which are functions that take other functions as inputs. It can incorporate\
    \ quantifiers and represent all of the first-order logic. Lambda calculus is not\
    \ easy to write, so using it to generate anything more than toy datasets is difficult\
    \ (Finegan-Dollak, 2018).Abstract Meaning Representation (AMR): represents the\
    \ sentences as rooted directed graphs with labels for the edges and leaf nodes.\
    \ It integrates PropBank frames, modality, co-reference, reification, negation,\
    \ and other concepts from a variety of views on what is important in semantics.\
    \ Most of the research on AMR has focused on the parsing of the English sentences\
    \ to AMR. Recently, many research work explored the usefulness of AMR in many\
    \ applications such as ATS, headline generation, machine comprehension, and question\
    \ answering (Finegan-Dollak, 2018).The linguistic properties of the original document\
    \ affect the quality of the generated summary (Vodolazova et al., 2013b). There\
    \ are many linguistic analysis and processing techniques that are widely used\
    \ in ATS (Vodolazova et al., 2013b). These techniques are also illustrated in\
    \ near the center of Fig. 8 under the ''Linguistic Analysis and Processing Techniques\"\
    \ subtree. Stanford CoreNLP  (Manning, Surdeanu, Bauer, Finkel, Bethard, & McClosky,\
    \ 2014) is a commonly used tool that provides many of the common core NLP steps\
    \ like words tokenization and co-reference resolution. In the following, we briefly\
    \ describe the most relevant linguistic analysis and processing techniques.There\
    \ are many pre-processing techniques like noise removal, sentence segmentation,\
    \ word tokenization, etc. Most of these techniques are usually used in the pre-processing\
    \ phase of an ATS system.Noise Removal: removes unnecessary text from the input\
    \ document like the header, footer, etc. (Gupta & Siddiqui, 2012) Sentence Segmentation:\
    \ divides the text into sentences (Gambhir & Gupta, 2017). Splitting sentences\
    \ by using end markers like ''.\", ''?\", or ''!\" is not suitable in many cases.\
    \ If we rely on these markers, words like ''e.g.\", ''i.e.\", ''4.5\", ''Mr.\"\
    , ''Dr.\", or ''etc.\" lead to the false identification of sentence boundaries.\
    \ To solve this problem, simple heuristics and regular expressions are used (Gupta\
    \ & Siddiqui, 2012).Removal of Punctuation Marks: punctuation marks are considered\
    \ as noisy terms in the text. So, removing them is very helpful before executing\
    \ most NLP tasks (Gambhir & Gupta, 2017).Word Tokenization: breaks the text into\
    \ separate words. Words are separated by white space, comma, dash, dot, etc. (Gupta\
    \ & Siddiqui, 2012) Named Entity Recognition (NER): identifies words of the input\
    \ text as names of things (i.e. person name, location name, company name, etc.).Removal\
    \ of Stop-Words: stop-words are words that occur frequently in the text like articles,\
    \ pronouns, prepositions, auxiliary verbs, and determiners. They are removed because\
    \ they do not add any useful meaning to the analysis (Jaradat & Al-Taani, 2016)\
    \ and have no effect in selecting the important sentences (Gambhir & Gupta, 2017).Stemming:\
    \ reduces the words with the same root or stem to a common form by removing the\
    \ variable suffixes (Gambhir & Gupta, 2017;Manning, Raghavan, & Sch√ºtze, 2008)\
    \ like ''es\" and ''ed\". The purpose of stemming is to obtain the stem or radix\
    \ of each word to put emphasis on its semantics (Gupta & Lehal, 2010).Part-Of-Speech\
    \ (POS) Tagging: assigns POS tags (e.g. verb, noun, etc.) to words in a sentence.Frequency\
    \ Computation: the frequency of words is computed and normalized by dividing it\
    \ by the maximum frequency of any word in the document (Gupta & Siddiqui, 2012).\
    \ Most frequent words are very important to help in the selection of the most\
    \ important sentences in the original document(s). By analyzing the human-made\
    \ summaries, they are very likely to include the high-frequency words from the\
    \ original documents (Lloret & Palomar, 2009).There are several uses of parsing\
    \ techniques in the ATS processing phase like constructing the text graph models\
    \ and in the postprocessing phase like sentence compression, sentences merging,\
    \ etc. There are many parsing techniques like syntactic parsing, text chunking,\
    \ semantic parsing, and shallow semantics.Syntactic Parsing: refers to the task\
    \ of recognizing a sentence and assigning a syntactic structure to it. The standard\
    \ way to represent the syntactic structure of a grammatical sentence is as a syntax\
    \ tree (or a parse tree) which is a representation of all the steps in the derivation\
    \ of the sentence from the root node (i.e. each internal node in the tree represents\
    \ an application of a grammar rule) (Indurkhya and Damerau, 2010). Parse trees\
    \ are useful in grammar checking: a sentence that cannot be parsed may have grammatical\
    \ errors (or at least is hard to read) (Jurafsky & Martin, 2017). Constructing\
    \ the syntactic structure of a sentence is important to understand it. Without\
    \ this, it is very challenging for language users to specify that sentences with\
    \ the same words and different word orders have different interpretations (e.g.\
    \ ''The woman sees the man\" and ''The man sees the woman\") or explain why a\
    \ sentence like ''The hunter killed the poacher with the rifle\" has two possible\
    \ interpretations (Levelt & Caramazza, 2007).Text Chunking: refers to a complete\
    \ partitioning of a sentence into chunks of different types like verb groups,\
    \ noun groups, etc. Full parsing is not very robust and expensive hence text chunking\
    \ plays an important role in NLP. Partial parsing is more robust, much faster,\
    \ and sufficient for many applications like question answering, information extraction,\
    \ etc. Besides, it can be used as the first step for the full parsing process.\
    \ Text chunking is considered as a shallow parsing technique (Maiti, Garain, Dhar,\
    \ & De, 2015).Semantic Parsing: refers to the task of converting natural language\
    \ text to a complete and formal meaning representation (Clarke, Goldwasser, Chang,\
    \ & Roth, 2010). Some researchers have focused on general-purpose meaning representations\
    \ such as lambda calculus, AMR, and Prolog, while others have focused on more\
    \ task-specific meaning representations (Finegan-Dollak, 2018).Shallow Semantics:\
    \ represents just a small portion of the semantic information about the sentence.\
    \ Semantic Role Labeling is a main example. SRL labels the constituents of a sentence\
    \ with their semantic roles (e.g. to identify the agent and patient of a verb).\
    \ In the labeled sentence S1 '' [John AGENT ] ate [the fish PATIENT ]-\", John\
    \ is the agent who did the eating. The sentence S2 ''[The fish AGENT ] ate [John\
    \ PATIENT ]\" has a different meaning as the fish did the eating. The sentence\
    \ S3 ''[The fish PATIENT ] was eaten by [John AGENT ]\" is syntactically different\
    \ from S1 while being semantically equivalent to it. As such S3 is a paraphrasing\
    \ of S1 (Finegan-Dollak, 2018).There are many semantic-based techniques like word\
    \ sense disambiguation, anaphora resolution, etc. The use of these techniques\
    \ covers all the ATS system phases as follows: 1) in the preprocessing, textual\
    \ entailment and word sense disambiguation techniques are used, 2) in the processing\
    \ phase, the latent semantic analysis and lexical chain techniques are used, and\
    \ 3) in the postprocessing phase, word sense disambiguation, anaphora resolution,\
    \ and textual entailment techniques are used.Word Sense Disambiguation (WSD):\
    \ identifies the proper meaning of the given word (i.e. computationally find the\
    \ correct sense of the ambiguous words using the context in which they occur).\
    \ For example: the word ''bank\" has many meanings in English. Such words with\
    \ multiple meanings are called polysemous words. WSD is the process of finding\
    \ out the exact meaning of the polysemous word (Sheth, Popat, & Vyas, 2018).Anaphora\
    \ Resolution: analyzes the pairs of nouns, pronouns, and proper nouns in a document.\
    \ A powerful anaphora resolution tool relates pronouns to their nominal antecedents.\
    \ It is used in all the summarization methods that depend on term overlap, from\
    \ the simple term frequency to latent semantic analysis (Vodolazova et al., 2013b).\
    \ A common problem in the extractive ATS is the anaphora ''dangling\" in the sentences\
    \ that contain pronouns while missing their referents when extracted out of context.\
    \ Moreover, stitching together decontextualized extracts may mislead the interpretation\
    \ of anaphors hence cause an inaccurate representation of the source information\
    \ (Gupta & Lehal, 2010).Latent Semantic Analysis (Deerwester, Dumais, Furnas,\
    \ Landauer, & Harshman, 1990): excerpts hidden semantic structures of words and\
    \ sentences. LSA is an unsupervised learning approach so it does not require any\
    \ training data. In LSA, a termby-sentence matrix representation is created from\
    \ the input document(s). LSA gets information such as words that frequently occur\
    \ together and words that are commonly used in different sentences. A high number\
    \ of common words among the sentences means that the sentences are semantically\
    \ related. SVD is a method that is applied to the term-by-sentence matrix. SVD\
    \ is used to find out the interrelations between words and sentences which has\
    \ the competence of noise reduction that helps to improve accuracy. SVD is applied\
    \ to document word matrices to group documents that are semantically related despite\
    \ lacking common words. The set of words that result in the connected text is\
    \ also connected in the same dimensional space (Moratanch & Chitrakala, 2017).Textual\
    \ Entailment (TE): determines if the meaning of one text snippet (the hypothesis)\
    \ can be inferred by another one (the text) (Lloret & Palomar, 2009) so it is\
    \ used to capture the semantic inference between text fragments (Vodolazova et\
    \ al., 2013b). If two sentences contain a true entailment relationship, they are\
    \ considered equivalent and the one which is entailed can be discarded (Lloret\
    \ et al., 2011). There are different uses of the TE in ATS systems: 1) in the\
    \ process of the final summary evaluation (i.e. in a set of generated summaries,\
    \ TE is used to decide which summary of them can be best deduced from the original\
    \ document), 2) in the segmentation of the input by using different algorithms\
    \ that employ TE, and 3) in the process of summary generation to avoid redundant\
    \ information appearing in the final summary (Lloret & Palomar, 2009). TE is often\
    \ applied to eliminate the semantic redundancy of the generated summary (Vodolazova\
    \ et al., 2013b).Lexical Chain: represents the semantic content that may cover\
    \ a small or big part of the text. The coverage and size of a lexical chain indicate\
    \ the accuracy of the lexical chain to represent the semantic content of the text.\
    \ A lexical chain contains a set of words (word senses) that are semantically\
    \ related in the text. Lexical chains are constructed by using relationships between\
    \ word senses. So it is required to know the word senses and semantic relations\
    \ between words. WordNet is a database that provides this type of information:\
    \ synonym sets, hyponym/hypernym, and meronym trees. For each lexical chain, the\
    \ number of semantic relations and the number of words among the words can be\
    \ different. Building a lexical chain is an exhaustive method because one word\
    \ can have many senses and it is required to select its correct sense of the word.\
    \ Lexical chains are used in text summarization and keyword extraction (i.e. keywords\
    \ are short forms and condensed versions of the documents and their summaries)\
    \ (Ercan & Cicekli, 2007).Discourse relations in the text represent connections\
    \ between sentences and parts in a text (Gambhir & Gupta, 2017). It is essential\
    \ to determine the overall discourse structure of the text for producing a coherent\
    \ and fluent summary (i.e. contains the flow of the author's argument) then being\
    \ able to remove unrelated sentences to the context of the text (Gupta & Lehal,\
    \ 2010). Mann and Thompson proposed the Rhetorical Structure Theory (RST) (Mann\
    \ William & Thompson Sandra, 1988) to act as a discourse structure. RST is one\
    \ of the well-known models for text structure representation and is mainly used\
    \ to represent the coherence of texts. Using RST, text can be divided into sub-parts\
    \ forming a hierarchical structure. Every sub-part has a relationship to another\
    \ sub-part with one of the rhetorical relation types (e.g. Motivation, Contrast,\
    \ and Elaboration). These relations form the overall coherence structure of the\
    \ text. A small number of defined rhetorical relations can be used to explain\
    \ the relationships among a wide range of texts (Takeuchi, 2002). RST has two\
    \ main concepts (Gambhir & Gupta, 2017): 1) coherent texts contain few units connected\
    \ together by rhetorical relations, and 2) there must be some relations between\
    \ various parts of the coherent texts.The similarity techniques are widely used\
    \ to measure the similarity between sentences or texts in general and can be classified\
    \ into three categories (Wali, Gargouri, & Ben Hamadou, 2017): syntactic similarity,\
    \ semantic similarity, or hybrid methods. In the following, we briefly describe\
    \ each of these.Syntactic Similarity Methods: string matching, word order, and\
    \ word co-occurring are counted to compute the syntactic similarity. Calculating\
    \ the co-occurring words in a string sequence may not always be useful because\
    \ sentences may be very similar while having rare co-occurring words. For literal\
    \ similarity, Levenshtein distance (edit distance) is a string metric that can\
    \ be used for measuring the difference between two sequences. Levenshtein distance\
    \ between two words is the minimum number of singlecharacter edits (e.g. insertions,\
    \ deletions, or substitutions) required to change one word to the other (Wali\
    \ et al., 2017;Wang et al., 2017).Semantic Similarity Methods: use the semantic\
    \ nets like WordNet, the vector space model, and the statistical corpus to compute\
    \ the semantic similarity between words using different known measures. The semantic-based\
    \ methods are limited to compute the sentence similarity based only on the semantic\
    \ similarity between words. The syntactic information and other semantic knowledge\
    \ (e.g. semantic class and thematic roles) are not used. For example: after training\
    \ the sentence vectors, they are used as features for the sentences. The semantic\
    \ similarity between two sentences is simply obtained by calculating the cosine\
    \ similarity of their vectors (Wali et al., 2017;S. Wang et al., 2017).Hybrid\
    \ Methods: use both semantic and syntactic knowledge. The main disadvantage of\
    \ these hybrid methods is that the semantic measurement is isolated from the syntactic\
    \ measurement (Wali et al., 2017).Natural Language Generation (NLG) is the task\
    \ of generating texts from the input information like texts, knowledge, or images.\
    \ It is a challenging task because it requires understanding the input information\
    \ then organizing the text that will be generated. Simply, NLG requires to answer\
    \ the following questions: ''what to say?\" and ''how to say it?\" (Li, Sun, &\
    \ Li, 2019). Generating good texts is highly dependent on the good representation\
    \ and understanding of the input information. NLG has been used in many applications\
    \ like text summarization, descriptions of museum artifacts, auto-completion,\
    \ dialog systems, auto-paraphrasing, question answering, machine translation (Kurup\
    \ & Narvekar, 2020). NLG systems are widely used in the abstractive text summarization\
    \ methods to generate the final abstractive summary with sentences different than\
    \ the original text sentences.Soft computing solves complex problems by manipulating\
    \ the uncertainty and imprecision in the decision making practices. Soft computing\
    \ is guided by a main principle of ''exploit the tolerance for imprecision, uncertainty,\
    \ partial truth, and approximation to achieve tractability, robustness and low\
    \ solution cost\" (Rao & Svp Raju, 2011). Many soft computing techniques have\
    \ been used as building blocks in the ATS systems such as machine learning algorithms,\
    \ fuzzy logic system, genetic algorithm, etc. All these techniques are not competitive\
    \ but complementary to each other, so each of these techniques can be used individually\
    \ or with each other to get better summarization results (Ibrahim, 2016). For\
    \ example, ''neurofuzzy\" system is an effective combination that merges between\
    \ the neural networks and fuzzy logic techniques (Rao & Svp Raju, 2011). This\
    \ survey cannot cover all soft computing techniques because there exists a huge\
    \ number of algorithms. Only the most commonly used ones are illustrated in the\
    \ right-hand side of Fig. 8. In the following, we provide a brief description\
    \ of each.Machine learning algorithms are widely used in ATS systems like Alguliyev,\
    \ Aliguliyev, Isazade, Abdi, and Idris (2019), Shetty andKallimani (2017), Yousefi-Azar\
    \ andHamey (2017). Machine learning algorithms are categorized as: supervised,\
    \ unsupervised, or semi-supervised.Supervised Learning Algorithms: require a large\
    \ amount of labeled or annotated data as input to a training stage (Moratanch\
    \ & Chitrakala, 2017). Commonly-used supervised learning algorithms include: Support\
    \ Vector Machine (SVM), Na√Øve Bayes Classification, Mathematical Regression, Decision\
    \ Trees, and Artificial Neural Networks (ANN).Unsupervised Learning Algorithms:\
    \ do not require any training data. They try to discover the hidden structure\
    \ in unlabeled data. These techniques are therefore suitable for any newly observed\
    \ data without any required modifications. Commonlyused techniques of that type\
    \ include clustering, and Hidden Markov Model (HMM). When used for ATS, these\
    \ systems access only the target documents and apply heuristic rules to extract\
    \ highly relevant sentences to generate a summary.Semi-Supervised Learning Algorithms:\
    \ require both labeled and unlabeled data to generate an appropriate function\
    \ or classifier.Optimization algorithms have been widely used for ATS systems\
    \ like Sanchez-Gomez, Vega-Rodr√≠guez, and P√©rez (2018) and Alguliyev et al. (2019).\
    \ The most common algorithms are the genetic algorithm and particle swarm optimization.Genetic\
    \ Algorithm (GA): A GA is a search based optimization algorithm inspired by the\
    \ analogy of evolution and population genetics. The GA is effective in searching\
    \ for very large and varied spaces in a wide range of applications (Jaradat &\
    \ Al-Taani, 2016). It optimizes initially-random solutions by applying natural\
    \ evolution operations like: selection, mutation, and crossover on them (Gambhir\
    \ & Gupta, 2017). Al-Radaideh and Bataineh (2018), Chatterjee, Mittal, andGoyal\
    \ (2012), Garc√≠a-Hern√°ndez andLedeneva (2013), Neri Mendoza, Ledeneva, and Garc√≠\
    a-Hern√°ndez (2019) show examples of ATS research making use of GAs. The genetic\
    \ algorithm steps include (Ibrahim, 2016):1. Initialization: creating an initial\
    \ population randomly. 2. Evaluation: evaluating each member of the population\
    \ and assessing its fitness based on how close it matches the preferred requirements.\
    \ 3. Selection: selecting some individuals while favoring those with higher fitness.\
    \ 4. Crossover: creating new individuals by combining the features of the selected\
    \ individuals. At the end of this step, it is expected that the created individuals\
    \ are closer to the preferred requirements. 5. Repeat steps 2 to 5 until the termination\
    \ condition is reached.Particle Swarm Optimization (PSO): PSO is one of the most\
    \ powerful bio-inspired algorithms used to obtain an optimal solution (Dalal &\
    \ Malik, 2018). PSO algorithm is inspired by the social movement of birds (Nazari\
    \ & Mahdavi, 2019). Al-Abdallah and Al-Taani (2017), Mandal, Singh, and Pal (2019),\
    \ Priya and Umamaheswari (2019) are examples of ATS research making relying on\
    \ the PSO algorithm. The PSO algorithm steps include (Venter & Sobieszczanski-Sobieski,\
    \ 2003):1. Starting with an initial population of particles (individuals) which\
    \ are randomly discovered through the design space. Each individual has a random\
    \ position and a velocity. 2. Calculating a velocity vector for each individual.\
    \ 3. Updating the position of each individual using its previous position and\
    \ the newly updated velocity vector. 4. Until convergence, repeating the above\
    \ steps from the second step.Fuzzy logic has been widely used in ATS systems like\
    \ Abbasighalehtaki, Khotanlou, and Esmaeilpour (2016), Jafari et al. (2016), andPatel,\
    \ Shah, andChhinkaniwala (2019). The inputs of text features that are given to\
    \ the fuzzy logic system include: sentence length, sentence similarity, etc. (Moratanch\
    \ & Chitrakala, 2017). Fuzzy logic systems mainly contain four components (Ibrahim,\
    \ 2016) as follows:Fuzzifier (fuzzification interface): it transforms the crisp\
    \ input value to a fuzzy linguistic value because the input values are always\
    \ crisp numerical values. Inference Engine: it uses the fuzzy inputs and the fuzzy\
    \ rules to generate the fuzzy outputs. Fuzzy Knowledge Base: it contains the fuzzy\
    \ rules in the form of ''IF-THEN\" rules including the linguistic variables. Defuzzifier\
    \ (defuzzification interface): it is the last step of a fuzzy logic system. It\
    \ converts the fuzzy outputs to crisp output actions."
- header: "Text summarization datasets and evaluation metrics"
  content: "In Dernoncourt et al. (2018), Dernoncourt et al. provide an overview of\
    \ many corpora that have been used in the summarization tasks. This survey presents\
    \ the most common benchmarking datasets which have been used for the ATS systems\
    \ evaluation as shown in Table 5 including Each dataset contains both documents\
    \ and their summaries in three forms: 1) manually created summaries, 2) automatically\
    \ created baseline summaries, and 3) automatically created summaries that were\
    \ generated by challenge participants systems. To get access to these datasets,\
    \ it is required to complete some application forms that exist on the DUC website.1\
    \ These datasets are usually used to evaluate the ATS systems but they do not\
    \ provide enough data to train the neural networks models (Lin & Ng, 2019). 2.\
    \ Text Analysis Conference (TAC) Datasets: in 2008, DUC became a summarization\
    \ track in the TAC. It is required to complete some application forms that exist\
    \ on the TAC website 2 in order to get access to the TAC datasets. 3. Essex Arabic\
    \ Summaries Corpus (EASC) Dataset (El-Haj, Kruschwitz, & Fox, 2015): it contains\
    \ Arabic articles and the human-generated extractive summaries of these articles.\
    \ EASC uses copyrighted material. It is the responsibility of the dataset users\
    \ to comply with all associated copyright rules. 4. SummBank Dataset (Radev, Teufel,\
    \ Saggion, Lam, Blitzer, Qi, & Drabek, 2003): it contains 40 clusters of news,\
    \ humanwritten non-extractive summaries, 360 multi-document, and approximately\
    \ two million multi-document and single document extracts produced by manual and\
    \ automatic methods. 5. Opinosis Dataset (Ganesan et al., 2010): it contains 51\
    \ files.Each file is about a feature of a product (e.g. the battery life of iPod)\
    \ that includes a set of reviews written by customers who bought that product.\
    \ So, the dataset represents 51 topics such that each topic includes approximately\
    \ 100 sentences. The dataset contains 5 manually written ''gold\" summaries for\
    \ each topic. For most topics, the 5 summaries are different. 6. Large-scale Chinese\
    \ Short Text Summarization (LCSTS) Dataset (Hu, Chen, & Zhu, 2015): It contains\
    \ more than 2 million short texts with short summaries. This dataset is created\
    \ from the SinaWeibo website (i.e. a Chinese microblogging website). 7. Computer-Aided\
    \ Summarization Tool (CAST) Corpus (Hasler, Orasan, & Mitkov, 2003): it contains\
    \ a set of newswire texts which are taken from the Reuters Corpus 3 and a few\
    \ science texts from the British National Corpus. 4 The news texts part of the\
    \ corpus is available after signing the agreement with Reuters 5 while the other\
    \ part cannot be distributed. The corpus contains three types of information annotation:\
    \ the sentence importance (essential or important), links between sentences, and\
    \ text fragments which can be removed from the marked sentences. A sentence is\
    \ considered unimportant if it is not annotated. This dataset could be very useful\
    \ for developing sentence reduction and sentence selection algorithms. 8. CNN-corpus\
    \ Dataset (Lins, Oliveira, et al., 2019): it can be used for single-document extractive\
    \ summarization. It contains the original texts, highlights, and gold-standard\
    \ summaries. This corpus was recently used in the extractive text summarization\
    \ competition ''DocEng'19\" (Lins, Mello, & Simske, 2019). For research purposes,\
    \ this corpus with all its annotated versions is freely available by requesting\
    \ it from its authors. 9. Gigaword 5 Dataset: It is a popular dataset for abstractive\
    \ summarization research. It contains ten million English news documents approximately\
    \ so it is suitable for training and testing the neural networks models. Gigaword\
    \ is criticized because it contains only headlines as summaries (Lin & Ng, 2019;Nallapati,\
    \ Zhou, Santos, Gulcehre, & Xiang, 2016). 10. CNN/Daily Mail Corpus (Hermann et\
    \ al., 2015): this corpus is used for the passage-based question answering task\
    \ then it has been widely used for evaluating the ATS systems. (Nallapati et al.,\
    \ 2016) provide a modified version of this corpus such that it contains multi-sentence\
    \ summaries for abstractive summarization evaluation.In Table 5, the following\
    \ features are defined for each dataset: 1) the dataset name, 2) the number of\
    \ documents, 3) the language of data, 4) the domain of data (e.g. news or blogs),\
    \ 5) whether the dataset supports single-document and/or multi-document summarization,\
    \ and 6) the dataset URL. In Table 5, the first 5 features have been filled from\
    \ Dernoncourt et al. (2018) except for the datasets ''EASC\", ''SummBank\", ''CAST''\
    \ and ''CNN-corpus\" as their features are extracted from their corresponding\
    \ papers and websites. The number of documents for a multi-document summarization\
    \ dataset is written like ''30 √Ç 10\" which means that the dataset includes 30\
    \ clusters of documents and each cluster contains 10 documents approximately.2\
    \ https://tac.nist.gov/data/forms/index.html. 3 http://about.reuters.com/researchandstandards/corpus/.\
    \ 4 http://www.natcorp.ox.ac.uk/. 5 http://about.reuters.com/researchandstandards/corpus/how_to_apply.asp.\
    \ In conclusion, there is a need for more datasets that 1) support non-English\
    \ languages, and 2) cover the various data domains for all languages because most\
    \ of the available datasets focus on the news domain as shown in Table 5. In addition,\
    \ Dernoncourt et al. conclude that 1) there is a need for more large-scale corpora\
    \ especially for evaluating machine-learning-based and deep-learningbased summarization\
    \ systems, and 2) a data standard is required for all summarization corpora because\
    \ each corpus is organized differently (Dernoncourt et al., 2018). If the researchers\
    \ evaluate their proposed ATS systems on many corpora, they will consume a lot\
    \ of time. As a result, the research papers in the ATS field usually use one or\
    \ very few corpora.In the past two decades, there were many efforts to solve summary\
    \ evaluation issues. NIST leads the effort by organizing the DUC and TAC challenges\
    \ (Lloret, Plaza, & Aker, 2017). In Huang, He, Wei, and Li (2010), Huang et al.\
    \ formulate four main objectives that should be considered to generate a condensed\
    \ and readable summary:1. Information Coverage: the summary should contain the\
    \ important information of the input document(s). 2. Information Significance:\
    \ the summary should cover the various topics of the input document(s). The most\
    \ important topics can either be the main topics in the input document(s) as in\
    \ the generic summarization or the user-preferred topics as in the query-based\
    \ summarization. 3. Information Redundancy: minimize the redundant or duplicate\
    \ information in the generated summary. 4. Text Coherence: the summary is not\
    \ just a set of important but disconnected phrases or words. The summary should\
    \ be readable and understandable text.There are two evaluation measures to evaluate\
    \ the generated summaries (Gupta & Lehal, 2010): 1) intrinsic methods: measure\
    \ summary quality using human evaluation. The intrinsic evaluation assesses the\
    \ coherence and the content coverage or informativeness of a summary (Lloret et\
    \ al., 2017), and 2) extrinsic methods: measure summary quality through a task-based\
    \ performance measure such as the information retrieval-oriented task. The extrinsic\
    \ evaluation assesses the utility of summaries in a given application context\
    \ (e.g. relevance assessment, reading comprehension, etc.) (Lloret et al., 2017).\
    \ There are two ways of text summarization evaluation: manual and automatic. Summary\
    \ evaluation is a very challenging issue in the text summarization research field.\
    \ The automatically generated summaries have to be evaluated in order to assess\
    \ the quality of the ATS systems that generated them (Lloret et al., 2017). The\
    \ ATS system performance is usually compared to different baseline systems such\
    \ as using leading sentences from the input document or using common text summarizers\
    \ such as LexRank (Erkan & Radev, 2004), TextRank (Mihalcea & Tarau, 2004), MEAD\
    \ (Radev, Blair-Goldensohn, & Zhang, 2001), etc.The human judges may be asked\
    \ to evaluate the computergenerated summaries using some or all of the following\
    \ quality metrics (Lloret et al., 2017;Mani, 2001):Readability: assess the linguistic\
    \ quality of the summary by checking that it does not contain gaps in its rhetorical\
    \ structure or dangling anaphora. Structure and Coherence: the summary has to\
    \ be wellorganized and well-structured. It consists of a set of coherent and related\
    \ sentences.Grammaticality: the summary should not include incorrect sentences\
    \ that violate the grammar rules or capitalization errors. Referential Clarity:\
    \ if the summary includes a pronoun, so the reader should identify the noun phrase\
    \ it refers to easily. Content coverage: the summary should include the various\
    \ topics that have been discussed in the input document(s). Conciseness and Focus:\
    \ each sentence in the summary should enclose information that is related to the\
    \ other sentences. Non-redundancy: summary should not include unnecessary repetition\
    \ which may take different forms like: whole sentences or parts of sentences that\
    \ are repeated, or the repeated use of a noun phrase or noun like ''Jack Tomson\"\
    \ when a pronoun ''he\" is sufficient.A qualitative evaluation like in Lloret\
    \ et al. (2013) may be used to measure the user satisfaction by focusing on a\
    \ five-point scale (1 = strongly disagree, 2 = disagree, 3 = neither agree nor\
    \ disagree, 4 = agree, and 5 = strongly agree) that are used to answer the following\
    \ questions: Q1: The summary reflects the most important issues of the document.\
    \ Q2: The summary allows the reader to know what the article is about. Q3: After\
    \ reading the original abstract provided with the article, the alternative summary\
    \ is also valid.As there is no best reference summary among the humancreated model\
    \ summaries, the Pyramid method (Nenkova & Passonneau, 2004;Nenkova, Passonneau,\
    \ & McKeown, 2007) was created to overcome this problem. This method is semiautomatic\
    \ and has been used in DUC 2006 to evaluate summary content. The main idea is\
    \ to create a gold standard summary by comparing the human-created reference summaries\
    \ based on Summary Content Units (SCUs). First, the similar sentences among the\
    \ N model summaries are identified manually. Next, SCUs of the similar sentences\
    \ are produced as a pyramid model which consists of N levels labeled from 1 to\
    \ N. Based on the occurrence of SCUs in the model summaries; they are ranked in\
    \ the pyramid. Last, a summary is considered as good if it encloses more SCUs\
    \ from the higher levels in the pyramid than the lower levels and vice versa for\
    \ the poor summary (Lloret et al., 2017).Manual evaluation and analysis of summaries\
    \ consumes a lot of time and effort because it needs humans to read the summaries\
    \ and also the original documents (Moratanch & Chitrakala, 2017). On the other\
    \ side, most of the developed automatic evaluation methods assess the summary's\
    \ content and the evaluation of readability is done almost manually (e.g. DUC\
    \ and TAC conferences assess the readability of each summary manually) (Lloret\
    \ et al., 2017).This subsection will explore the commonly used evaluation metrics\
    \ in literature such as: 1) Precision, Recall, and F-Measure scores, 2) Recall-Oriented\
    \ Understudy for Gisting Evaluation (ROUGE) (Lin, 2004), and 3) Basic Element\
    \ (BE) (Hovy, Lin, Zhou, & Fukumoto, 2006).Precision Score Metric: it is computed\
    \ by dividing the number of sentences existing in both reference and candidate\
    \ (i.e. system) summaries by the number of sentences in the candidate summary\
    \ as in Eq. (3) (Moratanch & Chitrakala, 2017).Recall Score Metric: it is computed\
    \ by dividing the number of sentences existing in both reference and candidate\
    \ summaries by the number of sentences in the reference summary as in Eq. ( 4)\
    \ (Moratanch & Chitrakala, 2017)F-Measure Score Metric: it is a measure that combines\
    \ recall and precision metrics as in Eq. ( 5) (Moratanch & Chitrakala, 2017).\
    \ F-measure is the harmonic mean between precision and recall.ROUGE Metric: it\
    \ is the most commonly used tool for automatic evaluation of the automatically\
    \ generated summaries (Ganesan et al., 2010). ROUGE is a set of metrics and a\
    \ software package used for evaluating automatic summarization and machine translation\
    \ software in NLP (Gupta & Siddiqui, 2012). It compares the computer-generated\
    \ summaries against several human-created reference summaries (Lloret et al.,\
    \ 2017). The basic idea of ROUGE is to count the number of overlapping units between\
    \ candidate (or system) summaries and the reference summaries, such as overlapped\
    \ n-grams (Wang et al., 2017). ROUGE has been proven to be effective in measuring\
    \ qualities of summaries and correlates well to human judgments (Sun et al., 2016).\
    \ There are various ROUGE metrics as follows:ROUGE-1 (R1): it is based on the\
    \ uni-gram measure between a candidate summary and a reference summary. ROUGE-N\
    \ is an n-gram recall between a candidate summary and reference summaries. ROUGE-L\
    \ (R-L): it is based on the longest common subsequences between a candidate summary\
    \ and a reference summary. ROUGE-S* (R-S*): it measures the overlap ratio of skip-bigrams\
    \ between a candidate summary and reference summaries. ROUGE-SU* (R-SU*): it extends\
    \ ROUGE-S* by using skipbigrams and using uni-gram as a counting unit. The ''*\"\
    \ refers to the number of skip words. For example, ROUGE-SU4 permits bi-grams\
    \ to consist of non-adjacent words with a maximum of four words between the two\
    \ bi-grams words.Although ROUGE was accepted as a standard for measuring the accuracy\
    \ of a summarization model since its development, it has the disadvantage that\
    \ it only matches strings between the summaries without considering the meaning\
    \ in single words or series of words (n-grams). Therefore, evaluation methods\
    \ to address the meaning issue have been proposed which use dependency parsing\
    \ to represent the information in the candidate and reference summaries such as\
    \ Basic Elements (BE) (Hovy et al., 2006), Basic Elements with Transformations\
    \ for Evaluation (BEwT-E) (Tratz & Hovy, 2008), and DEPEVAL(summ) (Owczarzak,\
    \ 2009) methods. In BE and BEwT-E, each sentence is divided into small content\
    \ units which are called basic elements and are used to match equivalent expressions.\
    \ Each basic element is a triplet of words consisting of: 1) a head, 2) a modifier\
    \ or argument, and 3) the relation between the head and modifier. The main disadvantage\
    \ is that these methods use many language-dependent pre-processing modules for\
    \ parsing and dividing the sentences. For non-English summaries, parser resources\
    \ for the summaries' languages are required (Lloret et al., 2017).Human judgment\
    \ has the disadvantage of being subjective with a wide variance on what is considered\
    \ a ''good\" summary. This variance implies that creating an automatic evaluation\
    \ and analysis method is very difficult and challenging (Moratanch & Chitrakala,\
    \ 2017). In automatic evaluation, summaries generated by ATS systems are assessed\
    \ by automated metrics to reduce the evaluation cost. However, human efforts are\
    \ still needed by the automated evaluation metrics because they depend on the\
    \ comparison of system-generated summaries with one or more human-created model\
    \ summaries (Lloret et al., 2017)."
