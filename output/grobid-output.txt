<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic text summarization: A comprehensive survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-11">11 July 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wafaa</forename><forename type="middle">S</forename><surname>El-Kassas</surname></persName>
							<email>wafaa.elkassas@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer and Systems Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Ain Shams University</orgName>
								<address>
									<settlement>Cairo</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cherif</forename><forename type="middle">R</forename><surname>Salama</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer and Systems Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Ain Shams University</orgName>
								<address>
									<settlement>Cairo</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The American University in Cairo</orgName>
								<address>
									<settlement>Cairo</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ahmed</forename><forename type="middle">A</forename><surname>Rafea</surname></persName>
							<email>rafea@aucegypt.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The American University in Cairo</orgName>
								<address>
									<settlement>Cairo</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hoda</forename><forename type="middle">K</forename><surname>Mohamed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer and Systems Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Ain Shams University</orgName>
								<address>
									<settlement>Cairo</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic text summarization: A comprehensive survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-11">11 July 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.eswa.2020.113679</idno>
					<note type="submission">Received 11 October 2019 Revised 26 April 2020 Accepted 18 June 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-11-23T06:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic text summarization Text summarization approaches Text summarization techniques</head><p>Text summarization evaluation a b s t r a c t Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet and the various archives of news articles, scientific papers, legal documents, etc. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical with the gigantic amount of textual content. Researchers have been trying to improve ATS techniques since the 1950s. ATS approaches are either extractive, abstractive, or hybrid. The extractive approach selects the most important sentences in the input document(s) then concatenates them to form the summary. The abstractive approach represents the input document(s) in an intermediate representation then generates the summary with sentences that are different than the original sentences. The hybrid approach combines both the extractive and abstractive approaches. Despite all the proposed methods, the generated summaries are still far away from the human-generated summaries. Most researches focus on the extractive approach. It is required to focus more on the abstractive and hybrid approaches. This research provides a comprehensive survey for the researchers by presenting the different aspects of ATS: approaches, methods, building blocks, techniques, datasets, evaluation methods, and future research directions.</p><p>Ó 2020 Elsevier Ltd. All rights reserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The web resources on the Internet (e.g. websites, user reviews, news, blogs, social media networks, etc.) are gigantic sources of textual data. Besides, there is a wealth of textual content on the various archives of news articles, novels, books, legal documents, biomedical documents, scientific papers, etc. The textual content on the Internet and other archives grow exponentially on a daily basis. As a result, users consume a lot of time to find the information they are looking for. They cannot even read and comprehend all the textual content of search results. There are many repeated or unimportant portions of the resulting texts. Therefore, summarizing and condensing the text resources becomes urgent and much more important. Manual summarization is an expensive task and consumes a lot of time and effort. Practically, it is very difficult for humans to manually summarize this huge amount of textual data <ref type="bibr" target="#b206">(Vilca &amp; Cabezudo, 2017)</ref>. The Automatic Text Summarization (ATS) is the key solution to this dilemma.</p><p>The main objective of an ATS system is to produce a summary that includes the main ideas in the input document in less space <ref type="bibr" target="#b166">(Radev, Hovy, &amp; McKeown, 2002)</ref> and to keep repetition to a minimum <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>. The ATS systems help the users to get the main points of the original document without the need to read the entire document <ref type="bibr" target="#b152">(Nazari &amp; Mahdavi, 2019)</ref>. The users will benefit from the automatically produced summaries and they will save a lot of time and effort. In <ref type="bibr" target="#b121">Maybury (1995)</ref>, Maybury defined the automated summary as follows: ''An effective summary distills the most important information from a source (or sources) to produce an abridged version of the original information for a particular user(s) and task(s)". In <ref type="bibr" target="#b166">Radev et al. (2002)</ref>, Radev et al. also defined the summary as follows: ''A summary can be loosely defined as a text that is produced from one or more texts, that conveys important information in the original text(s), and that is no longer than half of the original text(s) and usually significantly less than that. Text here is used rather loosely and can refer to speech, multimedia documents, hypertext, etc.". The produced summary should be shorter in length than the input text and include the most important information in the input text <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>.</p><p>ATS systems can be classified as single-document or multidocument summarization systems. The former produces the summary from a single document while the latter generates the summary from a cluster of documents. ATS systems are designed by applying one of the text summarization approaches: extractive, abstractive, or hybrid. The extractive approach selects the most important sentences from the input text and uses them to generate the summary. The abstractive approach represents the input text in an intermediate form then generates the summary with words and sentences that differ from the original text sentences. The hybrid approach combines both the extractive and abstractive approaches. The different classifications for ATS systems are defined in Section 2. The general architecture of an ATS system; as shown in Fig. <ref type="figure" target="#fig_0">1</ref>; consists of the following tasks:</p><p>1. Pre-Processing: producing a structured representation of the original text <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010</ref>) using many linguistic techniques like sentences segmentation, words tokenization, removal of stop-words, part-of-speech tagging, stemming, etc. 2. Processing: using one of the text summarization approaches by applying a technique or more to convert the input document(s) to the summary. Section 3 defines the different ATS approaches and Section 4 explores the different techniques and building blocks to implement an ATS system. 3. Post-Processing: solving some problems in the generated summary sentences like anaphora resolution and reordering the selected sentences before generating the final summary.</p><p>ATS is one of the most challenging tasks in Natural Language Processing (NLP) and Artificial Intelligence (AI) in general. ATS research began as early as 1958 with Luhn's work <ref type="bibr" target="#b107">(Luhn, 1958)</ref> that automatically excerpts abstracts of magazine articles and technical papers. ATS poses many challenges to the research community like: 1) identification of the most informative segments in the input text to be included in the generated summary <ref type="bibr" target="#b166">(Radev et al., 2002)</ref>, 2) summarization of long single documents such as books, 3) summarization of multi-documents <ref type="bibr" target="#b56">(Hahn &amp; Mani, 2000)</ref>, 4) evaluation of the computer-generated summary without the need for the human-produced summary to be compared with, and 5) generation of an abstractive summary <ref type="bibr" target="#b56">(Hahn &amp; Mani, 2000)</ref> similar to a human-produced summary. Researchers still dream of an accurate ATS system to produce a summary that 1) covers all the main topics in the input text, 2) does not include redundant or repeated data, and 3) is readable and cohesive to the users. Since the beginning of ATS research in the late 1950s, they have been trying and are still working to improve techniques and methods for generating summaries so that the computer-generated summaries can match with the human-made summaries <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>.</p><p>Many surveys have been recently published about ATS systems and methodologies. Most surveys focus on techniques and methods of extractive summarization like <ref type="bibr" target="#b152">Nazari and Mahdavi (2019)</ref> because the abstractive summarization needs extensive NLP. In <ref type="bibr" target="#b85">Kirmani, Manzoor Hakak, Mohd, and Mohd (2019)</ref>, <ref type="bibr">Kirmani et al.</ref> define the common statistical features and some extractive methods. In <ref type="bibr" target="#b90">Kumar and Sharma (2019)</ref>, Kumar and Sharma provide a survey about the extractive ATS systems that apply fuzzy logic methods. In <ref type="bibr" target="#b145">Mosa, Anwar, and Hamouda (2019)</ref>, <ref type="bibr">Mosa et al.</ref> provide a survey on how the swarm intelligence optimization techniques are applied for ATS. They aim to motivate researchers to use swarm intelligence optimization for ATS especially for short text summarization. <ref type="bibr" target="#b193">Suleiman and Awajan (2019)</ref> provide a survey about extractive deep-learning-based text summarization. Some surveys focus on abstractive summarization like <ref type="bibr">Gupta and Gupta (2019)</ref>, <ref type="bibr" target="#b97">Lin and Ng (2019)</ref> for different abstractive methods <ref type="bibr" target="#b197">and Tandel, Mistree, and Shah (2019)</ref> for abstractive neuralnetwork-based methods. Some surveys focus on a domainspecific summarization like <ref type="bibr" target="#b17">(Bhattacharya et al., 2019;</ref><ref type="bibr" target="#b78">Kanapala, Pal, &amp; Pamula, 2019)</ref> for legal documents summarization, <ref type="bibr">(Dutta et al., 2019)</ref> for comparing extractive algorithms used in the microblogs summarization, and <ref type="bibr" target="#b70">Jacquenet, Bernard, and Largeron (2019)</ref> for abstractive deep-learning-based methods and challenges of meeting summarization. Some surveys like <ref type="bibr" target="#b53">(Gupta, Bansal, &amp; Sharma, 2019;</ref><ref type="bibr" target="#b108">Mahajani, Pandya, Maria, &amp; Sharma, 2019)</ref> provide a review about some abstractive and extractive methods. A survey about the summarization evaluation methods is presented in <ref type="bibr" target="#b42">(Ermakova, Cossu, &amp; Mothe, 2019)</ref>.</p><p>Most of the state-of-the-art surveys tackle a subset of the ATS aspects like focusing on one approach (e.g. extractive summarization), one method for a specific approach (e.g. extractive fuzzy logic method), one specific-domain ATS systems (e.g. legal documents summarization), etc. Besides, <ref type="bibr">Dutta et al. (2019)</ref> highlights that different ATS algorithms produce different summaries from the same input texts so it is very promising to combine outputs from multiple ATS algorithms to produce better summaries. Also, <ref type="bibr" target="#b108">Mahajani et al. (2019)</ref> conclude that it is recommended to benefit from the advantages of both extractive and abstractive approaches by proposing hybrid ATS systems. Therefore, the main motivation for this work is to provide a comprehensive survey about the various ATS aspects in order to help researchers enhance computergenerated summaries potentially by combining different approaches and/or methods. The main contributions of this research include:</p><p>Illustrating the classifications of the ATS systems and explaining the different ATS applications provided with examples of ATS systems proposed in the literature for each application. Providing a systematic review about the ATS approaches: extractive, abstractive, and hybrid. Each approach is applied using several methods in the literature. This survey provides: 1) the general architecture, advantages, and disadvantages of each approach and 2) the methods of each approach along with the advantages, disadvantages, and examples of ATS systems for each method. A conclusion about the state-of-the-art research recommendations for each approach is provided at the end of the approach subsection in Section3.</p><p>Providing an overview about the various components and techniques that are used to design and implement the ATS systems. The survey illustrates: 1) the text summarization operations inspired from the analysis of the human experts' operations, 2) the widely-used statistical and linguistic features that identify the important words and sentences, and 3) the text summarization building blocks. The building blocks include: 1) the common text representation models, 2) the linguistic analysis and processing techniques, and 3) the soft computing techniques (e.g. machine learning, optimization algorithms, and fuzzy logic). Providing an overview about the standard datasets besides the manual evaluation criteria and automatic evaluation tools for the computer-generated summaries. Providing a listing and categorization of the future research directions for the research community based on the existing ATS systems limitations and challenges.</p><p>The structure of this paper mirrors its main contributions: Section 2 illustrates the various classifications of ATS systems and the ATS applications. Section 3 introduces ATS approaches and explores the different methods proposed in the literature for each approach. Section 4 highlights the techniques and building blocks that are used to implement ATS systems, while Section 5 explores the benchmarking datasets and ATS evaluation methods. Finally, Section 6 concludes the paper and provides future directions for ATS research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Ats systems classifications and applications</head><p>This section explores the different ways to classify ATS systems and highlights their different applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Classifications of the ATS systems</head><p>There are many classifications for ATS systems as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. ATS systems can be classified based on any of the criteria below.</p><p>Classification based on the Input Size: Single-document or Multi-document. Input size refers to the number of source documents used to generate the target summary. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, Single-Document Summarization (SDS) uses a single text document to generate a summary and the target is to shorten the input document while keeping the important information <ref type="bibr" target="#b76">(Joshi, Wang, &amp; McClean, 2018)</ref>. In Multi-Document Summarization (MDS), the summary is generated based on a set of input documents and the target is to remove repetitive content in the input documents <ref type="bibr" target="#b76">(Joshi et al., 2018)</ref>. MDS is more complex than SDS and has some prominent issues like redundancy, coverage, temporal relatedness, compression ratio, etc. <ref type="bibr" target="#b55">(Gupta &amp; Siddiqui, 2012)</ref>.</p><p>Classification based on the Text Summarization Approach: Extractive, Abstractive, or Hybrid. The extractive text summarization approach selects the most important sentences in the input document(s) and the output summary is composed by concatenating the selected sentences. The abstractive text summarization approach represents the input document(s) in an intermediate representation and the output summary is generated from this representation. Unlike extractive summaries, abstractive summaries consist of sentences that are different than the original document (s) sentences. The hybrid text summarization approach merges between both the extractive and abstractive approaches. These approaches will be explained in more detail in section 3.</p><p>Classification based on Nature of the Output Summary: Generic or Query-Based. A generic text summarizer extracts important information from one or more input documents to provide a general sense of its contents <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017;</ref><ref type="bibr" target="#b178">Sahoo, Balabantaray, Phukon, &amp; Saikia, 2016)</ref>. A query-based summarization means that the multi-document summarizer deals with a group of homogeneous documents, taken out from a large corpus as a result of a query <ref type="bibr" target="#b178">(Sahoo et al., 2016)</ref>. Then, the generated summary includes a query-related content. A query-based summary presents the information that is most relevant to the initial search query, while a generic summary gives an overall sense of the document content <ref type="bibr" target="#b140">(Mohan, Sunitha, Ganesh, &amp; Jaya, 2016)</ref>. The querybased summary is sometimes referred to as a like query-focused, topic-focused, or user-focused summary <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>.</p><p>Classification based on the Summary Language: Monolingual, Multilingual, or Cross-Lingual. A summarization system is monolingual when the language of source and target documents is the same. A summarization system is multi-lingual when the source text is written in a number of languages (e.g. English, Arabic, and French) and the summary is also generated in these languages. A summarization system is cross-lingual when the source text is written in one language (e.g. English) and the summary is generated in another one (e.g. Arabic or French) <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>. Classification based on the Summarization Algorithm: Supervised, or Unsupervised. The supervised algorithm needs a training phase which requires an annotated training data. Human efforts are required to manually annotate the training data, so the latter is difficult to create and expensive. On the other side, the unsupervised algorithm does not require a training phase nor training data <ref type="bibr" target="#b141">(Mohd, Jan, &amp; Shah, 2020)</ref>.</p><p>Classification based on the Summary Content: Indicative or Informative. An indicative summary contains only the general idea or information about the source text <ref type="bibr" target="#b16">(Bhat, Mohd, &amp; Hashmy, 2018)</ref>. So it is used to determine what the input text is about (i.e. what topics are addressed) to alert the user about the source content <ref type="bibr" target="#b195">(Takeuchi, 2002)</ref>. The purpose of an indicative summary is to inform users about the scope of the input text to help them decide whether or not to read the original text. On the other side, an informative summary contains the important information and ideas in the original text <ref type="bibr" target="#b16">(Bhat et al., 2018)</ref> such that it covers all topics of the text <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>. The purpose of an informative summary is to cover the main contents of the original text without details <ref type="bibr" target="#b195">(Takeuchi, 2002)</ref>.</p><p>Classification based on the Summary Type: Headline, Sentence-Level, Highlights, or Full Summary. The length of the generated summaries differs based on the purpose of the ATS system. Headline generation produces a headline which is usually shorter than a sentence <ref type="bibr" target="#b34">(Dernoncourt, Ghassemi, &amp; Chang, 2018)</ref>. A sentence-level summarization generates a single sentence from the input text which is usually an abstractive sentence <ref type="bibr" target="#b34">(Dernoncourt et al., 2018)</ref>. A highlights summarization produces a telegraphic style and highly compressed summary which is usually in the form of bullet points <ref type="bibr" target="#b214">(Woodsend &amp; Lapata, 2010)</ref>. The highlights summary provides the reader with a brief overview about the main information in the input document(s) <ref type="bibr" target="#b214">(Woodsend &amp; Lapata, 2010)</ref>. Finally, a full summary generation is usually guided by the required summary length or a compression ratio.</p><p>Classification based on the Summarization Domain: General, or Domain-Specific. The general or domain-independent ATS system summarizes documents that belong to different domains. On the other side, the domain-specific ATS system is specialized to summarize documents from a certain domain (e.g. medical documents or legal documents).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Applications of the ATS systems</head><p>ATS is widely used in text mining and analytics applications such as information retrieval, information extraction, question answering, etc. ATS is used with the information retrieval techniques to enhance the capabilities of the search engines. In <ref type="bibr" target="#b200">Tuarob, Bhatia, Mitra, and Giles (2016)</ref>, <ref type="bibr">Tuarob et al.</ref> propose a search engine to search for algorithms and pseudo-codes. First, a dataset is constructed by extracting algorithms from scientific papers. Then, ATS is used to add additional textual metadata to the extracted algorithms from the scientific documents. In <ref type="bibr" target="#b220">Yulianti, Chen, Scholer, Croft, and Sanderson (2018)</ref>, Yulianti et al. use text summarization to extract answers for non-factoid queries. In <ref type="bibr" target="#b94">Li, Zhu, Ma, Zhang, and Zong (2018)</ref>, <ref type="bibr">Li et al.</ref> propose an extractive multi-modal summarization (MMS) system for asynchronous collections of text, image, audio, and video. The proposed system generates a textual summary from the aforementioned sources.</p><p>There are different text genres like news articles, novels, books, forums, blogs, emails, opinion reviews, spoken dialogues which combine text summarization with speech recognition, medical documents, legal documents, etc. <ref type="bibr">(Vodolazova, Lloret, Muñoz, &amp; Palomar, 2013a</ref>). Each ATS system supports one or more text genres as inputs; hence the ATS systems are used for different applications like news summarization, email summarization, domain-specific summarization (e.g. legal or biomedical documents summarization), etc. Examples of various ATS applications are explored below.</p><p>News Summarization: Newsblaster <ref type="bibr" target="#b122">(McKeown et al., 2002)</ref> is a text summarizer that helps users find the most desirable news to them. On a daily basis, the system automatically collects, clusters, categorizes, and summarizes news from several news sites on the Internet (e.g. CNN and Reuters) <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>. <ref type="bibr" target="#b177">Sahni and Palwe (2017)</ref>, <ref type="bibr" target="#b188">Sethi, Sonawane, Khanwalker, and Keskar (2017)</ref> are other examples of ATS systems for news summarization.</p><p>Opinion/Sentiment Summarization: Sentiment Analysis (SA) is the study of people's opinions, emotions, and judgments about events and products. ASFuL is an aspect-based sentiment summarization system <ref type="bibr" target="#b119">(Mary &amp; Arockiam, 2017)</ref>. It uses fuzzy logic to classify sentiments or opinions polarity from the product reviews as ''Strong Positive", ''Positive", ''Negative", and ''Strong Negative". It also integrates the non-opinionated sentences using the Imputation of Missing Sentiment (IMS) technique. IMS is used to reduce the neutral score in sentiment polarity. E-commerce websites are growing rapidly hence there are hundreds of reviews for any product on average. ATS is very useful in this case to summarize the user reviews. <ref type="bibr" target="#b15">Bhargava and</ref><ref type="bibr">Sharma (2017), Lovinger, Valova, and</ref><ref type="bibr" target="#b106">Clough (2019)</ref>, <ref type="bibr" target="#b136">Mirani and Sasi (2017)</ref>, <ref type="bibr" target="#b174">Roul and Arora (2019)</ref>, <ref type="bibr" target="#b217">Yadav and Chatterjee (2016)</ref>, <ref type="bibr" target="#b224">Zhou, Wan, and Xiao (2016)</ref> are additional examples of ATS systems for opinion/sentiment or user reviews summarization.</p><p>Microblog/Tweet Summarization: Social networking sites like Facebook, Twitter, etc. include millions of messages (Vijay <ref type="bibr" target="#b205">Kumar &amp; Janga Reddy, 2019)</ref>. During emergency events, microblogging sites such as ''Twitter" are important sources of real-time information because hundreds, thousands or even more of microblogs (tweets) are posted on Twitter <ref type="bibr">(Dutta et al., 2019)</ref>. Therefore, microblog summarization became very important in recent years. Examples of ATS systems for microblog or tweet summarization can be found in <ref type="bibr" target="#b23">Chakraborty, Bhavsar, Dandapat, and Chandra (2019)</ref>, <ref type="bibr" target="#b175">Rudra, Goyal, Ganguly, Imran, and Mitra (2019)</ref>, Vijay <ref type="bibr" target="#b205">Kumar and Janga Reddy (2019)</ref>.</p><p>Books Summarization: most research focus on short documents summarization. In <ref type="bibr" target="#b131">Mihalcea and Ceylan (2007)</ref>, Mihalcea and Ceylan address the problems of book summarization and introduce a specific benchmark for book summarization. ATS systems developed for short documents are not suitable for summarizing long documents such as books because: 1) the selected sentences may not cover all the book topics, 2) considering the length of documents is essential for better performance, and 3) using the sentence position feature differs in books than short documents (i.e. it may be unuseful to include the sentences located at the beginning of the book in the generated summary).</p><p>Story/Novel Summarization: In Kazantseva and Szpakowicz (2010), Kazantseva and Szpakowicz present an approach for automatic extractive summarization that uses syntactic information and shallow semantics (provided by a GATE gazetteer <ref type="bibr" target="#b31">(Cunningham, Maynard, Bontcheva, &amp; Tablan, 2002)</ref>) to produce indicative summaries focusing on three types of entities: people, locations, and timestamps of short stories. The generated summary helps the reader to decide whether to read the complete story, but it does not attempt to retell the plot for two reasons: 1) many people do not want to know what happens in a story before reading it, and 2) the summarization problem would be too complex if summarizing the full plot was required.</p><p>Email Summarization: Email messages are domain-general text, they are unstructured and not always syntactically wellformed <ref type="bibr" target="#b146">(Muresan, Tzoukermann, &amp; Klavans, 2001)</ref>. In <ref type="bibr" target="#b146">Muresan et al. (2001)</ref>, <ref type="bibr">Muresan et al.</ref> propose an ATS system that combines linguistic techniques with machine learning algorithms to extract noun phrases to generate a summary of Email messages. More examples of ATS systems for email summarization can be found in <ref type="bibr" target="#b21">Carenini, Ng, and Zhou (2007)</ref>, <ref type="bibr" target="#b201">Ulrich, Carenini, Murray, and Ng (2009)</ref>.</p><p>Biomedical Documents Summarization: In Menéndez, Plaza, and Camacho (2014), <ref type="bibr">Menéndez et al.</ref> propose an ATS system that combines genetic clustering and graph connectivity information to improve the graph-based summarization process. Genetic clustering identifies the different topics in a document, and connectivity information (i.e. degree centrality) shows the importance of the different topics. <ref type="bibr" target="#b142">Morales, Esteban, and Gerv (2008)</ref>, Nasr Azadani, Ghadiri, and Davoodijam (2018), <ref type="bibr" target="#b171">Reeve, Han, and Brooks (2006)</ref>, <ref type="bibr" target="#b172">Reeve, Han, and Brooks (2007)</ref> are other examples of ATS systems for biomedical text summarization.</p><p>Legal Documents Summarization: In Kavila, Puli, Prasada Raju, and Bandaru (2013), Kavila et al. propose an ATS system and an automatic search system for legal documents to save the time of legal experts. The summarization task identifies the rhetorical roles presenting the sentences of a legal judgment document. The search task identifies the related past cases based on the given legal query. The hybrid system uses different techniques such as keyword or key phrase matching technique and the case-based technique. <ref type="bibr" target="#b12">Anand and Wagh (2019)</ref>, <ref type="bibr" target="#b79">Kavila et al. (2013)</ref>, <ref type="bibr" target="#b128">Merchant and Pande (2018)</ref> are among examples of ATS systems for legal documents summarization.</p><p>Scientific Papers Summarization: Scientific papers are wellstructured documents that have some common characteristics like the predictable locations of typical items in a document, cue words, and a template-like structure <ref type="bibr" target="#b80">(Kazantseva &amp; Szpakowicz, 2010)</ref>. In <ref type="bibr" target="#b139">Mohammad et al. (2009)</ref>, <ref type="bibr">Mohammad et al.</ref> propose a multi-document ATS framework that generates a technical survey on a given topic from multiple research papers by merging two methods: 1) mining the structure of citations and relations among citations to get citation information, and 2) summarization techniques that identify the content of the material in both the citing and cited papers. <ref type="bibr" target="#b10">(Alampalli Ramu, Bandarupalli, Nekkanti, &amp; Ramesh, 2020)</ref> propose a summarizer to extract the problem statement from one research paper then uses it to find the related papers. <ref type="bibr" target="#b73">Jiang et al. (2019)</ref> present an abstractive deep-learningbased summarizer used for automatic survey generation. <ref type="bibr" target="#b30">Cohan and Goharian (2018</ref><ref type="bibr" target="#b104">), Lloret, Romá-Ferri, and Palomar (2011</ref><ref type="bibr">, 2013)</ref>, <ref type="bibr" target="#b117">Marques, Cozman, and Santos (2019)</ref>, <ref type="bibr" target="#b139">Mohammad et al. (2009)</ref>, <ref type="bibr" target="#b198">Teufel and Moens (2002)</ref>, <ref type="bibr" target="#b222">Zhang, Li, and Yao (2018)</ref> represent various examples of ATS systems for scientific papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Ats approaches</head><p>There are three main text summarization approaches: extractive, abstractive, or hybrid. Each approach is applied using different methods as shown in Fig. <ref type="figure">3</ref>. This section will provide a detailed overview about each of these approaches along with the methods of each approach in the literature. There are many summarization methods like graph-based, semantic-based, soft computing (SC) based, etc.</p><p>3.1. Extractive text summarization 3.1.1. Approach Fig. <ref type="figure" target="#fig_2">4</ref> shows the extractive text summarization system architecture that consists of 1) pre-processing of the input text, 2) postprocessing like: reordering the extracted sentences, replacing pronouns with their antecedents, replacing the relative temporal expression with actual dates, etc. <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>, and 3) the processing tasks as follows:</p><p>1. Creating a suitable representation of the input text to facilitate the text analysis (e.g. N-gram, bag-of-words, graphs, etc.) <ref type="bibr" target="#b76">(Joshi et al., 2018)</ref>. 2. Scoring of sentences: ranking sentences based on the input text representation <ref type="bibr" target="#b153">(Nenkova &amp; McKeown, 2012)</ref>. 3. Extraction of high-scored sentences: selecting the most important sentences from the input document(s) and concatenating them to create the summary <ref type="bibr" target="#b153">(Nenkova &amp; McKeown, 2012;</ref><ref type="bibr" target="#b225">Zhu et al., 2017)</ref>. The generated summary length depends on the preferred compression rate using a length cutoff or threshold to limit the size of the summary and preserve the same order of the generated sentences as the input text <ref type="bibr" target="#b211">(Wang, Zhao, Li, Ge, &amp; Tang, 2017)</ref>.</p><p>Advantages: The extractive approach is faster and simpler than the abstractive approach. This approach leads to a higher accuracy Fig. <ref type="figure">3</ref>. Automatic text summarization approaches and their associated methods. because of the direct extraction of sentences so readers read the summary with the exact terminologies that exist in the original text <ref type="bibr" target="#b196">(Tandel, Modi, Gupta, Wagle, &amp; Khedkar, 2016)</ref>.</p><p>Disadvantages: The extractive approach is far from the method that human experts write summaries <ref type="bibr" target="#b62">(Hou, Hu, &amp; Bei, 2017)</ref>. Drawbacks of the generated extractive summary include:</p><p>1. Redundancy in some summary sentences <ref type="bibr" target="#b62">(Hou et al., 2017)</ref>. 2. Extracted sentences can be longer than average <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>. 3. Temporal expressions conflicts in the multi-document case because extractive summaries are selected from different input documents <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>. 4. Lack of semantics and cohesion in summary sentences <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref> because of the incorrect link between sentences and non-resolved co-reference relationships <ref type="bibr" target="#b104">(Lloret, Romá-Ferri, &amp; Palomar, 2011)</ref> and ''dangling" anaphora <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>. 5. Important information spread across sentences. Conflicting information may not be covered <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>. The output summary may be unfair for input texts that consist of several topics <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>. Summaries can overcome this issue only if the summary is long enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Methods</head><p>Fig. <ref type="figure">3</ref> shows various extractive text summarization methods like statistical-based, concept-based, etc. Table <ref type="table" target="#tab_0">1</ref> illustrates the pros and cons of each method along with examples of ATS systems that apply it. In the rest of this subsection, we list and briefly describe each of these methods.</p><p>Statistical-Based Methods: These methods extract important sentences and words from the source text based on the statistical analysis of a set of features. The ''most important" sentence is defined as the ''most favorably positioned", the ''most frequent", etc. <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>. The sentence scoring steps of a statistical-based extractive summarizer include <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>: 1) selecting and calculating some statistical and/or linguistic features then assigning weights to them <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>, and 2) assigning a final score to each sentence in the document <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref> that is determined using a feature-weight equation <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref> (i.e. all the selected features' scores are computed and summed to obtain the score of each sentence).</p><p>Concept-Based Methods: These methods extract concepts from a piece of text from external knowledge bases like WordNet <ref type="bibr" target="#b134">(Miller, 1995;</ref><ref type="bibr">WordNet: An Electronic Lexical Database, 1998)</ref>, HowNet, Wikipedia, etc. The importance of sentences is then calculated based on the concepts retrieved from the external knowledge base HowNet instead of words. The sentence scoring steps of a concept-based extractive summarizer include <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>: 1) retrieving concepts of a text from the external knowledge base, 2) building a conceptual vector or graph model to show the relationship between concept and sentences, and 3) applying a ranking algorithm to score the sentences.</p><p>Topic-Based Methods: These methods rely on identifying a document's topic which is its main subject (i.e. what the document is about). Some of the most common methods for topic representations are Term Frequency, Term Frequency-Inverse Document Frequency (TF-IDF), lexical chains, topic word approaches in which the topic representation consists of a simple table and their corresponding weights <ref type="bibr" target="#b153">(Nenkova &amp; McKeown, 2012)</ref>, etc. The processing steps of a topic-based extractive summarizer include <ref type="bibr" target="#b153">(Nenkova &amp; McKeown, 2012)</ref>: 1) converting the input text to an intermediate representation that captures the topics discussed in the input text, and 2) assigning an importance score to each of the sentences in the input document according to this representation.</p><p>Sentence Centrality or Clustering-Based Methods: In these methods, a multi-document extractive summarizer identifies the most central and important sentences in a cluster such that they cover the important information related to the cluster main subject. The sentence centrality is defined using the centrality of its words. The common way to measure the word centrality is to look at the centroid of the document cluster in a vector space. The centroid of a cluster is a pseudo-document that consists of words having TF-IDF scores above a predefined threshold <ref type="bibr" target="#b41">(Erkan &amp; Radev, 2004)</ref>. The sentence scoring steps in a centroid-based summarizer include: 1) building a representative centroid by computing the TF-IDF representations of each sentence in the document <ref type="bibr" target="#b125">(Mehta &amp; Majumder, 2018)</ref>, and 2) considering a sentence as central when it contains more words from the centroid of the cluster as a measure of its closeness to the cluster centroid <ref type="bibr" target="#b41">(Erkan &amp; Radev, 2004)</ref>. The closer a sentence is to the cluster centroid the more important it is <ref type="bibr" target="#b125">(Mehta &amp; Majumder, 2018)</ref>. Clustering-based summarization considers both relevance and redundancy removal in the generated summary. The clustering algorithms are used for ATS and the sentence selection steps include <ref type="bibr" target="#b152">(Nazari &amp; Mahdavi, 2019)</ref>: 1) using a clustering algorithm to cluster the input sentences, 2) ranking and ordering clusters such that a cluster gets a high rank when it has more important words, and 3) from these clusters, selecting representative sentences for the summary.</p><p>Graph-Based Methods: These methods use sentence-based graphs to represent a document or document cluster. Using such representation has been commonly used for extractive summarization (e.g. LexRank <ref type="bibr" target="#b41">(Erkan &amp; Radev, 2004)</ref> and TextRank <ref type="bibr">(Mihalcea &amp; Tarau)</ref>). In <ref type="bibr" target="#b41">Erkan and Radev (2004)</ref>, Erkan and Radev discuss how random walks on sentence-based graphs can help in text summarization. For example, LexRank's sentence scoring steps include <ref type="bibr" target="#b211">(Wang et al., 2017)</ref>: 1) representing the document sentences using an undirected graph such that each node in the graph represents a sentence from the input text and for each pair of sentences the weight of the connecting edge is the semantic similarity between the two corresponding sentences using cosine similarity, and 2) using a ranking algorithm to determine the importance of each sentence. The sentences are ranked based on their LexRank scores in a similar way to the PageRank <ref type="bibr" target="#b19">(Brin &amp; Page, 1998)</ref> algorithm except that the LexRank graph is undirected <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>.</p><p>Semantic-Based Methods: Latent Semantic Analysis (LSA) is a commonly used semantic-based extractive ATS method. LSA is an unsupervised technique that represents text semantics based on the observed co-occurrence of words <ref type="bibr" target="#b153">(Nenkova &amp; McKeown, 2012)</ref>. The sentence scoring steps of any LSA-based extractive summarizer include (Al-Sabahi, Zhang, Long, &amp; Alwesabi, 2018): 1) creating the input matrix (term-to-sentence matrix), and 2) applying Singular Value Decomposition (SVD) to the input matrix to identify the relationships between terms and sentences. Many other semantic-based techniques are used for ATS like Semantic Role Labelling (SRL) and Explicit Semantic Analysis (ESA). In <ref type="bibr" target="#b138">Mohamed and Oussalah (2019)</ref>, Mohamed and Oussalah propose a semantic-based extractive ATS system based on the SRL and ESA along with the Wikipedia knowledge base.</p><p>Machine-Learning-Based Methods: These methods convert the summarization problem to a supervised classification problem at the sentence level. The system learns by examples to classify each sentence of the test document either as ''summary" or ''non-summary" class using a training set of documents (i.e. a collection of documents and their respective human-generated summaries). For the machine-learning-based summarizer, the sentence scoring steps include <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017</ref>): 1) extracting features from the preprocessed document (i.e. based on multiple features of sentences and words), and 2) feeding the extracted features to a neural network which produces a single value as an output score.</p><p>Deep-Learning-Based Methods: In Kobayashi, Noguchi, and Yatsuka (2015), Kobayashi et al. propose a summarization system using document level similarity based on embeddings (i.e. distributed representations of words). An embedding of a word represents its meaning. A document is considered as a bag-of-sentences and a sentence as a bag-of-words. The task is formalized as the problem of maximizing a sub-modular function defined by the negative summation of the nearest neighbors distances on embedding distributions (i.e. a set of word embeddings in a document). Kobayashi et al. conclude that the document-level similarity can determine more complex meanings than sentence-level similarity. <ref type="bibr" target="#b25">Chen and Nguyen (2019)</ref> propose an ATS system for singledocument summarization using a reinforcement learning algorithm and a Recurrent Neural Network (RNN) sequence model of encoder-extractor network architecture. The important features  <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>. It does not require any extra linguistic knowledge or complex linguistic processing and it is language independent <ref type="bibr" target="#b86">(Ko &amp; Seo, 2008)</ref>.</p><p>Some important sentences may be not included in the summary because they have less score than others. Similar sentences may be included in the summary because they have high scores. <ref type="bibr" target="#b2">(Afsharizadeh, Ebrahimpour-Komleh, &amp; Bagheri, 2018)</ref> Concept-Based The summary sentences cover several concepts.</p><p>It requires to use similarity measures in order to reduce redundancy <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>. The selected concepts (i.e. they are based on the knowledge base) affect the quality of the summary. <ref type="bibr" target="#b185">(Sankarasubramaniam, Ramanathan, &amp; Ghosh, 2014)</ref> Topic-Based The summary sentences concentrate on the various topics in the input document(s).</p><p>Sentences that do not have the highest score will not appear in the summary even if they are very related to the main topics <ref type="bibr" target="#b212">(Wang &amp; Ma, 2013)</ref>. The selected topics affect the quality of the generated summary. <ref type="bibr" target="#b177">(Sahni &amp; Palwe, 2017)</ref> Sentence Centrality or Clustering-Based</p><p>It may avoid including repeated sentences in the summary. It is suitable for multidocument summarization because it groups different sentences about the same topic in the documents <ref type="bibr" target="#b152">(Nazari &amp; Mahdavi, 2019)</ref>.</p><p>It requires prior specification of the number of clusters. The highly scored sentences may be similar, therefore redundancy removal techniques are required <ref type="bibr" target="#b125">(Mehta &amp; Majumder, 2018)</ref>. Some sentences may express more than one topic but each sentence has to be assigned to only one cluster <ref type="bibr" target="#b152">(Nazari &amp; Mahdavi, 2019)</ref>. <ref type="bibr" target="#b167">(Radev, Jing, Stys ´, &amp; Tam, 2004;</ref><ref type="bibr" target="#b174">Roul &amp; Arora, 2019)</ref> Graph-Based It enhances coherency and detects redundant information <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>. It is language-independent <ref type="bibr" target="#b150">(Nasar, Jaffry, &amp; Malik, 2019)</ref>, and domain-independent <ref type="bibr">(Nallapati, Zhai, &amp; Zhou, 2017)</ref>.</p><p>It assumes that the weights of the words are equal, so it does not consider the importance of words in the document <ref type="bibr" target="#b43">(Fang, Mu, Deng, &amp; Wu, 2017)</ref>. It does not focus on issues like the dangling anaphora problem <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>. Graphs that represent sentences as Bag of Words and use similarity measure; may fail to identify semantically equivalent sentences <ref type="bibr" target="#b82">(Khan et al., 2018)</ref>. The selected sentences are affected by the accuracy of similarity computation <ref type="bibr" target="#b211">(Wang et al., 2017)</ref>. <ref type="bibr" target="#b13">(Baralis, Cagliero, Mahoto, &amp; Fiori, 2013;</ref><ref type="bibr">Dutta, Das, Mallick, Sarkar, &amp; Das, 2019;</ref><ref type="bibr">Mallick, Das, Dutta, Das, &amp; Sarkar, 2019;</ref><ref type="bibr" target="#b130">Mihalcea, 2004;</ref><ref type="bibr" target="#b186">Sarracén &amp; Rosso, 2018)</ref> Semantic-Based LSA is a language-independent technique and generates semantically related sentences in the summary <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>.</p><p>The generated summary depends on the quality of the semantic representation of the input text. Computing SVD consumes a large time <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>. <ref type="bibr" target="#b120">(Mashechkin, Petrovskiy, Popov, &amp; Tsarev, 2011;</ref><ref type="bibr" target="#b138">Mohamed &amp; Oussalah, 2019)</ref> Machine-Learning-Based To improve the sentence selection for the summary, a large set of training data is required <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>.</p><p>Relatively-simple regression models can achieve better results than other classifiers <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>.</p><p>It requires a large data set of manually created extractive summaries such that each sentence in the original training documents can be labeled as either ''summary" or ''non-summary" <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>. <ref type="bibr" target="#b11">(Alguliyev et al., 2019;</ref><ref type="bibr" target="#b75">John &amp; Wilscy, 2013;</ref><ref type="bibr" target="#b191">Shetty &amp; Kallimani, 2017)</ref> Deep-Learning-Based The network can be trained based on the human reader's style. The features set can be changed based on the user's requirements <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>.</p><p>It requires human efforts to manually build the training data. Neural networks are slow in the training and testing phases. It is hard to define how the network generates a decision <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>. <ref type="bibr" target="#b26">(Cheng &amp; Lapata, 2016;</ref><ref type="bibr" target="#b87">Kobayashi et al., 2015;</ref><ref type="bibr">Nallapati et al., 2017;</ref><ref type="bibr" target="#b213">Warule, Sawarkar, &amp; Gulati, 2019;</ref><ref type="bibr" target="#b218">Yao, Zhang, Luo, &amp; Wu, 2018;</ref><ref type="bibr" target="#b219">Yousefi-Azar &amp; Hamey, 2017)</ref> Optimization-Based Using the strength of genetic algorithms to find the optimal weights <ref type="bibr" target="#b123">(Meena &amp; Gopalani, 2015)</ref>.</p><p>High computational time and cost. It is required to define the number of iterations for the optimization algorithms. <ref type="bibr" target="#b106">(Lovinger et al., 2019</ref></p><formula xml:id="formula_0">; Sanchez-Gomez, Vega-Rodríguez, &amp; Pérez, 2020a; Verma &amp; Om, 2019) Fuzzy-Logic-Based</formula><p>Fuzzy logic is compatible with the real world which is not a two-value (0 and 1) world <ref type="bibr" target="#b152">(Nazari &amp; Mahdavi, 2019)</ref>. It handles the uncertainties in the input as the fuzzy inference systems can produce a logical assessment in an uncertain and ambiguous environment <ref type="bibr" target="#b90">(Kumar &amp; Sharma, 2019)</ref>.</p><p>The redundancy of the selected sentences in the summary is a negative factor that may occur and affect the quality of summary <ref type="bibr" target="#b160">(Patel et al., 2019)</ref>. So, a redundancy removal technique is required in the postprocessing phase to improve the quality of the final summary. <ref type="bibr" target="#b147">(Mutlu, Sezer, &amp; Akcayol, 2019;</ref><ref type="bibr" target="#b160">Patel et al., 2019;</ref><ref type="bibr" target="#b163">Qassem, Wang, Barada, Al-Rubaie, &amp; Almoosa, 2019)</ref> are selected by a sentence-level selective encoding technique then the summary sentences are extracted.</p><p>Optimization-Based Methods: These methods convert the summarization problem to an optimization problem. For example, a generic extractive multi-document ATS system is formulated as a multi-objective problem in Sanchez-Gomez, Vega-Rodríguez, and Pérez (2020b). The sentence scoring steps of an optimization-based extractive summarizer include: 1) creating a suitable representation to the input text such as the commonly-used vector representation (i.e. each sentence in the input text is represented as a vector of words), and 2) using an optimization algorithm (e.g. A Multi-Objective Artificial Bee Colony (MOABC) algorithm) to select the summary sentences based on the required summary length limit besides one or more optimization criteria: content coverage, redundancy reduction, relevance and coherence <ref type="bibr" target="#b184">(Sanchez-Gomez et al., 2020b)</ref>. In addition, the strength of genetic algorithms in adjusting weights could be used for ATS. In <ref type="bibr" target="#b123">Meena and Gopalani (2015)</ref>, the sentence scoring steps for an extractive genetic-algorithm-based summarizer include: 1) identifying the text features from the input text like sentence location, sentence length, etc. and, 2) using the genetic algorithm to adjust weights of these features then calculating the sentences scores.</p><p>Fuzzy-Logic-Based Methods: These methods use the fuzzy logic concept for ATS. Fuzzy logic resembles the human reasoning system and provides an efficient way to represent the feature values of sentences because not everything in the world can be defined as zero and one <ref type="bibr" target="#b90">(Kumar &amp; Sharma, 2019)</ref>. The sentence scoring steps include <ref type="bibr" target="#b152">(Nazari &amp; Mahdavi, 2019)</ref>: 1) selecting a set of features for each sentence like sentence length, term weight, etc., and 2) using the fuzzy logic system (i.e. after inserting the required rules in the knowledge base of this system) to provide a score for each sentence that reflects the sentence importance. So, each sentence in the output gets a score value from 0 to 1 based on the sentence features and the predefined rules in the knowledge base.</p><p>In conclusion, using different methods together produce better summaries because this combination uses their strength and eliminates their shortcomings. Besides, using a combination of different features most probably produces better results when calculating the weights of the input sentences <ref type="bibr" target="#b152">(Nazari &amp; Mahdavi, 2019)</ref>. Many ATS systems combine multiple methods to benefit from the advantages of each method like <ref type="bibr" target="#b9">Alami, Meknassi, and En-nahnahi (2019)</ref>, <ref type="bibr" target="#b116">Mao, Yang, Huang, Liu, and Li (2019)</ref>, <ref type="bibr" target="#b141">Mohd et al. (2020)</ref>, <ref type="bibr" target="#b144">Moratanch and Chitrakala (2017)</ref>, <ref type="bibr" target="#b168">Rahman, Rafiq, Saha, Rafian, and Arif (2019)</ref>. In <ref type="bibr" target="#b144">Moratanch and Chitrakala (2017)</ref>, Moratanch and Chitrakala combine both graph-based and concept-based methods to build a summarization system. In <ref type="bibr" target="#b168">Rahman et al. (2019)</ref>, <ref type="bibr">Rahman et al.</ref> propose an extractive ATS system to summarize Bengali text using TextRank, Fuzzy C-Means, and aggregate sentence scoring methods. <ref type="bibr" target="#b141">Mohd et al. (2020)</ref> propose an extractive summarizer which uses a Distributional Semantic Model to capture the semantics of text, the K-means clustering algorithm to cluster the semantically similar sentences, and a ranking algorithm to rank sentences in each cluster. <ref type="bibr">Mohd et al.</ref> conclude that capturing semantics to be used for summarization improves the precision of the generated summaries. <ref type="bibr" target="#b9">Alami et al. (2019)</ref> propose an extractive ATS system relying on an ensemble model with a majority voting technique that combines two models: Bag-of-Words and Sentence2Vec (i.e. it represents each sentence in the input text as a vector). <ref type="bibr">Alami et al.</ref> conclude that the ensemble model usually achieves more precise results than a single model because the data of each vector is complementary to each other. <ref type="bibr" target="#b116">Mao et al. (2019)</ref> use three different methods of combining unsupervised learning with supervised learning to produce single documents summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Abstractive text summarization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Approach</head><p>Abstractive summarization needs a deeper analysis of the input text <ref type="bibr" target="#b140">(Mohan et al., 2016)</ref>. Abstractive text summarizers generate a summary by understanding the main concepts in the input document using NLP methods, then paraphrasing the text to express those concepts in fewer words and using a clear language (Al-Abdallah &amp; Al-Taani, 2017; <ref type="bibr" target="#b89">Krishnakumari &amp; Sivasankar, 2018)</ref>. This approach does not copy sentences from the original text for the summary generation <ref type="bibr" target="#b16">(Bhat et al., 2018)</ref> instead it requires the ability to make new sentences. Fig. <ref type="figure" target="#fig_3">5</ref> shows architecture of an abstractive text summarizer. It consists of the pre-processing, post-processing, and the processing tasks which include: 1) building an internal semantic representation <ref type="bibr" target="#b27">(Chitrakala, Moratanch, Ramya, Revanth Raaj, &amp; Divya, 2018)</ref>, and 2) generating a summary using natural language generation techniques to create a summary that is closer to the human-generated summaries <ref type="bibr" target="#b27">(Chitrakala et al., 2018)</ref>.</p><p>Advantages: It generates better summaries with different words that do not belong to the original text by using more flexible expressions based on paraphrasing, compression, or fusion <ref type="bibr" target="#b62">(Hou et al., 2017)</ref>. The generated summary is closer to the manual summary <ref type="bibr" target="#b194">(Sun, Wang, Ren, &amp; Ji, 2016)</ref>. Abstractive methods can further reduce the text when compared to the extractive ones <ref type="bibr" target="#b211">(Wang et al., 2017)</ref>. This higher condensation is because the production of new sentences can further reduce any redundancy eventually achieving a good compression rate <ref type="bibr" target="#b180">(Sakhare, Kumar, &amp; Janmeda, 2018)</ref>.</p><p>Disadvantages: In practice, generating a high-quality abstractive summary is very difficult <ref type="bibr" target="#b62">(Hou et al., 2017)</ref>. Good abstractive summarizers are very hard to develop as they require the use of natural language generation technology which itself is still a growing field <ref type="bibr" target="#b211">(Wang et al., 2017)</ref>. The abstractive approach needs a full interpretation of the input text in order to generate new sentences. Most abstractive summarizers suffer from the generation of repeated words and are not able to deal with out-of-vocabulary words appropriately <ref type="bibr" target="#b62">(Hou et al., 2017)</ref>. Capabilities of the abstractive summarizers are constrained by the richness of their representations. Systems cannot summarize what their representations cannot capture. In limited domains, it may be feasible to devise appropriate structures, but a general-purpose solution depends on an open-domain semantic analysis <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Methods</head><p>The abstractive ATS methods can be categorized into three categories <ref type="bibr">(Gupta &amp; Gupta, 2019)</ref>: 1) structure-based: using predefined structures (e.g. graphs, trees, rules, templates, and ontologies), 2) semantic-based: using the text semantic representation and the natural language generation systems (e.g. based on information items, predicate arguments, and semantic graphs), and 3) deep-learning-based methods. <ref type="bibr" target="#b97">Lin and Ng (2019)</ref> provide another categorization to the abstractive methods as neural-based or classical which broadly refers to any method that is not neural-based. Structure-based methods identify the most important data in the input text then use graphs, trees, rules, templates, or ontology to produce the abstractive summaries <ref type="bibr">(Gupta &amp; Gupta, 2019)</ref>. Semantic-based methods build the semantic representation of the input text by using the information-items, predicatearguments, or semantic-graphs then use a natural language generation system to produce the abstractive summaries <ref type="bibr">(Gupta &amp; Gupta, 2019)</ref>. Various abstractive text summarization methods are shown in Fig. <ref type="figure">3</ref> categorized as structure-based, semanticbased, and deep-learning-based. Table <ref type="table" target="#tab_1">2</ref> illustrates the pros and cons of each method along with examples of summarizers that applying them. We use the rest of this subsection to list and briefly describe each of these methods.</p><p>Graph-Based Methods: In Ganesan, Zhai, and Han (2010), <ref type="bibr">Ganesan et al.</ref> propose an abstractive summarizer ''Opinosis" that uses a graph model. Each node represents a word and positional information is linked to nodes. Directed edges represent the structure of sentences. The processing steps of the graph-based method in <ref type="bibr" target="#b46">Ganesan et al. (2010)</ref> include: 1) graph generation: constructing a textual graph to represent the source text, and 2) summary generation: generating the target abstractive summary. To achieve this, various sub-paths in the graph are explored and scored as follows:</p><p>1. Rank all the paths then sort their scores in descending order.</p><p>The ranking also includes the collapsed paths.</p><p>2. Eliminate duplicated (or extremely similar) paths by using a similarity measure (e.g. Jaccard). 3. Select the top few remaining paths as the generated summary, with the number of paths to be chosen controlled by a parameter, which represents summary size.</p><p>Tree-Based Methods: These methods identify similar sentences that share mutual information then accumulate these sentences to produce the abstractive summary <ref type="bibr">(Gupta et al., 2019)</ref>. The similar sentences are represented into a tree-like structure. Parsers are used to construct the dependency trees which are the most used tree-form representations for the text. To create the final summaries, some tasks are performed to process the trees like pruning, linearization (i.e. converting trees to strings), etc. <ref type="bibr">(Gupta &amp; Gupta, 2019)</ref>. <ref type="bibr" target="#b69">Kurisinkel, Zhang, and Varma (2017)</ref> propose a multidocument abstractive summarizer as follows: 1) parse the input texts in the corpus to build a set of all syntactic dependency trees, 2) extract a set of partial dependency trees (with variable sizes) from the syntactic dependency trees, 3) cluster the extracted partial dependency trees to guarantee topical diversity, and 4) use the partial trees in each cluster to form a single sentence that represents the cluster in the abstractive summary.</p><p>Rule-Based Methods: These methods require defining the rules and categories to discover the important concepts in the input text then use these concepts to produce the summary. The steps of using this method include: 1) categorize the input text based on  <ref type="bibr" target="#b81">(Khan, Salim, &amp; Farman, 2016;</ref><ref type="bibr" target="#b169">Ranjitha &amp; Kallimani, 2017)</ref>. A new sentence is created by linking all words in a word graph path <ref type="bibr" target="#b92">(Le &amp; Le, 2013)</ref>.</p><p>Word graphs do not reflect the word/phrase meaning. Diverse words/phrases that refer to the same subject are represented as different nodes, so sentences consisting of these nodes cannot be merged <ref type="bibr" target="#b92">(Le &amp; Le, 2013)</ref>. <ref type="bibr" target="#b81">(Khan et al., 2016;</ref><ref type="bibr" target="#b104">Lloret et al., 2011;</ref><ref type="bibr" target="#b169">Ranjitha &amp; Kallimani, 2017)</ref> Tree-Based The generated summaries have improved quality when using language generators because they produce less-redundant and fluent summaries <ref type="bibr">(Gupta &amp; Gupta, 2019)</ref>.</p><p>It cannot identify the relations between sentences without discovering the shared phrases between these sentences. It does not consider the context so it misses many significant phrases in the text. This method performance depends on the available parsers which limit its efficiency. It focuses on the syntax more than the semantics <ref type="bibr">(Gupta &amp; Gupta, 2019)</ref>.</p><p>(Kurisinke, Zhang, &amp; Varma, 2017)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rule-Based</head><p>The generated summaries have high information density <ref type="bibr">(Gupta &amp; Gupta, 2019)</ref>. The ability to expand the complexity and variety of the abstraction schemes and generation patterns to handle more aspects <ref type="bibr" target="#b49">(Genest &amp; Lapalme, 2012)</ref>.</p><p>Preparing the rules is a time-wasting and tedious process because the rules are hand-crafted and written manually <ref type="bibr">(Gupta &amp; Gupta, 2019)</ref>. <ref type="bibr" target="#b49">(Genest &amp; Lapalme, 2012)</ref> Template-Based It produces informative and coherent summaries and can be used for multi-document summarization as the template slots filled by the snippets extracted by the information extraction rules <ref type="bibr">(Gupta &amp; Gupta, 2019)</ref>.</p><p>The extraction rules and linguistic patterns for template slots need to be created manually. Similar information across multiple documents cannot be handled. <ref type="bibr" target="#b81">(Khan et al., 2016)</ref>. The templates are predefined so they lack diversity. Similarity and differences among the documents cannot be considered <ref type="bibr">(Gupta &amp; Gupta, 2019)</ref>. <ref type="bibr">(Embar, Deshpande, Vaishnavi, Jain, &amp; Kallimani, 2013;</ref><ref type="bibr" target="#b159">Oya, Mehdad, Carenini, &amp; Ng, 2014)</ref> Ontology-Based It focuses on documents that are related to a specific domain. It provides coherent summaries <ref type="bibr" target="#b207">(Vodolazova &amp; Lloret, 2019)</ref> and can handle uncertainties in the text easily <ref type="bibr">(Gupta &amp; Gupta, 2019)</ref>.</p><p>It needs a good ontology but preparing this ontology is a very time-consuming process <ref type="bibr">(Gupta &amp; Gupta, 2019)</ref> because it heavily requires domain experts to build the domain ontology. As such it needs more time and effort. It cannot be generalized to other domains <ref type="bibr" target="#b81">(Khan et al., 2016)</ref>. <ref type="bibr" target="#b140">(Mohan et al., 2016;</ref><ref type="bibr" target="#b157">Okumura &amp; Miura, 2015)</ref> Semantic-Based Using the SRL will help providing the semantic relationship between the phrase of words <ref type="bibr" target="#b169">(Ranjitha &amp; Kallimani, 2017)</ref>.</p><p>The generated summary depends on the quality of the semantic representation of the input text.</p><p>( <ref type="bibr" target="#b83">Khan et al., 2015)</ref> Deep-Learning-Based</p><p>Seq2seq performs very well in short text summarization <ref type="bibr" target="#b211">(Wang et al., 2017)</ref>. the terms and concepts existing in it, 2) formulate the questions based on the domain of the input text, 3) answer the questions by finding the terms and concepts in the text, and 4) fed the answers into some patterns to produce the abstractive summary. For example, questions may be like ''what is the event?", ''who did the event?", ''when the event has occurred?", ''where the event has occurred?", ''what was the impact of the event?", etc. <ref type="bibr">(Gupta &amp; Gupta, 2019)</ref>. In Genest and Lapalme (2012), Genest and Lapalme propose an architecture based on the abstraction schemes. Each abstraction scheme is designed to address a subcategory or theme that consists of content selection heuristics, Information Extraction (IE) rules, and straightforward generation patterns are designed for each scheme. All these rules are created manually. An abstraction scheme targets to answer one or more aspects and more than one scheme that can be related to the same aspect. The IE rules can detect several candidates for each aspect and the content selection module can select the best ones to send to the generation module.</p><formula xml:id="formula_1">It</formula><p>Template-Based Methods: For some domains (e.g. meeting summaries), human summaries have shared sentence structures that can be defined as templates. Based on the input text genre, the abstractive summary can be produced by using the data in the input text to fill the slots in the suitable pre-defined templates <ref type="bibr" target="#b97">(Lin &amp; Ng, 2019)</ref>. Extraction rules and linguistic patterns are used to determine the text snippets which fill the template slots <ref type="bibr">(Gupta et al., 2019)</ref>.</p><p>Ontology-Based Methods: Many documents are related to specific domains and every domain has its own information structure that can be represented by a knowledge dictionary like an ontology <ref type="bibr" target="#b143">(Moratanch &amp; Chitrakala, 2016)</ref>. The basic idea is to get the proper information from the input text to form an abstractive summary by using an ontology <ref type="bibr" target="#b157">(Okumura &amp; Miura, 2015)</ref>.</p><p>Semantic-Based Methods: These methods represent the input document(s) by a semantic representation (e.g. information items, predicate-argument structures, or semantic graphs) then fed this representation to the natural language generation system which uses the verb and noun phrases to produce the final abstractive summary <ref type="bibr">(Gupta &amp; Gupta, 2019)</ref>. In <ref type="bibr" target="#b83">Khan, Salim, and Jaya Kumar (2015)</ref>, <ref type="bibr">Khan et al.</ref> propose a multi-document abstractive summarizer that: 1) represents the input documents with predicateargument structures using SRL, 2) clusters the semantically similar predicate-argument structures across the text using a semantic similarity measure, 3) ranks the predicate-argument structures based on features weighted and optimized using a Genetic Algorithm, and 4) uses the language generation to generate sentences from these predicate-argument structures.</p><p>Deep-Learning-Based Methods: The recent success of sequence-to-sequence learning (seq2seq) makes abstractive summarization feasible <ref type="bibr" target="#b62">(Hou et al., 2017)</ref>. Seq2seq has achieved great success in various NLP tasks like machine translation, voice recognition, and dialogue systems <ref type="bibr" target="#b211">(Wang et al., 2017)</ref>. A set of RNN models based on attention encoder-decoder achieves promising results for short text summarization; however, deep learning methods still suffer from some problems such as: 1) generating repeated words or phrases, and 2) inability to deal with out-ofvocabulary (OOV) words (i.e. rare and limited-occurrence words) <ref type="bibr" target="#b62">(Hou et al., 2017)</ref>. The steps of the summarization system in <ref type="bibr" target="#b62">Hou et al. (2017)</ref> include: 1) converting the dataset into plain texts and saving the original documents (e.g. news articles) and their summaries separately, 2) applying word segmentation then using a subword model to process the data, 3) initializing the word vectors using the pre-trained Gensim toolkit <ref type="bibr" target="#b173">(Rehurek &amp; Sojka, 2010)</ref> that will be further trained in the proposed model, 4) using Tensorflow <ref type="bibr" target="#b118">(Mart et al., 2016)</ref> for implementation with one layer of bidirectional Long Short-Term Memory (LSTM) for the encoder and a unidirectional LSTM layer for the decoder. Cross-entropy is used to calculate the loss and the Adam optimizer is used to optimize the loss.</p><p>In conclusion, the recent abstractive summarization efforts mainly focus to use the deep learning models especially in short text summarization <ref type="bibr" target="#b88">(Kouris, Alexandridis, &amp; Stafylopatis, 2019)</ref>. It is recommended to combine different methods and techniques to benefit from their advantages for generating better abstractive summaries. The different ATS algorithms produce different summaries from the same input texts so it is very promising to combine outputs from multiple ATS algorithms to produce better summaries than the summaries generated by using individual algorithms <ref type="bibr">(Dutta et al., 2019)</ref>. The structure-based methods are usually used with the extractive methods for producing hybrid summaries and with the semantic-based or deep-learning-based methods to produce abstractive summaries <ref type="bibr">(Gupta &amp; Gupta, 2019)</ref>. For example, these methods can be used in the preprocessing phase to extract the important key-phrases in the input text then use other methods to generate the abstractive summary <ref type="bibr">(Gupta &amp; Gupta, 2019)</ref>. In <ref type="bibr" target="#b88">Kouris et al. (2019)</ref>, <ref type="bibr">Kouris et al.</ref> propose an abstractive ATS system that combines the semantic-based data transformations and the encoder-decoder deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Hybrid text summarization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Approach</head><p>The hybrid approach combines both the abstractive and extractive approaches. The typical architecture of a hybrid text summarizer is shown in Fig. <ref type="figure">6</ref>. It commonly consists of the following phases <ref type="bibr" target="#b16">(Bhat et al., 2018;</ref><ref type="bibr" target="#b104">Lloret et al., 2011)</ref>: 1) Pre-Processing, 2) sentence extraction (extractive ATS phase): extract the key sentences from the input text <ref type="bibr" target="#b211">(Wang et al., 2017)</ref>, 3) summary generation (abstractive ATS phase): generate the final abstractive summary by applying the abstractive methods and techniques on the extracted sentences from the first phase, and 4) Post-Processing: to ensure that the generated sentences are valid, some general rules need to be defined like <ref type="bibr" target="#b105">(Lloret, Romá-Ferri, &amp; Palomar, 2013)</ref>:</p><p>1. The minimal length for a sentence must be 3 words (i.e.</p><p>subject + verb + object). 2. Every sentence must contain a verb. 3. The sentence should not end with an article (e.g. ''a", and ''the"), a preposition (e.g. ''of"), a conjunction (e.g. ''and"), nor an interrogative word (e.g. ''who").</p><p>Advantages: Combining the advantages of both extractive and abstractive approaches. The two approaches are complementary and the overall performance of summarization is improved <ref type="bibr" target="#b211">(Wang et al., 2017)</ref>.</p><p>Disadvantages: Generating a less quality abstractive summary than the pure abstractive approach because the generated summary depends on the extracts instead of the original text.</p><p>The research community is focusing more on the extractive ATS approach using different methods and techniques, trying to Fig. <ref type="figure">6</ref>. The architecture of a hybrid text summarization system. achieve more coherent and meaningful summaries <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref> because the abstractive approach is highly complex and needs extensive NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Methods</head><p>Fig. <ref type="figure">3</ref> shows various hybrid text summarization methods. Table <ref type="table" target="#tab_3">3</ref> illustrates the pros, cons, and examples of summarizers for each method. Up to our knowledge, there are mainly two methods that have been used in the hybrid text summarization: extractive to abstractive and extractive to shallow abstractive methods. Both methods are briefly discussed next.</p><p>Extractive to Abstractive Methods: These methods start by using one of the extractive ATS methods then they use one of the abstractive text summarization methods which is applied to the extracted sentences. In <ref type="bibr" target="#b211">Wang et al. (2017)</ref>, Wang et al. propose a hybrid system for long text summarization ''EA-LTS". The system consists of two phases: 1) the extraction phase that uses a graph model to extract the key sentences, and 2) the abstraction phase that constructs an RNN based encoder-decoder and uses a pointer and attention mechanisms to generate summaries.</p><p>Extractive to Shallow Abstractive Methods: These methods start by using one of the extractive ATS methods then they use a shallow abstractive text summarization method that applies one or more of the following techniques to the extracted sentences: information compression techniques, information fusion techniques <ref type="bibr" target="#b105">(Lloret et al., 2013)</ref>, synonym replacement techniques <ref type="bibr" target="#b161">(Patil, Dalmia, Ansari, Aul, &amp; Bhatnagar, 2014)</ref>, etc. In <ref type="bibr" target="#b16">Bhat et al. (2018)</ref>, <ref type="bibr">Bhat et al.</ref> propose a single-document hybrid ATS system called ''SumItUp". The hybrid system consists of two phases as follows:</p><p>1. Extractive Sentence Selection: uses some statistical features (sentence length, sentence position, TF-IDF, noun phrase and verb phrase, proper noun, aggregate cosine similarity, and cue-phrases) and a semantic feature (emotion described by text) to generate the summary. Cosine similarity is used to remove the redundant sentences in the extractive summary. 2. Abstractive Summary Generation: the extracted sentences are fed to a language generator (i.e. a combination of WordNet, Lesk algorithm and part-of-speech tagger) to convert the extractive summary to the abstractive summary. To retain the original sequence, sentences are reordered based on their initial index.</p><p>In conclusion, the hybrid summarization approach is a promising research direction. <ref type="bibr">Mahajani et al. recommend</ref> researchers to propose hybrid ATS systems in order to benefit from the advantages of both extractive and abstractive approaches <ref type="bibr" target="#b108">(Mahajani et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Techniques and building blocks to implement the ATS systems</head><p>This section provides an overview about the different components and techniques that are used to design and implement ATS systems. First, the text summarization operations are defined.</p><p>These operations are defined based on the analysis of the human experts' operations. Second, the statistical and linguistic features are presented. These features are widely used to distinguish important words and sentences. Then, the text summarization building blocks are presented as follows: text representation models that are widely used to represent the input texts, linguistic analysis and processing techniques that are used in the different ATS phases, and the soft computing techniques that are useful in ATS implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Text summarization operations</head><p>Text summarization operations can be classified into two categories <ref type="bibr" target="#b14">(Belkebir &amp; Guessoum, 2018)</ref>: 1) single-sentence operations, and 2) multi-sentence operations. A single-sentence operation is applied to a single sentence and a multi-sentence operation is applied to two sentences at least. Text summarization operations can also be classified as either <ref type="bibr" target="#b58">(Hasler, 2007)</ref>: 1) atomic operation which cannot be further divided into other operations (e.g. insertion and deletion of words), and 2) complex operation which can be divided into other operations (e.g. replacement and reordering of words and merging of sentences). Fig. <ref type="figure" target="#fig_4">7</ref> shows the text summarization operations classified as either single-sentence or multisentence operations. Some operations can be used alone, in sequence, or in parallel to convert a source document into a summary document <ref type="bibr" target="#b74">(Jing, 2002)</ref>. Based on analyzing the operations of the human experts in <ref type="bibr" target="#b74">(Jing, 2002)</ref>, Jing defines the first 7 of the following operations:</p><p>1. Sentence Compression/Reduction: removal of unimportant parts (e.g. phrases) to shorten the original sentence. 2. Syntactic Transformation: transformation of a sentence by changing its syntactic structure (e.g. the position of the subject in a sentence may be moved from the end to the front). This operation may be used in both sentence compression and sentence combination operations. 3. Lexical Paraphrasing: replacement of phrases by their paraphrases. 4. Generalization: replacement of phrases or clauses by more general descriptions. 5. Specification: replacement of phrases or clauses by more specific descriptions. 6. Sentence Combination/Fusion: merge of two or more original sentences into a single summary sentence. 7. Sentence Reordering: change of the order of summary sentences (e.g. an ending sentence in an original text is placed at the beginning of the summary). 8. Sentence Selection: selection of one sentence from two or more similar sentences. 9. Sentence Clustering: grouping of the sentences into different clusters. This operation is very useful in multi-document summarization (e.g. identify the subject and cluster the sentences by subject <ref type="bibr" target="#b223">(Zhong et al., 2017)</ref>).</p><p>Each ATS system uses one or more of the above operations. In the literature, there are many proposed techniques and algorithms that perform these operations automatically like Linhares Pontes, Huet, Torres-Moreno, and Linhares (2020), Vanetik, Litvak, Churkin, and Last (2020) for sentence compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Statistical and linguistic features</head><p>The statistical and linguistic features are mainly used to identify the important sentences and phrases in the input document(s). Both the word level and sentence level features are used in the text summarization literature <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>. Many ATS systems in the literature are based on human-engineered features. Table <ref type="table" target="#tab_4">4</ref> lists the most common features that are clues to include a sentence or phrase in the final summary. The simplest automatic text summarizer is based on the sentence selection such that sentences are ranked for potential inclusion in the summary using a weighted combination of statistical and linguistic features <ref type="bibr" target="#b50">(Goldstein, Kantrowitz, Mittal, &amp; Carbonell, 1999)</ref>. This paradigm transforms the problem of summarization, which generally requires the ability to understand, interpret, abstract and generate a new summary document, into a simpler problem that is solved by the following steps <ref type="bibr" target="#b50">(Goldstein et al., 1999)</ref>: 1) assigning a score to each sentence based on the score of their features, and 2) concatenating the top-ranked sentences to form a summary <ref type="bibr" target="#b211">(Wang et al., 2017)</ref>. The sentence score is computed using Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ScoreðS</head><formula xml:id="formula_2">j Þ ¼ X N i¼1 F i ðS j Þ Ã W i<label>ð1Þ</label></formula><p>Where S is the sentences list, j is the sentence's index within the sentences list, F is the features set, W is the list of weights values for these features, and N is the number of features. Defining proper weights for the features plays a major role in choosing the important sentences <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref> and hence affects the quality of the generated summary. The weights assigned to a feature may differ according to the type of summary and genre of corpus or document. These weights can be optimized for specific applications and genres. For each sentence ''j" in the input document(s) sentences list ''S", the sentence score can be normalized using Equation (2) by dividing this sentence score ''Score(Sj)" by the maximum value of all sentences' scores ''MaxScore(S)".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalized ScoreðS</head><formula xml:id="formula_3">j Þ ¼ ScoreðS j Þ MaxScore S ð Þ<label>ð2Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Text representation models/schemes</head><p>Many text representation models have been used to represent the input document(s) in the processing phase of the ATS systems. These text representation models are depicted in the left-hand side of Fig. <ref type="figure" target="#fig_5">8</ref>. In the following, we briefly describe the most relevant text representation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Graph model</head><p>Graph theory has been successfully applied to represent the semantic contents of a document <ref type="bibr">(Vodolazova et al., 2013b)</ref> or the document structure, so graph modeling is widely used in document summarization. In a graph, text elements (words or sentences) are represented by nodes and edges connect the related text elements together (e.g. semantic relation) <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>. There are two types of graphs to represent text: lexical graph and semantic graph.</p><p>Lexical Graph: uses the lexical features of the text to create a graph. TextRank <ref type="bibr">(Mihalcea &amp; Tarau)</ref> and LexRank <ref type="bibr" target="#b41">(Erkan &amp; Radev, 2004</ref>) are lexical graph-based systems. They create graphs by representing sentences as nodes and the edges connecting two sentences represent the degree of content similarity between them <ref type="bibr" target="#b76">(Joshi et al., 2018)</ref>. TextRank and LexRank are very similar. The key difference between them is that TextRank computes similarity as the number of similar words between two sentences, while Lexrank uses the cosine similarity between sentences and it is adjusted for multi-document summarization <ref type="bibr" target="#b8">(Alami, El Adlouni, En-nahnahi, &amp; Meknassi, 2018)</ref>.</p><p>Semantic Graph: uses the semantic properties of the text. Semantic properties are the ontological relationship between two words (such as synonymy and hyponymy) and the relationship among a set of words representing the syntactic structure of sentences (such as the dependency tree and syntactic trees). The semantic relations between words are very important because a set of words and their way of arrangement provide a meaning. The same set of words arranged in different ways has a different meaning <ref type="bibr" target="#b76">(Joshi et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Vector model</head><p>There are many models of vector representation like Bag-of-Words, Vector Space Model, and Word Embedding.</p><p>Bag-of-Words Model (BoW) <ref type="bibr" target="#b57">(Harris, 1954)</ref>: each sentence in the input text is represented as an N-dimensional vector, where (N-1) is the number of all possible words in the text language <ref type="bibr" target="#b41">(Erkan &amp; Radev, 2004</ref>). An entry ''UNK" is added to the vector to represent any out-of-vocabulary words (Finegan-Dollak, 2018). For each word in the sentence, its entry value in the BoW vector is the number of occurrences of the word in the sentence multiplied by the IDF of the word <ref type="bibr" target="#b41">(Erkan &amp; Radev, 2004)</ref>. The BoW representation has some weaknesses such as (Finegan-Dollak, 2018): 1) it does not include term dependency and has a high dimensionality <ref type="bibr" target="#b35">(dos Santos et al., 2018)</ref>, 2) it is very sparse because most of the values in the vector are zeros, 3) it ignores syntax (e.g. ''The dog chased the cat" and ''The cat chased the dog" have the same vectors), 4) it ignores the meaning of words (e.g. the dogs and cats are and the chasing is a type of movement), and 5) inaccurate semantic representation of text because word order and arrangement are ignored <ref type="bibr" target="#b84">(Kim, Park, Lu, &amp; Zhai, 2012)</ref>.</p><p>Vector Space Model (VSM) <ref type="bibr" target="#b181">(Salton, Wong, &amp; Yang, 1975)</ref> or Term Vector Model: it is a classical form of document representation that represents text documents as vectors <ref type="bibr" target="#b66">(Ibrahim, Elghazaly, &amp; Gheith, 2013)</ref>. VSM is commonly used in Information Retrieval (IR) by representing documents and queries as vectors of weights. A document d consists of a set of terms (t 1 , t 2 . . .t n ) where each term is a word that exists in the document, and n is the total number of different words. Each term t has a corresponding weight w <ref type="bibr" target="#b135">(Mingzhen &amp; Yu, 2009)</ref> such that each weight measures the significance of the term in the document or query respectively. The weights are computed based on the frequency of the terms in the query or the document (e.g. TF-IDF weights) <ref type="bibr" target="#b126">(Melucci, 2009)</ref>.</p><p>The great advantage of VSM is the simplicity to search for documents or compare documents by using one of the similarity or distance measures like the Spearman distance, cosine measure, Euclidean distance, Vector inner product, Hamming distance, Correlation distance, or Jaccard distance <ref type="bibr" target="#b135">(Mingzhen &amp; Yu, 2009)</ref>. At retrieval time, the documents are ranked based on the cosine similarity values between the document vectors and the query vector and returned to the user from the highest to the lowest values <ref type="bibr" target="#b126">(Melucci, 2009)</ref>. The same concept is used for query-based text summarization and clustering the similar sentences where text parts (e.g. title, query, and sentences) are represented as vectors. The bag-of-words model is the most popular VSM approach due to its simplicity and general applicability <ref type="bibr" target="#b35">(dos Santos et al., 2018)</ref>.</p><p>Word Vector or Word Embedding: neural networks are used for learning vector representations of words. For example, different methods are used to get the word embedding (Mogren, Kageback, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Explanation</head><p>Term Frequency (TF) In single-document summarization, the TF of a word is the number of times it occurs in the document. In multi-document summarization, the TF of a word is computed by dividing the number of times it occurs in all the documents by the number of documents. TF helps to identify the most significant concepts or topics in the input document(s) by identifying the most frequent words <ref type="bibr" target="#b104">(Lloret et al., 2011;</ref><ref type="bibr">Vodolazova, Lloret, Muñoz, &amp; Palomar, 2013b)</ref>. Inverse Document Frequency (IDF)</p><p>IDF is used to measure how much information the word provides across all documents in the cluster. IDF values are close to zero for the common words like articles (e.g. ''a" and ''the") and IDF values are higher for rare words (e.g. medical terms and proper nouns) <ref type="bibr" target="#b41">(Erkan &amp; Radev, 2004)</ref>. IDF is computed by dividing the total number of documents in a cluster by the number of documents that contain the word and then taking the logarithm of that quotient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TF-IDF</head><p>The topics of the document cluster are identified by the words with higher TF-IDF values <ref type="bibr" target="#b196">(Tandel et al., 2016)</ref>. TF-IDF of a certain word is computed by multiplying the TF and IDF values of this word. Code Quantity Principle (CQP)</p><p>The most important information within a text is expressed by a high number of units (i.e. words or noun phrases) <ref type="bibr" target="#b104">(Lloret et al., 2011)</ref>. CQP proves the existence of a proportional relationship between how important the information is, and the number of coding elements it has. A coding element can vary depending on the desired granularity (e.g. syllables or noun-phrases) <ref type="bibr" target="#b102">(Lloret &amp; Palomar, 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noun and Verb Phrases</head><p>If a sentence contains noun and verb phrases, it is considered an important sentence and is included in the generated summary as it contains valuable information <ref type="bibr" target="#b85">(Kirmani et al., 2019)</ref>. Content Word (Keyword) Key expressions that appear in an article <ref type="bibr" target="#b195">(Takeuchi, 2002)</ref>. Content words or Keywords are usually nouns and determined using the TF-IDF measure <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>. Title Word</p><p>The frequency of a keyword's appearance in a title or headline of an article <ref type="bibr" target="#b195">(Takeuchi, 2002)</ref>. The resemblance of a sentence to the title <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>. Sentences containing words that appear in the title indicate the theme of the document <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proper Noun</head><p>Presence of a proper noun (name entity) in the sentence <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>. Sentences containing proper nouns (e.g. name of a person, place, and concept) have greater chances to be included in the summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cue-Phrase</head><p>Sentences containing cue phrases are most likely to be in the summary: ''argue", ''in conclusion", ''this letter", ''this paper", ''summary", ''purpose", ''develop", ''propose", ''attempt", etc. <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010;</ref><ref type="bibr">Vodolazova et al., 2013b)</ref> The occurrence of Non-Essential Information Some words are indicators or markers of non-essential information like: ''because", ''furthermore", and ''additionally". These words usually occur at the beginning of a sentence. This is a binary feature: the value is ''true" if the sentence contains at least one of these words, and ''false" otherwise <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Biased Word</head><p>The sentence is important if it includes one word or more from a biased word list (i.e. a predefined list that may contain domainspecific words) <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Font based</head><p>Sentences containing words that appear in upper case (e.g. acronyms or proper names), italics, bold, or underlined fonts are usually more important and are included in the summary <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positive Keyword</head><p>The frequency count of positive words in the sentence <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negative Keyword</head><p>The frequency count of negative words in the sentence <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Numerical Data</head><p>Presence of numerical data in the sentence <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence Location (Position of</head><p>Sentence)</p><p>The position of a sentence in an article or in a paragraph <ref type="bibr" target="#b195">(Takeuchi, 2002)</ref>. Sentences located in the beginning or end of the text are usually considered to be more important and are selected for the final summary <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010;</ref><ref type="bibr">Vodolazova et al., 2013b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence Length</head><p>The relative length of the sentence. Sentence with the length below some predefined threshold can be automatically ignored <ref type="bibr">(Vodolazova et al., 2013b)</ref>. Very short and very long sentences are usually not selected in the summary <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>. Sentiment Features (Emotions described by Text)</p><p>Emotion described by text is a semantic feature. Sentences that include implicit emotional content are important to the writer and should be added to the summary <ref type="bibr" target="#b16">(Bhat et al., 2018)</ref>. There is several emotion classes that are used to define emotions in the input text: positive, negative, joy, fear, hate, surprise, disgust, etc. <ref type="bibr" target="#b85">(Kirmani et al., 2019)</ref>. Sentence Reference Index (SRI) SRI gives more weight to a sentence that precedes a sentence containing a pronominal reference. Using a list of pronouns, if a sentence contains a pronoun then the weight of the preceding sentence is increased <ref type="bibr" target="#b55">(Gupta &amp; Siddiqui, 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence-to-Sentence Cohesion</head><p>The similarity with other sentences <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>. For each sentence S, compute the similarity between S and each of the other document sentences then sum all these similarity values to get this feature value for S <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence-to-Centroid Cohesion</head><p>For each sentence S, compute the vector representing the centroid of the document (i.e. the arithmetic average over the corresponding coordinate values of all the sentences of the document) then compute the similarity between the centroid and each sentence to get this feature value for each sentence <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept Similarity of a Sentence</head><p>The concept similarity of a sentence is the number of synonym sets (synsets) of query words matching with words in the sentence. The set of synsets obtained from WordNet <ref type="bibr">(Wordnet, 2020)</ref> is used to assign concept similarity weight to a sentence <ref type="bibr" target="#b55">(Gupta &amp; Siddiqui, 2012)</ref>.</p><p>&amp; Dubhashi, 2015) like the Word2Vector model proposed by <ref type="bibr" target="#b133">Mikolov et al. in Mikolov, Corrado, and Dean (2013)</ref>. Each word embedding (i.e. distributed representation of a word) is a realvalued vector in the Euclidean space that corresponds to the ''meaning" of the word <ref type="bibr" target="#b87">(Kobayashi et al., 2015)</ref>. The word vectors are averaged or concatenated to predict the next word in a context <ref type="bibr" target="#b211">(Wang et al., 2017)</ref>. Recently, new related types of text representations have emerged such as sentence, paragraph, or document embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">N-gram model</head><p>N-gram is perfect for multi-language operations because it requires no linguistic preparations (e.g. stop word removal or stemming). N-gram is a set of words or characters that contains N elements. Word N-grams are a sequence of one word (unigrams), two words (bi-grams), three-word (tri-grams) or any other N-grams <ref type="bibr" target="#b1">(Abdolahi &amp; Zahedh, 2017)</ref>. Each word can be represented as a set of character N-grams. For example, the word ''TEST" could be represented with the following N-grams <ref type="bibr" target="#b22">(Cavnar, 1994)</ref>:</p><p>1. bi-grams: _T, TE, ES, ST, T_ 2. tri-grams: _TE, TES, EST, ST_ 3. quad-grams: _TES, TEST, EST_ Where the underscore character represents a leading or tailing space. Similar words will share a large number of character Ngrams. For example, the words ''RETRIEVE", ''RETRIEVING", and ''RETRIEVAL" share the bi-grams: _R, RE, ET, TR, RI, IE, and EV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4.">Topic model</head><p>A document is represented as a combination of topics and each topic is a probability distribution over words <ref type="bibr" target="#b192">(Singh, Devi, &amp; Mahanta, 2017)</ref>. A topic model is a generative probabilistic model that can be used to identify topics of the input text represented as word distributions. A word distribution represents a topic by appointing high probabilities to words that portray a topic (e.g. in reviews of iPhone, a topic about battery life may have high prob-abilities for words like ''hour", ''battery", and ''life") <ref type="bibr" target="#b84">(Kim et al., 2012)</ref>. The two basic representative topic modeling approaches are Latent Dirichlet Analysis (LDA) <ref type="bibr" target="#b18">(Blei, Ng, &amp; Jordan, 2003;</ref><ref type="bibr" target="#b84">Kim et al., 2012)</ref> and Probabilistic Latent Semantic Analysis (PLSA) <ref type="bibr" target="#b61">(Hofmann, 1999)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5.">Meaning representations (MR) model</head><p>Commonly used general-purpose meaning representations include Lambda Calculus and Abstract Meaning Representation.</p><p>Lambda Calculus: represents meanings as functions applied to variables and constants. For example: the sentence ''what states border Texas" may be represented as kx: state(x) ^borders(x; Texas), x is a variable, state() is a function that applies to a single entity, and borders() is a function that defines a relationship between two entities. Lambda calculus can use higher-order functions, which are functions that take other functions as inputs. It can incorporate quantifiers and represent all of the first-order logic. Lambda calculus is not easy to write, so using it to generate anything more than toy datasets is difficult <ref type="bibr" target="#b44">(Finegan-Dollak, 2018)</ref>.</p><p>Abstract Meaning Representation (AMR): represents the sentences as rooted directed graphs with labels for the edges and leaf nodes. It integrates PropBank frames, modality, co-reference, reification, negation, and other concepts from a variety of views on what is important in semantics. Most of the research on AMR has focused on the parsing of the English sentences to AMR. Recently, many research work explored the usefulness of AMR in many applications such as ATS, headline generation, machine comprehension, and question answering (Finegan-Dollak, 2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Linguistic analysis and processing techniques</head><p>The linguistic properties of the original document affect the quality of the generated summary <ref type="bibr">(Vodolazova et al., 2013b)</ref>. There are many linguistic analysis and processing techniques that are widely used in ATS <ref type="bibr">(Vodolazova et al., 2013b)</ref>. These techniques are also illustrated in near the center of Fig. <ref type="figure" target="#fig_5">8</ref> under the ''Linguistic Analysis and Processing Techniques" subtree. Stanford CoreNLP  <ref type="bibr" target="#b115">(Manning, Surdeanu, Bauer, Finkel, Bethard, &amp; McClosky, 2014</ref>) is a commonly used tool that provides many of the common core NLP steps like words tokenization and co-reference resolution. In the following, we describe the most relevant linguistic analysis and processing techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Pre-processing techniques</head><p>There many pre-processing techniques like noise removal, sentence word tokenization, etc. Most of these techniques are usually used in the pre-processing phase of an ATS system.</p><p>Noise Removal: removes unnecessary text from the input document like the header, footer, etc. <ref type="bibr" target="#b55">(Gupta &amp; Siddiqui, 2012)</ref> Sentence Segmentation: divides the text into sentences <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>. Splitting sentences by using end markers like ''.", ''?", or ''!" is not suitable in many cases. If we rely on these markers, words like ''e.g.", ''i.e.", ''4.5", ''Mr.", ''Dr.", or ''etc." lead to the false identification of sentence boundaries. To solve this problem, simple heuristics and regular expressions are used <ref type="bibr" target="#b55">(Gupta &amp; Siddiqui, 2012)</ref>.</p><p>Removal of Punctuation Marks: punctuation marks are considered as noisy terms in the text. So, removing them is very helpful before executing most NLP tasks <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>.</p><p>Word Tokenization: breaks the text into separate words. Words are separated by white space, comma, dash, dot, etc. <ref type="bibr" target="#b55">(Gupta &amp; Siddiqui, 2012)</ref> Named Entity Recognition (NER): identifies words of the input text as names of things (i.e. person name, location name, company name, etc.).</p><p>Removal of Stop-Words: stop-words are words that occur frequently in the text like articles, pronouns, prepositions, auxiliary verbs, and determiners. They are removed because they do not add any useful meaning to the analysis <ref type="bibr" target="#b72">(Jaradat &amp; Al-Taani, 2016)</ref> and have no effect in selecting the important sentences <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>.</p><p>Stemming: reduces the words with the same root or stem to a common form by removing the variable suffixes <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017;</ref><ref type="bibr" target="#b114">Manning, Raghavan, &amp; Schütze, 2008)</ref> like ''es" and ''ed". The purpose of stemming is to obtain the stem or radix of each word to emphasis on its semantics <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>.</p><p>Part-Of-Speech (POS) Tagging: assigns POS tags (e.g. verb, noun, etc.) to words in a sentence.</p><p>Frequency Computation: the frequency of words is computed and normalized by dividing it by the maximum frequency of any word in the document <ref type="bibr" target="#b55">(Gupta &amp; Siddiqui, 2012)</ref>. Most frequent words are very important to help in the selection of the most important sentences in the original document(s). By analyzing the human-made summaries, they are very likely to include the high-frequency words from the original documents <ref type="bibr" target="#b102">(Lloret &amp; Palomar, 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Parsing techniques</head><p>There are several uses of parsing techniques in the ATS processing phase like constructing the text graph models and in the postprocessing phase like sentence compression, sentences merging, etc. There are many parsing techniques like syntactic parsing, text chunking, semantic parsing, and shallow semantics.</p><p>Syntactic Parsing: refers to the task of recognizing a sentence and assigning a syntactic structure to it. The standard way to represent the syntactic structure of a grammatical sentence is as a syntax tree (or a parse tree) which is a representation of all the steps in the derivation of the sentence from the root node (i.e. each internal node in the tree represents an application of a grammar rule) <ref type="bibr" target="#b68">(Indurkhya and Damerau, 2010)</ref>. Parse trees are useful in grammar checking: a sentence that cannot be parsed may have grammatical errors (or at least is hard to read) <ref type="bibr" target="#b77">(Jurafsky &amp; Martin, 2017)</ref>. Constructing the syntactic structure of a sentence is important to understand it. Without this, it is very challenging for language users to specify that sentences with the same words and different word orders have different interpretations (e.g. ''The woman sees the man" and ''The man sees the woman") or explain why a sentence like ''The hunter killed the poacher with the rifle" has two possible interpretations <ref type="bibr" target="#b93">(Levelt &amp; Caramazza, 2007)</ref>.</p><p>Text Chunking: refers to a complete partitioning of a sentence into chunks of different types like verb groups, noun groups, etc. Full parsing is not very robust and expensive hence text chunking plays an important role in NLP. Partial parsing is more robust, much faster, and sufficient for many applications like question answering, information extraction, etc. Besides, it can be used as the first step for the full parsing process. Text chunking is considered as a shallow parsing technique <ref type="bibr" target="#b109">(Maiti, Garain, Dhar, &amp; De, 2015)</ref>.</p><p>Semantic Parsing: refers to the task of converting natural language text to a complete and formal meaning representation <ref type="bibr" target="#b29">(Clarke, Goldwasser, Chang, &amp; Roth, 2010)</ref>. Some researchers have focused on general-purpose meaning representations such as lambda calculus, AMR, and Prolog, while others have focused on more task-specific meaning representations <ref type="bibr" target="#b44">(Finegan-Dollak, 2018)</ref>.</p><p>Shallow Semantics: represents just a small portion of the semantic information about the sentence. Semantic Role Labeling is a main example. SRL labels the constituents of a sentence with their semantic roles (e.g. to identify the agent and patient of a verb). In the labeled sentence S1 '' <ref type="bibr">[John AGENT</ref> ] ate [the fish PATIENT ]-", John is the agent who did the eating. The sentence S2 ''[The fish AGENT ] ate <ref type="bibr">[John PATIENT</ref> ]" has a different meaning as the fish did the eating. The sentence S3 ''[The fish PATIENT ] was eaten by <ref type="bibr">[John AGENT</ref> ]" is syntactically different from S1 while being semantically equivalent to it. As such S3 is a paraphrasing of S1 (Finegan-Dollak, 2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">Semantic-based techniques</head><p>There are many semantic-based techniques like word sense disambiguation, anaphora resolution, etc. The use of these techniques covers all the ATS system phases as follows: 1) in the preprocessing, textual entailment and word sense disambiguation techniques are used, 2) in the processing phase, the latent semantic analysis and lexical chain techniques are used, and 3) in the postprocessing phase, word sense disambiguation, anaphora resolution, and textual entailment techniques are used.</p><p>Word Sense Disambiguation (WSD): identifies the proper meaning of the given word (i.e. computationally find the correct sense of the ambiguous words using the context in which they occur). For example: the word ''bank" has many meanings in English. Such words with multiple meanings are called polysemous words. WSD is the process of finding out the exact meaning of the polysemous word <ref type="bibr" target="#b190">(Sheth, Popat, &amp; Vyas, 2018)</ref>.</p><p>Anaphora Resolution: analyzes the pairs of nouns, pronouns, and proper nouns in a document. A powerful anaphora resolution tool relates pronouns to their nominal antecedents. It is used in all the summarization methods that depend on term overlap, from the simple term frequency to latent semantic analysis <ref type="bibr">(Vodolazova et al., 2013b)</ref>. A common problem in the extractive ATS is the anaphora ''dangling" in the sentences that contain pronouns while missing their referents when extracted out of context. Moreover, stitching together decontextualized extracts may mislead the interpretation of anaphors hence cause an inaccurate representation of the source information <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>.</p><p>Latent Semantic Analysis <ref type="bibr" target="#b33">(Deerwester, Dumais, Furnas, Landauer, &amp; Harshman, 1990)</ref>: excerpts hidden semantic structures of words and sentences. LSA is an unsupervised learning approach so it does not require any training data. In LSA, a termby-sentence matrix representation is created from the input document(s). LSA gets information such as words frequently occur together and words that are commonly used in different sentences. A high number of common words among the sentences means that the sentences are semantically related. is a method that is applied to the term-by-sentence SVD is used to find out the between words and sentences which has the competence of noise reduction that helps to improve accuracy. SVD is applied to document word matrices to group documents that are semantically related despite lacking common words. The set of words that result in the connected text is also connected in the same dimensional space <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>.</p><p>Textual Entailment (TE): determines if the meaning of one text snippet (the hypothesis) can be inferred by another one (the text) <ref type="bibr" target="#b102">(Lloret &amp; Palomar, 2009</ref>) so it is used to capture the semantic inference between text fragments <ref type="bibr">(Vodolazova et al., 2013b)</ref>. If two sentences contain a true entailment relationship, they are considered equivalent and the one which is entailed can be discarded <ref type="bibr" target="#b104">(Lloret et al., 2011)</ref>. There are different uses of the TE in ATS systems: 1) in the process of the final summary evaluation (i.e. in a set of generated summaries, TE is used to decide which summary of them can be best deduced from the original document), 2) in the segmentation of the input by using different algorithms that employ TE, and 3) in the process of summary generation to avoid redundant information appearing in the final summary <ref type="bibr" target="#b102">(Lloret &amp; Palomar, 2009)</ref>. TE is often applied to eliminate the semantic redundancy of the generated summary <ref type="bibr">(Vodolazova et al., 2013b)</ref>.</p><p>Lexical Chain: represents the semantic content that may cover a small or big part of the text. The coverage and size of a lexical chain indicate the accuracy of the lexical chain to represent the semantic content of the text. A lexical chain contains a set of words (word senses) that are semantically related in the text. Lexical chains are constructed by using relationships between word senses. So it is required to know the word senses and semantic relations between words. WordNet is a database that provides this type of information: synonym sets, hyponym/hypernym, and meronym trees. For each lexical chain, the number of semantic relations and the number of words among the words can be different. Building a lexical chain is an exhaustive method because one word can have many senses and it is required to select its correct sense of the word. Lexical chains are used in text summarization and keyword extraction (i.e. keywords are short forms and condensed versions of the documents and their summaries) <ref type="bibr" target="#b40">(Ercan &amp; Cicekli, 2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4.">Discourse analysis</head><p>Discourse relations in the text represent connections between sentences and parts in a text <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>. It is essential to determine the overall discourse structure of the text for producing a coherent and fluent summary (i.e. contains the flow of the author's argument) then being able to remove unrelated sentences to the context of the text <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>. Mann and Thompson proposed the Rhetorical Structure Theory (RST) <ref type="bibr" target="#b113">(Mann William &amp; Thompson Sandra, 1988)</ref> to act as a discourse structure. RST is one of the well-known models for text structure representation and is mainly used to represent the coherence of texts. Using RST, text can be divided into sub-parts forming a hierarchical structure. Every sub-part has a relationship to another sub-part with one of the rhetorical relation types (e.g. Motivation, Contrast, and Elaboration). These relations form the overall coherence structure of the text. A small number of defined rhetorical relations can be used to explain the relationships among a wide range of texts <ref type="bibr" target="#b195">(Takeuchi, 2002)</ref>. RST has two main concepts <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>: 1) coherent texts contain few units connected together by rhetorical relations, and 2) there must be some relations between various parts of the coherent texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.5.">Sentence similarity</head><p>The similarity techniques are widely used to measure the similarity between sentences or texts in general and can be classified into three categories <ref type="bibr" target="#b210">(Wali, Gargouri, &amp; Ben Hamadou, 2017)</ref>: syntactic similarity, semantic similarity, or hybrid methods. In the following, we briefly describe each of these.</p><p>Syntactic Similarity Methods: string matching, word order, and word co-occurring are counted to compute the syntactic similarity. Calculating the co-occurring words in a string sequence may not always be useful because sentences may be very similar while having rare co-occurring words. For literal similarity, Levenshtein distance (edit distance) is a string metric that can be used for measuring the difference between two sequences. Levenshtein distance between two words is the minimum number of singlecharacter edits (e.g. insertions, deletions, or substitutions) required to change one word to the other <ref type="bibr" target="#b210">(Wali et al., 2017;</ref><ref type="bibr" target="#b211">Wang et al., 2017)</ref>.</p><p>Semantic Similarity Methods: use the semantic nets like WordNet, the vector space model, and the statistical corpus to compute the semantic similarity between words using different known measures. The semantic-based methods are limited to compute the sentence similarity based only on the semantic similarity between words. The syntactic information and other semantic knowledge (e.g. semantic class and thematic roles) are not used. For example: after training the sentence vectors, they are used as features for the sentences. The semantic similarity between two sentences is simply obtained by calculating the cosine similarity of their vectors <ref type="bibr" target="#b210">(Wali et al., 2017;</ref><ref type="bibr" target="#b211">S. Wang et al., 2017)</ref>.</p><p>Hybrid Methods: use both semantic and syntactic knowledge. The main disadvantage of these hybrid methods is that the semantic measurement is isolated from the syntactic measurement <ref type="bibr" target="#b210">(Wali et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.6.">Natural language generation</head><p>Natural Language Generation (NLG) is the task of generating texts from the input information like texts, knowledge, or images. It is a challenging task because it requires understanding the input information then organizing the text that will be generated. Simply, NLG requires to answer the following questions: ''what to say?" and ''how to say it?" <ref type="bibr" target="#b95">(Li, Sun, &amp; Li, 2019)</ref>. Generating good texts is highly dependent on the good representation and understanding of the input information. NLG has been used in many applications like text summarization, descriptions of museum artifacts, auto-completion, dialog systems, auto-paraphrasing, question answering, machine translation <ref type="bibr" target="#b91">(Kurup &amp; Narvekar, 2020)</ref>. NLG systems are widely used in the abstractive text summarization methods to generate the final abstractive summary with sentences different than the original text sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Soft computing techniques</head><p>Soft computing solves complex problems by manipulating the uncertainty and imprecision in the decision making practices. Soft computing is guided by a main principle of ''exploit the tolerance for imprecision, uncertainty, partial truth, and approximation to achieve tractability, robustness and low solution cost" <ref type="bibr" target="#b170">(Rao &amp; Svp Raju, 2011)</ref>. Many soft computing techniques have been used as building blocks in the ATS systems such as machine learning algorithms, fuzzy logic system, genetic algorithm, etc. All these techniques are not competitive but complementary to each other, so each of these techniques can be used individually or with each other to get better summarization results <ref type="bibr" target="#b67">(Ibrahim, 2016)</ref>. For example, ''neurofuzzy" system is an effective combination that merges between the neural networks and fuzzy logic techniques <ref type="bibr" target="#b170">(Rao &amp; Svp Raju, 2011)</ref>. This survey cannot cover all soft computing techniques because there exists a huge number algorithms. Only the most commonly used are illustrated in the right-hand side of Fig. <ref type="figure" target="#fig_5">8</ref>. In the following, we provide a brief description each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1.">Machine learning algorithms</head><p>Machine learning algorithms are widely used in ATS systems like <ref type="bibr">Alguliyev, Aliguliyev, Isazade, Abdi, and Idris Shetty and Kallimani (2017)</ref>, Yousefi-Azar and Hamey Machine learning algorithms are categorized as: supervised, unsupervised, or semi-supervised.</p><p>Supervised Learning Algorithms: require a large amount of labeled or annotated data as input to a training stage <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>. Commonly-used supervised learning algorithms include: Support Vector Machine (SVM), Naïve Bayes Classification, Mathematical Regression, Decision Trees, and Artificial Neural Networks (ANN).</p><p>Unsupervised Learning Algorithms: do not require any training data. They try to discover the hidden structure in unlabeled data. These techniques are therefore suitable for any newly observed data without any required modifications. Commonlyused techniques of that type include clustering, and Hidden Markov Model (HMM). When used for ATS, these systems access only the target documents and apply heuristic rules to extract highly relevant sentences to generate a summary.</p><p>Semi-Supervised Learning Algorithms: require both labeled and unlabeled data to generate an appropriate function or classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2.">Optimization algorithms</head><p>Optimization algorithms have been widely used for ATS systems like Sanchez-Gomez, Vega-Rodríguez, and Pérez (2018) and <ref type="bibr" target="#b11">Alguliyev et al. (2019)</ref>. The most common algorithms are the genetic algorithm and particle swarm optimization.</p><p>Genetic Algorithm (GA): A GA is a search based optimization algorithm inspired by the analogy of evolution and population genetics. The GA is effective in searching for very large and varied spaces in a wide range of applications <ref type="bibr" target="#b72">(Jaradat &amp; Al-Taani, 2016)</ref>. It optimizes initially-random solutions by applying natural evolution operations like: selection, mutation, and crossover on them <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>. <ref type="bibr" target="#b4">Al-Radaideh and Bataineh (2018)</ref>, <ref type="bibr" target="#b24">Chatterjee, Mittal, and</ref><ref type="bibr">Goyal (2012), García-Hernández and</ref><ref type="bibr" target="#b47">Ledeneva (2013)</ref>, <ref type="bibr" target="#b156">Neri Mendoza, Ledeneva, and García-Hernández (2019)</ref> show examples of ATS research making use of GAs. The genetic algorithm steps include <ref type="bibr" target="#b67">(Ibrahim, 2016)</ref>:</p><p>1. Initialization: creating an initial population randomly. 2. Evaluation: evaluating each member of the population and assessing its fitness based on how close it matches the preferred requirements. 3. Selection: selecting some individuals while favoring those with higher fitness. 4. Crossover: creating new individuals by combining the features of the selected individuals. At the end of this step, it is expected that the created individuals are closer to the preferred requirements. 5. Repeat steps 2 to 5 until the termination condition is reached.</p><p>Particle Swarm Optimization (PSO): PSO is one of the most powerful bio-inspired algorithms used to obtain an optimal solution <ref type="bibr" target="#b32">(Dalal &amp; Malik, 2018)</ref>. PSO algorithm is inspired by the social movement of birds <ref type="bibr" target="#b152">(Nazari &amp; Mahdavi, 2019)</ref>. Al-Abdallah and Al-Taani (2017), <ref type="bibr" target="#b111">Mandal, Singh, and Pal (2019)</ref>, <ref type="bibr" target="#b162">Priya and Umamaheswari (2019)</ref> are examples of ATS research making relying on the PSO algorithm. The PSO algorithm steps include <ref type="bibr" target="#b203">(Venter &amp; Sobieszczanski-Sobieski, 2003)</ref>:</p><p>1. Starting with an initial population of particles <ref type="bibr">(individuals)</ref> which are randomly discovered through the design space. Each individual has a random position and a velocity. 2. Calculating a velocity vector for each individual. 3. Updating the position of each individual using its previous position and the newly updated velocity vector. 4. Until convergence, repeating the above steps from the second step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3.">Fuzzy logic</head><p>Fuzzy logic has been widely used in ATS systems like <ref type="bibr">Abbasighalehtaki, Khotanlou, and Esmaeilpour (2016)</ref>, <ref type="bibr" target="#b71">Jafari et al. (2016), and</ref><ref type="bibr" target="#b160">Patel, Shah, and</ref><ref type="bibr" target="#b160">Chhinkaniwala (2019)</ref>. The inputs of text features that are given to the fuzzy logic system include: sentence length, sentence similarity, etc. <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>. Fuzzy logic systems mainly contain four components <ref type="bibr" target="#b67">(Ibrahim, 2016)</ref> as follows:</p><p>Fuzzifier (fuzzification interface): it transforms the crisp input value to a fuzzy linguistic value because the input values are always crisp numerical values. Inference Engine: it uses the fuzzy inputs and the fuzzy rules to generate the fuzzy outputs. Fuzzy Knowledge Base: it contains the fuzzy rules in the form of ''IF-THEN" rules including the linguistic variables. Defuzzifier (defuzzification interface): it is the last step of a fuzzy logic system. It converts the fuzzy outputs to crisp output actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Text summarization datasets and evaluation metrics</head><p>This section provides an overview about the basic resources that are used to evaluate and compare ATS systems. These resources include the well-known and standard datasets besides the manual criteria and automatic summary evaluation tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Standard datasets</head><p>In <ref type="bibr" target="#b34">Dernoncourt et al. (2018)</ref>, Dernoncourt et al. provide an overview of many corpora that have been used in the summarization tasks. This survey presents the most common benchmarking datasets which have been used for the ATS systems evaluation as shown in Table <ref type="table" target="#tab_5">5 including</ref> Each dataset contains both documents and their summaries in three forms: 1) manually created summaries, 2) automatically created baseline summaries, and 3) automatically created summaries that were generated by challenge participants systems. To get access to these datasets, it is required to complete some application forms that exist on the DUC website.<ref type="foot" target="#foot_2">1</ref> These datasets are usually used to evaluate the ATS systems but they do not provide enough data to train the neural networks models <ref type="bibr" target="#b97">(Lin &amp; Ng, 2019)</ref>. 2. Text Analysis Conference (TAC) Datasets: in 2008, DUC became a summarization track in the TAC. It is required to complete some application forms that exist on website 2 in order to get access to the TAC datasets. 3. Essex Arabic Summaries Corpus (EASC) Dataset <ref type="bibr" target="#b38">(El-Haj, Kruschwitz, &amp; Fox, 2015)</ref>: it contains Arabic articles and human-generated extractive summaries of these articles. EASC uses copyrighted It is the responsibility of the dataset to comply with all associated copyright rules. 4. SummBank Dataset <ref type="bibr" target="#b164">(Radev, Teufel, Saggion, Lam, Blitzer, Qi, &amp; Drabek, 2003)</ref>: it contains 40 clusters of news, humanwritten non-extractive summaries, 360 multi-document, and approximately two million multi-document and single document extracts produced by manual and automatic methods. 5. Opinosis Dataset <ref type="bibr" target="#b46">(Ganesan et al., 2010)</ref>: it contains 51 files.</p><p>Each file is about a feature of a product (e.g. the battery life of iPod) that includes a set of reviews written by customers who bought that product. So, the dataset represents 51 topics such that each topic includes approximately 100 sentences. The dataset contains 5 manually written ''gold" summaries for each topic. For most topics, the 5 summaries are different. 6. Large-scale Chinese Short Text Summarization (LCSTS) Dataset <ref type="bibr" target="#b64">(Hu, Chen, &amp; Zhu, 2015)</ref>: It contains more than 2 million short texts with short summaries. This dataset is created from the SinaWeibo website (i.e. a Chinese microblogging website). 7. Computer-Aided Summarization Tool (CAST) Corpus <ref type="bibr" target="#b59">(Hasler, Orasan, &amp; Mitkov, 2003)</ref>: it contains a set of newswire texts which are taken from the Reuters Corpus 3 and a few science texts from the British National Corpus. 4 The news texts part of the corpus is available after signing the agreement with Reuters 5 while the other part cannot be distributed. The corpus contains three types of information annotation: the sentence importance (essential or important), links between sentences, and text fragments which can be removed from the marked sentences. A sentence is considered unimportant if it is not annotated. This dataset could be very useful for developing sentence reduction and sentence selection algorithms. 8. CNN-corpus Dataset <ref type="bibr" target="#b100">(Lins, Oliveira, et al., 2019)</ref>: it can be used for single-document extractive summarization. It contains the original texts, highlights, and gold-standard summaries. This corpus was recently used in the extractive text summarization competition ''DocEng'19" <ref type="bibr" target="#b99">(Lins, Mello, &amp; Simske, 2019)</ref>. For research purposes, this corpus with all its annotated versions is freely available by requesting it from its authors. 9. Gigaword 5 Dataset: It is a popular dataset for abstractive summarization research. It contains ten million English news documents approximately so it is suitable for training and testing the neural networks models. Gigaword is criticized because it contains only headlines as summaries <ref type="bibr" target="#b97">(Lin &amp; Ng, 2019;</ref><ref type="bibr" target="#b149">Nallapati, Zhou, Santos, Gulcehre, &amp; Xiang, 2016)</ref>. 10. CNN/Daily Mail Corpus <ref type="bibr" target="#b60">(Hermann et al., 2015)</ref>: this corpus is used for the passage-based question answering task then it has been widely used for evaluating the ATS systems. <ref type="bibr" target="#b149">(Nallapati et al., 2016)</ref> provide a modified version of this corpus such that it contains multi-sentence summaries for abstractive summarization evaluation.</p><p>In Table <ref type="table" target="#tab_5">5</ref>, the following features are defined for each dataset: 1) the dataset name, 2) the number of documents, 3) the language of data, 4) the domain of data (e.g. news or blogs), 5) whether the dataset supports single-document and/or multi-document summarization, and 6) the dataset URL. In Table <ref type="table" target="#tab_5">5</ref>, the first 5 features have been filled from <ref type="bibr" target="#b34">Dernoncourt et al. (2018)</ref> except for the datasets ''EASC", ''SummBank", ''CAST'' and ''CNN-corpus" as their features are extracted from their corresponding papers and websites. The number of documents for a multi-document summarization dataset is written like ''30 Â 10" which means that the dataset includes 30 clusters of documents and each cluster contains 10 documents approximately.</p><p>2 https://tac.nist.gov/data/forms/index.html. 3 http://about.reuters.com/researchandstandards/corpus/. 4 http://www.natcorp.ox.ac.uk/. 5 http://about.reuters.com/researchandstandards/corpus/how_to_apply.asp. In conclusion, there is a need for more datasets that 1) support and 2) cover the various data domains for all languages because most of the available datasets focus on the news domain as shown in Table <ref type="table" target="#tab_5">5</ref>. In addition, Dernoncourt et al. conclude 1) there is a need more large-scale corpora especially for evaluating and deep-learningbased summarization systems, and 2) a data standard is required for all summarization corpora because each corpus is organized differently <ref type="bibr" target="#b34">(Dernoncourt et al., 2018)</ref>. If the researchers evaluate their proposed ATS systems on many corpora, they will consume a lot of time. As a result, the research papers in the ATS field usually use one or very few corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Summary evaluation</head><p>In the past two decades, there were many efforts to solve summary evaluation issues. NIST leads the effort by organizing the DUC and TAC challenges <ref type="bibr">(Lloret, Plaza, &amp; Aker, 2017)</ref>. In Huang, He, Wei, and Li (2010), Huang et al. formulate four main objectives that should be considered to generate a condensed and readable summary:</p><p>1. Information Coverage: the summary should contain the important information of the input document(s). 2. Information Significance: the summary should cover the various topics of the input document(s). The most important topics can either be the main topics in the input document(s) as in the generic summarization or the user-preferred topics as in the query-based summarization. 3. Information Redundancy: minimize the redundant or duplicate information in the generated summary. 4. Text Coherence: the summary is not just a set of important but disconnected phrases or words. The summary should be readable and understandable text.</p><p>There are two evaluation measures to evaluate the generated summaries <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>: 1) intrinsic methods: measure summary quality using human evaluation. The intrinsic evaluation assesses the coherence and the content coverage or informativeness of a summary <ref type="bibr" target="#b103">(Lloret et al., 2017)</ref>, and 2) extrinsic methods: measure summary quality through a task-based performance measure such as the information retrieval-oriented task. The extrinsic evaluation assesses the utility of summaries in a given application context (e.g. relevance assessment, reading comprehension, etc.) <ref type="bibr" target="#b103">(Lloret et al., 2017)</ref>. There are two ways of text summarization evaluation: manual and automatic. Summary evaluation is a very challenging issue in the text summarization research field. The automatically generated summaries have to be evaluated in order to assess the quality of the ATS systems that generated them <ref type="bibr" target="#b103">(Lloret et al., 2017</ref>). The ATS system performance is usually compared to different baseline systems such as using leading sentences from the input document or using common text summarizers such as LexRank <ref type="bibr" target="#b41">(Erkan &amp; Radev, 2004)</ref>, TextRank <ref type="bibr" target="#b132">(Mihalcea &amp; Tarau, 2004)</ref>, MEAD <ref type="bibr" target="#b165">(Radev, Blair-Goldensohn, &amp; Zhang, 2001)</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Manual evaluation of summaries</head><p>The human judges may be asked to evaluate the computergenerated summaries using some or all of the following quality metrics <ref type="bibr" target="#b103">(Lloret et al., 2017;</ref><ref type="bibr" target="#b112">Mani, 2001)</ref>:</p><p>Readability: assess the linguistic quality of the summary by checking that it does not contain gaps in its rhetorical structure or dangling anaphora. Structure and Coherence: the summary has to be wellorganized and well-structured. It consists of a set of coherent and related sentences.</p><p>Grammaticality: the summary should not include incorrect sentences that violate the grammar rules or capitalization errors. Referential Clarity: if the summary includes a pronoun, so the reader should identify the noun phrase it refers to easily. Content coverage: the summary should include the various topics that have been discussed in the input document(s). Conciseness and Focus: each sentence in the summary should enclose information that is related to the other sentences. Non-redundancy: summary should not include unnecessary repetition which may take different forms like: whole sentences or parts of sentences that are repeated, or the repeated use of a noun phrase or noun like ''Jack Tomson" when a pronoun ''he" is sufficient.</p><p>A qualitative evaluation like in <ref type="bibr" target="#b105">Lloret et al. (2013)</ref> may be used to measure the user satisfaction by focusing on a five-point scale (1 = strongly disagree, 2 = disagree, 3 = neither agree nor disagree, 4 = agree, and 5 = strongly agree) that are used to answer the following questions: Q1: The summary reflects the most important issues of the document. Q2: The summary allows the reader to know what the article is about. Q3: After reading the original abstract provided with the article, the alternative summary is also valid.</p><p>As there is no best reference summary among the humancreated model summaries, the Pyramid method <ref type="bibr" target="#b154">(Nenkova &amp; Passonneau, 2004;</ref><ref type="bibr" target="#b155">Nenkova, Passonneau, &amp; McKeown, 2007)</ref> was created to overcome this problem. This method is semiautomatic and has been used in DUC 2006 to evaluate summary content. The main idea is to create a gold standard summary by comparing the human-created reference summaries based on Summary Content Units (SCUs). First, the similar sentences among the N model summaries are identified manually. Next, SCUs of the similar sentences are produced as a pyramid model which consists of N levels labeled from 1 to N. Based on the occurrence of SCUs in the model summaries; they are ranked in the pyramid. Last, a summary is considered as good if it encloses more SCUs from the higher levels in the pyramid than the lower levels and vice versa for the poor summary <ref type="bibr" target="#b103">(Lloret et al., 2017)</ref>.</p><p>Manual evaluation and analysis of summaries consumes a lot of time and effort because it needs humans to read the summaries and also the original documents <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>. On the other side, most of the developed automatic evaluation methods assess the summary's content and the evaluation of readability is done almost manually (e.g. DUC and TAC conferences assess the readability of each summary manually) <ref type="bibr" target="#b103">(Lloret et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Automatic evaluation of summaries</head><p>This subsection will explore the commonly used evaluation metrics in literature such as: 1) Precision, Recall, and F-Measure scores, 2) Recall-Oriented Understudy for Gisting Evaluation (ROUGE) <ref type="bibr" target="#b96">(Lin, 2004)</ref>, and 3) Basic Element (BE) <ref type="bibr" target="#b63">(Hovy, Lin, Zhou, &amp; Fukumoto, 2006)</ref>.</p><p>Precision Score Metric: it is computed by dividing the number of sentences existing in both reference and candidate (i.e. system) summaries by the number of sentences in the candidate summary as in Eq. (3) <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>.</p><formula xml:id="formula_4">Precision ¼ S ref \ S cand S cand<label>ð3Þ</label></formula><p>Recall Score Metric: it is computed by dividing the number of sentences existing in both reference and candidate summaries by the number of sentences in the reference summary as (4) <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref></p><formula xml:id="formula_5">. Recall ¼ S ref \ S cand S ref<label>ð4Þ</label></formula><p>F-Measure Score Metric: it is a measure that combines recall and precision metrics as Eq. ( <ref type="formula" target="#formula_6">5</ref>) <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>. F-measure is the mean between precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F-Measure</head><formula xml:id="formula_6">¼ Precision ð ÞRecall ð Þ Precision þ Recall<label>ð5Þ</label></formula><p>ROUGE Metric: it is the most commonly used tool for automatic evaluation of the automatically generated summaries <ref type="bibr" target="#b46">(Ganesan et al., 2010)</ref>. ROUGE is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in NLP <ref type="bibr" target="#b55">(Gupta &amp; Siddiqui, 2012)</ref>. It compares the computer-generated summaries against several human-created reference summaries <ref type="bibr" target="#b103">(Lloret et al., 2017)</ref>. The basic idea of ROUGE is to count the number of overlapping units between candidate (or system) summaries and the reference summaries, such as overlapped n-grams <ref type="bibr" target="#b211">(Wang et al., 2017)</ref>. ROUGE has been proven to be effective in measuring qualities of summaries and correlates well to human judgments <ref type="bibr" target="#b194">(Sun et al., 2016)</ref>. There are various ROUGE metrics as follows:</p><p>ROUGE-1 (R1): it is based on the uni-gram measure between a candidate summary and a reference summary. ROUGE-N is an n-gram recall between a candidate summary and reference summaries. ROUGE-L (R-L): it is based on the longest common subsequences between a candidate summary and a reference summary. ROUGE-S* (R-S*): it measures the overlap ratio of skip-bigrams between a candidate summary and reference summaries. ROUGE-SU* (R-SU*): it extends ROUGE-S* by using skipbigrams and using uni-gram as a counting unit. The ''*" refers to the number of skip words. For example, ROUGE-SU4 permits bi-grams to consist of non-adjacent words with a maximum of four words between the two bi-grams words.</p><p>Although ROUGE was accepted as a standard for measuring the accuracy of a summarization model since its development, it has the disadvantage that it only matches strings between the summaries without considering the meaning in single words or series of words (n-grams). Therefore, evaluation methods to address the meaning issue have been proposed which use dependency parsing to represent the information in the candidate and reference summaries such as Basic Elements (BE) <ref type="bibr" target="#b63">(Hovy et al., 2006)</ref>, Basic Elements with Transformations for Evaluation (BEwT-E) <ref type="bibr" target="#b199">(Tratz &amp; Hovy, 2008)</ref>, and DEPEVAL(summ) <ref type="bibr" target="#b158">(Owczarzak, 2009)</ref> methods. In BE and BEwT-E, each sentence is divided into small content units which are called basic elements and are used to match equivalent expressions. Each basic element is a triplet of words consisting of: 1) a head, 2) a modifier or argument, and 3) the relation between the head and modifier. The main disadvantage is that these methods use many language-dependent pre-processing modules for parsing and dividing the sentences. For non-English summaries, parser resources for the summaries' languages are required <ref type="bibr" target="#b103">(Lloret et al., 2017)</ref>.</p><p>Human judgment has the disadvantage of being subjective with a wide variance on what is considered a ''good" summary. This variance implies that creating an automatic evaluation and analysis method is very difficult and challenging <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>. In automatic evaluation, summaries generated by ATS systems are assessed by automated metrics to reduce the evaluation cost. However, human efforts are still needed by the automated evaluation metrics because they depend on the comparison of system-generated summaries with one or more human-created model summaries <ref type="bibr" target="#b103">(Lloret et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and future research directions</head><p>Manual text summarization is a time consuming and costly task that includes many steps. For example, the following steps are done to manually summarize a single document <ref type="bibr" target="#b195">(Takeuchi, 2002)</ref>: 1) trying to understand what the document is about, 2) trying to extract the ''most important" parts from it, and 3) trying to compose a summary that satisfies the following requirements <ref type="bibr" target="#b103">(Lloret et al., 2017)</ref>:</p><p>The summary readability and linguistic quality. The summary consistency and content coverage. The non-redundancy of the produced summary.</p><p>Due to the difficulty of manual text summarization of the huge amount of the textual content on the Internet or various archives, ATS systems have appeared as the main technology to solve this urgent and pressing issue. There are many automatic text summarizers in the literature, but they are still far away from the results of the human text summarization. It is still difficult for the computer to understand and identify the ''most important" parts in the text (i.e. the importance of a text varies with its type, application domain, user preference, etc.).</p><p>There are continuous research efforts since the 1950s to overcome these difficulties and investigate new solutions. Over time, the scientific community mainly has focused on the extractive text summarization approach and has implemented the summarization methods of this approach for various types of applications such as user reviews, news articles, blogs, email messages, scientific articles, legal documents, biomedical documents, etc. In practice, extractive ATS systems produce very different summaries than the ones generated by humans. There have been some trials to propose abstractive and hybrid text summarization systems. There is still a long way to go <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>. Researchers dream to automate the generation of human-like summaries.</p><p>The available literature for abstractive summarization is much less than extractive text summarization. Most survey papers <ref type="bibr" target="#b6">(Al-Saleh &amp; Menai, 2016;</ref><ref type="bibr" target="#b7">Al Qassem et al., 2017;</ref><ref type="bibr" target="#b32">Dalal &amp; Malik, 2013;</ref><ref type="bibr" target="#b45">Gambhir &amp; Gupta, 2017;</ref><ref type="bibr" target="#b51">Gupta, Tiwari, &amp; Robert, 2016;</ref><ref type="bibr" target="#b54">Gupta &amp; Lehal, 2010;</ref><ref type="bibr" target="#b124">Meena, Jain, &amp; Gopalani, 2014;</ref><ref type="bibr" target="#b144">Moratanch &amp; Chitrakala, 2017;</ref><ref type="bibr" target="#b189">Shah &amp; Desai, 2016;</ref><ref type="bibr" target="#b196">Tandel et al., 2016)</ref> concentrate more on the extractive text summarization because the abstractive approach is much harder and much less mature than the extractive approach. The aim of this survey is to give a comprehensive review and global overview about the different aspects of ATS. The main contributions of this survey include:</p><p>Explaining the various classifications and the different applications of the ATS systems. Providing a systematic review about the ATS approaches (namely extractive, abstractive, and hybrid) and the methods that apply these approaches in the literature. Providing a categorization and overview of the various building blocks and techniques that have been used to design and implement the ATS systems including: 1) the text summarization operations, 2) the statistical and linguistic features, and 3) the text summarization building blocks (namely the text represen-tation models, the linguistic analysis and processing techniques, and the soft computing Providing a general overview about the standard datasets, the manual evaluation criteria, and the automatic evaluation tools that are commonly used for evaluating the computergenerated summaries. Providing a listing and of the future research directions the ATS research community. These research directions explained next in the rest of this section.</p><p>There are many limitations of the existing ATS systems that act as challenges and future research directions for the research community. These challenges will help the researchers to identify areas where further research is needed. Fig. <ref type="figure" target="#fig_7">9</ref> shows the different categories of ATS challenges that will be explained in this section.</p><p>There are some challenges related to usage of the ATS systems such as: 1) multi-document summarization, 2) user-specific summarization, and 3) applications of text summarization.</p><p>Challenges Related to Muti-Document Summarization: Multi-document summarization is a complex task and has many issues like redundancy, temporal dimension, co-reference, and sentence reordering <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>. Multi-document summarization may cause incorrect reference: one sentence may contain a proper noun and the next sentence may contain a pronoun as a reference to the proper noun. If the summarizer handles the pronoun without handling the proper noun, it will generate an reference <ref type="bibr" target="#b178">(Sahoo et al., 2016)</ref>. Challenges Related to User-Specific Summarization: The key challenge here is to summarize content from a number of textual and semi-structured sources (e.g. databases and web pages) in the right way (language, format, size, and time) for a specific user <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>. Due to the availability of a large amount of data in different formats and different languages, it is required to dedicate more research efforts on the multi-document, multilingual, and multimedia summaries. Also, it is required to generate summaries with a specified focus like sentiment-based, personalized summaries, etc. <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>.</p><p>Challenges Related to Applications of Text Summarization: Most of the existing systems mainly focus on certain applications like online reviews, text news, text pages, etc. <ref type="bibr" target="#b216">(Wu et al., 2017)</ref>. It is important to focus now on the most challenging applications like long text, novel, and book summarization.</p><p>Another set of challenges is related to the input and output document(s) of an ATS system such as: 1) input and output formats, 2) length of input documents, and 3) supported languages.</p><p>Challenges Related to Input and Output Formats: Most of the ATS systems deal with textual input and output. It is required to propose new summarizers in which the input can be in the form of meetings, videos, sounds, etc. and the output in a format other than text. For example, the input might be in the form of text and the output can be represented as tables, statistics, graphics, visual rating scales, etc. ATS systems that allow visualization of the summaries will help users to get the required content in less time <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>.</p><p>Challenges Related to Length of Input Documents: Most ATS systems focus on relatively short text documents. For example, the length of a news article is shorter than that of a novel chapter (about 641 words versus 4973 words) <ref type="bibr" target="#b216">(Wu et al., 2017)</ref>. The existing ATS methods may achieve good performance in short texts, but they achieve low accuracy and efficiency when summarizing long texts <ref type="bibr" target="#b211">(Wang et al., 2017)</ref>.</p><p>Challenges Related to Supported Languages: Most ATS systems focus on the English language content. For many other languages, the quality of the current ATS systems needs to be improved. It is required to develop and improve NLP tools that are used to generate summaries for non-English languages like NER, POS tagging, syntactic and semantic parsing, etc. <ref type="bibr" target="#b14">(Belkebir &amp; Guessoum, 2018)</ref>.</p><p>There are other challenges related to the methods and techniques of ATS systems such as: 1) text summarization approaches, 2) statistical and linguistic features, and 3) using deep learning for text summarization.</p><p>Challenges Related to Text Summarization Approaches: Most research focus on the extractive approach, it is required to focus more research efforts to propose and improve summarization systems based on the abstractive and hybrid approaches.</p><p>Challenges Related to Statistical and Linguistic Features: It is required to discover some new linguistic and statistical features for sentences and words which can semantically extract the key sentences from the source document(s) <ref type="bibr" target="#b45">(Gambhir &amp; Gupta, 2017)</ref>. Besides, deciding the proper weights of individual features is very important because the quality of the final summary depends on it <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>.</p><p>Challenges Related to Using Deep Learning for Text Summarization: In the summary generation phase, the RNN in the seq2seq system requires a large-scale structured training data. The required training data is not always available in practical NLP applications. It is a very important research topic to build an ATS system using a small amount of training data through the combination of traditional NLP techniques such as syntactic analysis, grammar analysis, semantic analysis, etc. <ref type="bibr" target="#b211">(Wang et al., 2017)</ref>.</p><p>Finally, there are some challenges related to the output and generated summary from ATS systems such as: 1) stop criteria of the summarization process, 2) quality of generated summary, and 3) evaluation of generated summary.</p><p>Challenges Related to Stop Criteria of the Summarization Process: Humans summarize documents in an iterative process. After producing the first summary, one (or the system) should decide whether to stop or continue in the summarization process. The most common way is to set a retention rate upon which a decision is made. The retention rate is not fixed for all the texts. It should vary according to the content and type of the text. It is highly required to propose a more convincing technique to stop summarizing <ref type="bibr" target="#b14">(Belkebir &amp; Guessoum, 2018)</ref>.</p><p>Challenges Related to the Quality of Generated Summary: It is required to achieve a good balance between readability, compression ratio, and summarization quality. It is difficult for the existing ATS systems to achieve the higher compression ratio requirement for summarizing long documents like novels and books <ref type="bibr" target="#b216">(Wu et al., 2017)</ref>. It is required to improve the summary readability by refining the initially generated summary to over-  come the semantic confusion caused by ambiguous or synonymous words <ref type="bibr">(Wu al., 2017)</ref>. Without using NLP, the generated extractive summaries may suffer lack of cohesion and semantics. Also, texts contain multiple topics, the generated summary may not be balanced <ref type="bibr" target="#b54">(Gupta &amp; Lehal, 2010)</ref>.</p><p>Challenges Related to Evaluation of the Generated Summary: Evaluating summaries (either automatically or manually) is a cult 1) it is very challenging to define use a good standard to evaluate whether the summaries generated from the ATS systems are good enough <ref type="bibr" target="#b103">(Lloret et al., 2017)</ref>, and 2) it is very hard to find out what an ideal (or even correct) summary is because the ATS systems can generate good summaries that are different from the human-generated summaries <ref type="bibr" target="#b144">(Moratanch &amp; Chitrakala, 2017)</ref>. Humans are different and they may select entirely different sentences for the extractive summaries and may paraphrase the abstractive summaries in a completely different way. It is very subjective to identify a good summary. Therefore, manual evaluations may not be suitable for all types of summaries <ref type="bibr" target="#b103">(Lloret et al., 2017)</ref>. There is a need to propose new approaches and solutions for the automatic evaluation of the computer-generated summaries.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Single-document or (b) Multi-document, automatic text summarizer.</figDesc><graphic coords="2,63.89,657.41,453.77,79.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Classifications of the ATS systems.</figDesc><graphic coords="3,130.39,505.87,340.21,233.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The architecture of an extractive text summarization system.</figDesc><graphic coords="5,323.60,428.60,227.15,122.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The architecture of an abstractive text summarization system.</figDesc><graphic coords="8,313.74,640.35,227.15,87.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Single-sentence and multi-sentence text summarization operations.</figDesc><graphic coords="12,92.24,67.92,397.13,139.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Building blocks of the ATS systems.</figDesc><graphic coords="14,44.00,67.91,496.90,247.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>: 1. Document Understanding Conference (DUC) Datasets: these datasets are provided by the National Institute of Standards and Technology (NIST) and they are the most common and frequently used datasets in the text summarization research. The DUC corpora were released as part of the summarization shared task hosted at the DUC conference. The last DUC challenge was held in 2007. The DUC website contains datasets for DUC 2001 through DUC 2007.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. Challenges for ATS systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>W.S. El-Kassas al. / Expert Systems with Applications 165 (2021) 113679</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Pros and cons of the extractive text summarization methods.</figDesc><table><row><cell>Method</cell><cell>Pros</cell><cell>Cons</cell><cell>Examples</cell></row><row><cell>Statistical-Based</cell><cell>It requires less processor capacity and</cell><cell></cell><cell></cell></row><row><cell></cell><cell>memory</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Pros and cons of the abstractive text summarization methods.</figDesc><table><row><cell>Method</cell><cell>Pros</cell><cell>Cons</cell><cell>Examples</cell></row><row><cell>Graph-Based</cell><cell>It is applicable to any domain and it does not require</cell><cell></cell><cell></cell></row><row><cell></cell><cell>any intervention by human experts</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Pros and cons of the hybrid text summarization methods.</figDesc><table><row><cell>Method</cell><cell>Pros</cell><cell>Cons</cell><cell>Examples</cell></row><row><cell>Extractive to</cell><cell>Using both extractive and abstractive</cell><cell></cell><cell></cell></row><row><cell>Abstractive</cell><cell>methods to improve the quality of the</cell><cell></cell><cell></cell></row><row><cell></cell><cell>generated summary (Sahba, Ebadi, Jamshidi,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>&amp; Rad, 2018).</cell><cell></cell><cell></cell></row></table><note><p><p><p><p><p><p><p><p>The extractive method used in the first stage has an important effect on the final performance</p><ref type="bibr" target="#b101">(Liu, Saleh, Pot, Goodrich, Sepassi, Kaiser, &amp; Shazeer, 2018)</ref></p>.</p><ref type="bibr" target="#b48">(Gehrmann, Deng, &amp; Rush, 2018;</ref><ref type="bibr" target="#b101">Liu et al., 2018;</ref><ref type="bibr" target="#b175">Rudra et al., 2019;</ref><ref type="bibr" target="#b176">Sahba et al., 2018;</ref><ref type="bibr" target="#b221">Zeng, Luo, Fidler, &amp; Urtasun, 2016)</ref> </p>Extractive to Shallow Abstractive</p>Enhance the disadvantages of the generated extractive summary.</p>The final summary is a shallow abstractive summary.</p><ref type="bibr" target="#b179">(Sahoo, Bhoi, &amp; Balabantaray, 2018)</ref> </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Statistical and linguistic features.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Standard datasets for text summarization.</figDesc><table><row><cell></cell><cell cols="2">Dataset Name Number of</cell><cell>Language</cell><cell>Domain</cell><cell>Single-</cell><cell>Multi-</cell><cell>URL</cell></row><row><cell></cell><cell></cell><cell>Documents</cell><cell></cell><cell></cell><cell>Document</cell><cell>Document</cell><cell></cell></row><row><cell>1</cell><cell>DUC 2001</cell><cell>60 Â 10</cell><cell>English</cell><cell>News</cell><cell>U</cell><cell>U</cell><cell>https://www-nlpir.nist.gov/projects/duc/data.</cell></row><row><cell>2</cell><cell>DUC 2002</cell><cell>60 Â 10</cell><cell>English</cell><cell>News</cell><cell>U</cell><cell>U</cell><cell>html</cell></row><row><cell>3</cell><cell>DUC 2003</cell><cell>60 Â 10, 30 Â 25</cell><cell>English</cell><cell>News</cell><cell>U</cell><cell>U</cell><cell></cell></row><row><cell>4</cell><cell>DUC 2004</cell><cell>100 Â 10</cell><cell cols="2">Arabic, English News</cell><cell>U</cell><cell>U</cell><cell></cell></row><row><cell>5</cell><cell>DUC 2005</cell><cell>50 Â 32</cell><cell>English</cell><cell>News</cell><cell>✗</cell><cell>U</cell><cell></cell></row><row><cell>6</cell><cell>DUC 2006</cell><cell>50 Â 25</cell><cell>English</cell><cell>News</cell><cell>✗</cell><cell>U</cell><cell></cell></row><row><cell>7</cell><cell>DUC 2007</cell><cell>25 Â 10</cell><cell>English</cell><cell>News</cell><cell>✗</cell><cell>U</cell><cell></cell></row><row><cell>8</cell><cell>TAC 2008</cell><cell>48 Â 20</cell><cell>English</cell><cell>News</cell><cell>✗</cell><cell>U</cell><cell>https://tac.nist.gov/data/index.html</cell></row><row><cell>9</cell><cell>TAC 2009</cell><cell>44 Â 20</cell><cell>English</cell><cell>News</cell><cell>✗</cell><cell>U</cell><cell></cell></row><row><cell cols="2">10 TAC 2010</cell><cell>46 Â 20</cell><cell>English</cell><cell>News</cell><cell>✗</cell><cell>U</cell><cell></cell></row><row><cell cols="2">11 TAC 2011</cell><cell>44 Â 20</cell><cell>English</cell><cell>News</cell><cell>✗</cell><cell>U</cell><cell></cell></row><row><cell cols="2">12 EASC</cell><cell>153</cell><cell>Arabic</cell><cell>News,</cell><cell>U</cell><cell>✗</cell><cell>https://www.lancaster.ac.uk/staff/elhaj/corpora.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Wikipedia</cell><cell></cell><cell></cell><cell>htm</cell></row><row><cell cols="2">13 SummBank</cell><cell>40 Â 10</cell><cell>Chinese,</cell><cell>News</cell><cell>U</cell><cell>U</cell><cell>https://catalog.ldc.upenn.edu/LDC2003T16</cell></row><row><cell></cell><cell></cell><cell></cell><cell>English</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">14 Opinosis</cell><cell>51 Â 100</cell><cell>English</cell><cell>Reviews</cell><cell>✗</cell><cell>U</cell><cell>http://kavita-ganesan.com/opinosis-opinion-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dataset/</cell></row><row><cell cols="2">15 LCSTS</cell><cell>2,400,591</cell><cell>Chinese</cell><cell>Blogs</cell><cell>U</cell><cell>✗</cell><cell>http://icrc.hitsz.edu.cn/Article/show/139.html</cell></row><row><cell cols="2">16 CAST</cell><cell>147</cell><cell>English</cell><cell>News</cell><cell>U</cell><cell>✗</cell><cell>http://clg.wlv.ac.uk/projects/CAST/corpus/index.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>php</cell></row><row><cell cols="2">17 CNN-corpus</cell><cell>3,000</cell><cell>English</cell><cell>News</cell><cell>U</cell><cell>✗</cell><cell>By email request through (Lins, Oliveira, et al.,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2019)</cell></row><row><cell cols="2">18 Gigaword 5</cell><cell>9,876,086</cell><cell>English</cell><cell>News</cell><cell>U</cell><cell>✗</cell><cell>https://catalog.ldc.upenn.edu/LDC2011T07</cell></row><row><cell cols="2">19 CNN/Daily</cell><cell>312,084</cell><cell>English</cell><cell>News</cell><cell>U</cell><cell>✗</cell><cell>https://github.com/deepmind/rc-data/</cell></row><row><cell></cell><cell>Mail</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>W.S. El-Kassas et al. / Expert Systems with Applications 165 (2021) 113679</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>W.S. El-Kassas et al. / Expert Systems with Applications 165 (2021) 113679</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2"><p>https://www-nlpir.nist.gov/projects/duc/data.html. W.S. El-Kassas et al. / Expert Systems with Applications 165 (2021) 113679</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Competing Interest</head><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fuzzy evolutionary cellular learning automata model for text summarization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Abbasi-Ghalehtaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khotanlou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Esmaeilpour</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.swevo.2016.03.004</idno>
		<ptr target="https://doi.org/10.1016/j.swevo.2016.03.004" />
	</analytic>
	<monogr>
		<title level="j">Swarm and Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="11" to="26" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sentence matrix normalization using most likely n-grams vector</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdolahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zahedh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>KBEI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Query-oriented text summarization using sentence extraction technique</title>
		<author>
			<persName><forename type="first">M</forename><surname>Afsharizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ebrahimpour-Komleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bagheri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Tehran</pubPlace>
		</imprint>
	</monogr>
	<note>Paper presented at the 2018 4th</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Arabic single-document text summarization using particle swarm optimization algorithm</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Z</forename><surname>Al-Abdallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Al-Taani</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2017.10.091</idno>
		<ptr target="https://doi.org/10.1016/j.procs.2017.10.091" />
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A hybrid approach for arabic text summarization using domain knowledge and genetic algorithms</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">A</forename><surname>Al-Radaideh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Bataineh</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12559-018-9547-z</idno>
		<ptr target="https://doi.org/10.1007/s12559-018-9547-z" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="651" to="669" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An enhanced latent semantic analysis approach for Arabic document summarization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Al-Sabahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alwesabi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13369-018-3286-z</idno>
		<ptr target="https://doi.org/10.1007/s13369-018-3286-z" />
	</analytic>
	<monogr>
		<title level="j">Arabian Journal for Science and Engineering</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic Arabic text summarization: A survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Al-Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E B</forename><surname>Menai</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-015-9442-x</idno>
		<ptr target="https://doi.org/10.1007/s10462-015-9442-x" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="234" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic Arabic summarization: A survey of methodologies and systems</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Al Qassem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Al Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Barada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Rubaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I</forename><surname>Almoosa</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2017.10.088</idno>
		<ptr target="https://doi.org/10.1016/j.procs.2017.10.088" />
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using statistical and semantic analysis for Arabic text summarization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>El Adlouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>En-Nahnahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meknassi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITCS 2017: International Conference on Information Technology and Communication Systems</title>
		<meeting><address><addrLine>Khouribga, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Enhancing unsupervised neural networks based text summarization with word embedding and ensemble learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meknassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>En-Nahnahi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2019.01.037</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2019.01.037" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="195" to="211" />
		</imprint>
	</monogr>
	<note>Expert Systems with Applications</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Summarization of research publications using automatic extraction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alampalli Ramu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bandarupalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S S</forename><surname>Nekkanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Data Communication Technologies and Internet of Things</title>
		<meeting><address><addrLine>Coimbatore, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Paper presented at the ICICI 2019</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">COSUM: Text summarization based on clustering and optimization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Alguliyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Aliguliyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Isazade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Idris</surname></persName>
		</author>
		<idno type="DOI">10.1111/exsy.12340</idno>
		<ptr target="https://doi.org/10.1111/exsy.12340e12340" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Effective deep learning approaches for summarization of legal texts</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wagh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jksuci.2019.11.015</idno>
		<ptr target="https://doi.org/10.1016/j.jksuci.2019.11.015" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Journal of King Saud University -Computer and Information Sciences</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GraphSum: Discovering correlations among multiple terms for graph-based summarization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Baralis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cagliero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mahoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fiori</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2013.06.046</idno>
		<ptr target="https://doi.org/10.1016/j.ins.2013.06.046" />
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">249</biblScope>
			<biblScope unit="page" from="96" to="109" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TALAA-ATSF: A global operation-based Arabic text summarization framework</title>
		<author>
			<persName><forename type="first">R</forename><surname>Belkebir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guessoum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent natural language processing: Trends and applications</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Shaalan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassanien</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Tolba</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="435" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MSATS: Multilingual sentiment analysis via text summarization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Cloud Computing, Data Science &amp; Engineering -Confluence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SumItUp: A hybrid single-document text summarizer</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">K</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hashmy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soft computing: Theories and applications: Proceedings of SoCTA 2016</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Pant</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Ray</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Sharma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Rawat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Bandyopadhyay</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="619" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A comparative study of summarization algorithms applied to legal case judgments</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiware</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajgaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pochhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>Paper presented at the Advances in Information Retrieval</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The anatomy of a large-scale hypertextual Web search engine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0169-7552(98)00110-X</idno>
		<ptr target="https://doi.org/10.1016/S0169-7552(98)00110-X" />
	</analytic>
	<monogr>
		<title level="j">Computer Networks and ISDN Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="117" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving transformer with sequential context representations for abstractive text summarization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Chinese Computing</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Summarizing email conversations with clue words</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web<address><addrLine>Banff, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Using an n-gram-based document representation with a vector processing retrieval model. Paper presented at the 3rd Text Retrieval Conference</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cavnar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>Gaithersburg, Maryland, US</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tweet summarization of news articles: An objective ordering-based perspective</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bhavsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Dandapat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chandra</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSS.2019.2926144</idno>
		<ptr target="https://doi.org/10.1109/TCSS.2019.2926144" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Social Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="761" to="777" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Single document extractive text summarization using Genetic Algorithms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 Third International Conference on Emerging Applications of Information Technology</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sentence selective neural extractive summarization with reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 11th International Conference on Knowledge and Systems Engineering (KSE)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Paper presented at the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Concept-based extractive text summarization using graph modelling and weighted iterative ranking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chitrakala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moratanch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Revanth Raaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Divya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emerging research in computing, information, communication and applications: ERCICA 2016</title>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Shetty</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Patnaik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Prasad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Nalini</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="149" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Paper presented at the NAACL-HLT 2016</title>
		<meeting><address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the world&apos;s response</title>
		<author>
			<persName><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M. -W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scientific document summarization via citation contextualization and scientific discourse</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00799-017-0216-8</idno>
		<ptr target="https://doi.org/10.1007/s00799-017-0216-8" />
	</analytic>
	<monogr>
		<title level="j">International Journal on Digital Libraries</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="287" to="303" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GATE: An architecture for development of robust HLT applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tablan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">40th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>Paper presented at the the</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic graph based automatic text summarization for Hindi documents using particle swarm optimization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 6th International Conference on Emerging Trends in Engineering and Technology</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Satapathy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2013">2013. 2018. 2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="284" to="289" />
		</imprint>
	</monogr>
	<note>Information and communication technology for intelligent systems</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
		<idno type="DOI">10.1002/(sici)1097-4571(199009)41:6&lt;391::aid-asi1&gt;3.0.co;2-9</idno>
		<ptr target="https://doi.org/10.1002/(sici)1097-4571" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990. 199009</date>
		</imprint>
	</monogr>
	<note>&lt;391::aid-asi1&gt;3.0.co;2-9</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A repository of corpora for summarization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Miyazaki, Japan</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Latent association rule cluster based model to extract topics for classification and recommendation applications</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Domingues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Sundermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">O</forename><surname>De Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Rezende</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2018.06.021</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2018.06.021" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="34" to="60" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A graph based approach on extractive summarization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mallick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Emerging Technologies in Data Mining and Information Security</title>
		<meeting><address><addrLine>India</addrLine></address></meeting>
		<imprint>
			<publisher>Kolkata</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>IEMIS 2018</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Summarizing microblogs during emergency events: A comparison of extractive summarization algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghatak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Emerging Technologies in Data Mining and Information Security (IEMIS 2018)</title>
		<meeting><address><addrLine>Kolkata, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Creating language resources underresourced languages: Methodologies, and experiments with Arabic</title>
		<author>
			<persName><forename type="first">M</forename><surname>El-Haj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fox</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10579-014-9274-3</idno>
		<ptr target="https://doi.org/10.1007/s10579-014-9274-3" />
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="549" to="580" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S R</forename><surname>Embar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Vaishnavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Kallimani</surname></persName>
		</author>
		<title level="m">2013 International Conference on Advances in Computing, Communications and (ICACCI)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>sArAmsha -A Kannada abstractive summarizer</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Using lexical chains for keyword extraction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ercan</surname></persName>
		</author>
		<author>
			<persName><surname>Cicekli</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2007.01.015</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2007.01.015" />
	</analytic>
	<monogr>
		<title level="j">Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1705" to="1714" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">LexRank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A survey on evaluation of summarization methods</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Cossu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2019.04.001</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2019.04.001" />
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1794" to="1814" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Word-sentence co-ranking for automatic extractive text summarization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2016.12.021</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2016.12.021" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="189" to="195" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Selecting and generating computational meaning representations for short texts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finegan-Dollak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>University of Michigan</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recent automatic text summarization techniques: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gambhir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-016-9475-9</idno>
		<ptr target="https://doi.org/10.1007/s10462-016-9475-9" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="66" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Opinosis: A graph-based approach to abstractive summarization of highly redundant opinions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Single extractive text summarization based on a genetic algorithm</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>García-Hernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ledeneva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Bottom-up abstractive summarization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Brussels, Belgium</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fully abstractive approach to guided summarization</title>
		<author>
			<persName><forename type="first">P.-E</forename><surname>Genest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lapalme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Summarizing text documents: Sentence selection and evaluation metrics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kantrowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 22nd annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>Berkeley, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sentiment analysis and text summarization of online reviews: A survey</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Communication and Signal Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Abstractive summarization: An overview of the state of the art</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2018.12.011</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2018.12.011" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="49" to="65" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Text summarization for big data: A comprehensive survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Innovative Computing and Communications</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A survey of text summarization extractive techniques</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Lehal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Emerging Technologies in Web Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="258" to="268" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi-document summarization using sentence clustering</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Siddiqui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Paper presented at the 2012 4th international conference on intelligent human computer interaction (IHCI)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The challenges of automatic summarization</title>
		<author>
			<persName><forename type="first">U</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mani</surname></persName>
		</author>
		<idno type="DOI">10.1109/2.881692</idno>
		<ptr target="https://doi.org/10.1109/2.881692" />
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="29" to="36" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Distributional structure</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
		<idno type="DOI">10.1080/00437956.1954.11659520</idno>
		<ptr target="https://doi.org/10.1080/00437956.1954.11659520" />
	</analytic>
	<monogr>
		<title level="j">WORD</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="146" to="162" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">From extracts to abstracts: Human summary production operations for computer-aided summarisation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hasler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>University of Wolverhampton</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Building better corpora for summarisation. Paper presented at the Corpus Linguistics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Orasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mitkov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<pubPlace>Lancaster, UK.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koc ˇisky ´</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Teaching machines to read and comprehend</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 22nd annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>Berkeley, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Abstractive document summarization via neural model with joint attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Chinese Computing</title>
		<meeting><address><addrLine>Dalian, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Automated summarization evaluation with basic elements</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fukumoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Paper presented at the the 5th Conference on Language Resources and Evaluation</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">LCSTS: A large scale Chinese short text summarization dataset</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<idno>CoRR abs/1506.05865</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Modeling document summarization as multi-objective optimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 Third International Symposium on Intelligent Information Technology and Security Informatics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A novel Arabic text summarization model based on rhetorical structure theory and vector space model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Elghazaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gheith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Linguistics and Natural Language Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">An overview of soft computing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ibrahim</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2016.09.366</idno>
		<ptr target="https://doi.org/10.1016/j.procs.2016.09.366" />
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="34" to="38" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Indurkhya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Damerau</surname></persName>
		</author>
		<title level="m">Handbook of Natural language processing</title>
		<imprint>
			<publisher>Chapman &amp; Hall/CRC</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Abstractive multi-document summarization by partial tree extraction, recombination and linearization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kurisinkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Meeting summarization, a challenge for deep learning. Paper presented at the Advances in Computational Intelligence</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jacquenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Largeron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Automatic text summarization using fuzzy inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gheisari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 22nd International Conference on Automation and Computing (ICAC)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Hybrid-based Arabic single-document text summarization approach using genatic algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Jaradat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Al-Taani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 7th International Conference on Information and Communication Systems (ICICS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">HSDS: An abstractive model for automatic survey generation</title>
		<author>
			<persName><forename type="first">X. -J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X. -L</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B. -S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B. -B</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>Paper presented at the Database Systems for Advanced Applications</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Using hidden Markov modeling to decompose human-written summaries</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jing</surname></persName>
		</author>
		<idno type="DOI">10.1162/089120102762671972</idno>
		<ptr target="https://doi.org/10.1162/089120102762671972" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="527" to="543" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Random forest classifier based multi-document summarization system. Paper presented at the 2013 IEEE Recent Advances in Intelligent Computational Systems (RAICS)</title>
		<author>
			<persName><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wilscy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Dense semantic graph and its application in single document summarisation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcclean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emerging ideas on information filtering and retrieval: DART 2013: Revised and invited papers</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Lai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Giuliani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Semeraro</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="55" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<title level="m">Speech and language processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>rd ed.</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Text summarization from legal documents: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kanapala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pamula</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-017-9566-2</idno>
		<ptr target="https://doi.org/10.1007/s10462-017-9566-2" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="371" to="402" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">An automatic legal document summarization and search using hybrid system</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Kavila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Puli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S V</forename><surname>Prasada Raju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bandaru</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Summarizing short stories</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kazantseva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szpakowicz</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli.2010.36.1.36102</idno>
		<ptr target="https://doi.org/10.1162/coli.2010.36.1.36102" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="109" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Clustered genetic semantic graph approach for multi-document abstractive summarization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Salim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Intelligent Systems Engineering (ICISE)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Abstractive text summarization based on improved semantic graph approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Salim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<idno type="DOI">10.1007/s10766-018-0560-3</idno>
		<ptr target="https://doi.org/10.1007/s10766-018-0560-3" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Parallel Programming</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="992" to="1016" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A framework for multi-document abstractive summarization based on semantic role labelling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Salim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.asoc.2015.01.070</idno>
		<ptr target="https://doi.org/10.1016/j.asoc.2015.01.070" />
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="737" to="747" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Enriching text representation with frequent pattern mining for probabilistic topic modeling</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<idno type="DOI">10.1002/meet.14504901209</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Hybrid text summarization: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kirmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Manzoor Hakak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soft Computing: Theories and Applications</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">An effective sentence-extraction technique using contextual information and statistical approaches for text summarization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2008.02.008</idno>
		<ptr target="https://doi.org/10.1016/j.patrec.2008.02.008" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1366" to="1371" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Noguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yatsuka</surname></persName>
		</author>
		<title level="m">Summarization based on embedding distributions. Paper presented at the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Abstractive text summarization based on deep learning and semantic content generalization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kouris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alexandridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stafylopatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Scalable aspect-based summarization in the hadoop environment</title>
		<author>
			<persName><forename type="first">K</forename><surname>Krishnakumari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sivasankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Big data analytics: Proceedings of CSI 2015</title>
		<editor>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Aggarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Bhatnagar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Mishra</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="439" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Systematic literature review of fuzzy logic based text summarization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="DOI">10.22111/ijfs.2019.4906</idno>
		<ptr target="https://doi.org/10.22111/ijfs.2019.4906" />
	</analytic>
	<monogr>
		<title level="j">Iranian Journal of Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="45" to="59" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">A roadmap to realization approaches in natural language generation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kurup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Narvekar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Le</surname></persName>
		</author>
		<title level="m">2013 International Conference on Soft Computing and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>An approach to abstractive text summarization. Paper presented at the. SoCPaR</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">The Oxford handbook of psycholinguistics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Levelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Caramazza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Read, watch, listen and summarize: Multi-modal summarization for asynchronous text, image, audio and video</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2018.2848260</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2018.2848260" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Multi-agent discussion mechanism natural language generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference Artificial Intelligence</title>
		<meeting>the AAAI Conference Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">A package for automatic evaluation of summaries. Paper presented at the Workshop on Text Summarization Branches Out</title>
		<author>
			<persName><forename type="first">C. -Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Barcelona, Spain</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Abstractive summarization: A survey of the of art</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Paper presented at the The Thirty-Third Conference on Artificial Intelligence (AAAI-19)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Compressive approaches for cross-language multi-document summarization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Linhares Pontes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Torres-Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Linhares</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.datak.2019.101763</idno>
		<ptr target="https://doi.org/10.1016/j.datak" />
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">101763</biblScope>
			<biblScope unit="page">101763</biblScope>
			<date type="published" when="2019">2020. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">DocEng&apos;19 competition on extractive text summarization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Lins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Simske</surname></persName>
		</author>
		<idno type="DOI">10.1145/3342558.3351874</idno>
		<ptr target="https://doi.org/10.1145/3342558.3351874" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Document Engineering</title>
		<meeting>the ACM Symposium on Document Engineering<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">The CNN-corpus: A large textual corpus for single-document extractive summarization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Lins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tenorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Simske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename></persName>
		</author>
		<idno type="DOI">10.1145/3342558.3345388</idno>
		<ptr target="https://doi.org/10.1145/3342558.3345388" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Document Engineering</title>
		<meeting>the ACM Symposium on Document Engineering<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Generating wikipedia by summarizing long sequences</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1801.10198.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Paper presented at the ICLR 2018. Last Accessed: 26/4/2020</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">A gradual combination of features for building automatic summarisation systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lloret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Text, Speech and Dialogue</title>
		<meeting><address><addrLine>Pilsen, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">The challenging task of summary evaluation: An overview. Language Resources and Evaluation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lloret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10579-017-9399-2</idno>
		<ptr target="https://doi.org/10.1007/s10579-017-9399-2" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">COMPENDIUM: A text summarization system for generating abstracts of research papers. Paper presented at the natural language processing and information systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lloret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Romá-Ferri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palomar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">COMPENDIUM: A text summarization system for generating abstracts of research papers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lloret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Romá-Ferri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palomar</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.datak.2013.08.005</idno>
		<ptr target="https://doi.org/10.1016/j.datak.2013.08.005" />
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="164" to="175" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Gist: General integrated summarization of text and reviews</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lovinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Valova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clough</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00500-017-2882-2</idno>
		<ptr target="https://doi.org/10.1007/s00500-017-2882-2" />
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1589" to="1601" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">The automatic creation of literature abstracts</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Luhn</surname></persName>
		</author>
		<idno type="DOI">10.1147/rd.22.0159</idno>
		<ptr target="https://doi.org/10.1147/rd.22.0159" />
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="165" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">A comprehensive survey on extractive and abstractive techniques for text summarization. Paper presented at the ambient communications and computer systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sharma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">A novel method for performance evaluation of text chunking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maiti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Garain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10579-013-9250-3</idno>
		<ptr target="https://doi.org/10.1007/s10579-013-9250-3" />
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="215" to="226" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Graph-based text summarization using modified TextRank</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mallick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">PSO-Based Text Summarization Approach Using Sentiment Analysis. Paper presented at the computing, communication and signal processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Automatic summarization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>John Benjamins Publishing Company</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Rhetorical structure theory: Toward a functional theory of text organization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mann William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thompson Sandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text -Interdisciplinary Journal for the Study of Discourse</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">243</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
		<title level="m">The Stanford CoreNLP natural language processing toolkit. Paper presented at the 52nd annual meeting of the association for computational linguistics: System demonstrations</title>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Extractive summarization using supervised and unsupervised learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2019.05.011</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2019.05.011" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="173" to="181" />
		</imprint>
	</monogr>
	<note>Expert Systems with Applications</note>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Automatic summarization of technical documents in the oil and gas industry</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M C</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Cozman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H F</forename><surname>Santos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Paper presented at the 2019 8th Brazilian conference on intelligent systems (BRACIS</note>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">TensorFlow: A system for large-scale machine learning. Paper presented at the proceedings of the 12th USENIX conference on operating systems design and implementation</title>
		<author>
			<persName><surname>Mart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<pubPlace>Savannah, GA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">ASFuL: Aspect based sentiment summarization using fuzzy logic</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Arockiam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Algorithms, Methodology, Models and Applications in Emerging Technologies (ICAMMAET)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Automatic text summarization using latent semantic analysis</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Mashechkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Petrovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Tsarev</surname></persName>
		</author>
		<idno type="DOI">10.1134/s0361768811060041</idno>
		<ptr target="https://doi.org/10.1134/s0361768811060041" />
	</analytic>
	<monogr>
		<title level="j">Programming and Computer Software</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="299" to="305" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Generating summaries from event data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Maybury</surname></persName>
		</author>
		<idno type="DOI">10.1016/0306-4573(95)00025-C</idno>
		<ptr target="https://doi.org/10.1016/0306-4573" />
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Tracking and summarizing news on a daily basis with Columbia&apos;s Newsblaster</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Klavans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Sigelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second international conference on Human Language Technology Research</title>
		<meeting>the second international conference on Human Language Technology Research<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Evolutionary algorithms for extractive automatic text summarization</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Meena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gopalani</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2015.04.177</idno>
		<ptr target="https://doi.org/10.1016/j.procs.2015.04.177" />
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="244" to="249" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Survey on graph and cluster based approaches in multi-document text summarization</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Meena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gopalani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Recent Advances and Innovations in Engineering</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>ICRAIE-2014</note>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Effective aggregation of various summarization techniques</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Majumder</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2017.11.002</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2017.11.002" />
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="158" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Vector-space model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Melucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of database systems</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Özsu</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3259" to="3263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Combining graph connectivity and genetic clustering to improve biomedical summarization</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Menéndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Camacho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<publisher>CEC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">NLP based latent semantic analysis for legal text summarization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Advances in Computing, Communications and Informatics (ICACCI)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Improving accuracy of key information acquisition for social media text summarization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conferences on Ubiquitous Computing &amp; Communications (IUCC) and Data Science and Computational Intelligence (DSCI) and Smart Computing, Networking and Services</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Paper presented at the. SmartCNS</note>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Graph-based ranking algorithms for sentence extraction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2004 on Interactive poster and demonstration sessions</title>
		<meeting>the ACL 2004 on Interactive poster and demonstration sessions<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>applied to text summarization. Paper presented at the</note>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Explorations in Automatic book summarization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ceylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">TextRank: Bringing order into texts. Paper presented at the Empirical Methods in Natural Language Processing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tarau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<pubPlace>Barcelona, Spain</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1301.3781.pdf" />
		<title level="m">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Last Accessed: 26/4/2020</note>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for English</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Summarization of text clustering based vector space model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mingzhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 10th International Conference on Computer-Aided Industrial Design &amp; Conceptual Design</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Two-level text summarization from online news sources with sentiment analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Mirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Networks &amp; Advances in Computational Technologies (NetACT)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title level="m" type="main">Extractive summarization by aggregating multiple similarities. Paper presented at the Recent Advances in Natural Language Processing</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mogren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kageback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dubhashi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<pubPlace>Hissar, Bulgaria</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">SRL-ESA-TextSum: A text summarization approach based on semantic role labeling and explicit semantic analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oussalah</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2019.04.003</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2019.04.003" />
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1356" to="1372" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Using citations to generate surveys of scientific paradigms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Egan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Muthukrishan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Qazvinian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">A study on ontology based abstractive summarization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sunitha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaya</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2016.05.122</idno>
		<ptr target="https://doi.org/10.1016/j.procs.2016.05.122" />
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="32" to="37" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Text document summarization using word embedding. Expert Systems with Applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mohd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2019.112958</idno>
		<ptr target="https://doi.org/10.1016/j.eswa" />
		<imprint>
			<date type="published" when="2019">2020. 2019.112958</date>
			<biblScope unit="page">143</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">Concept-graph based biomedical automatic summarization using ontologies. Paper presented at the Proceedings of the 3rd Textgraphs Workshop on Graph-Based Algorithms for Natural Language Processing</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Esteban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gerv</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Manchester, United Kingdom</pubPlace>
		</imprint>
	</monogr>
	<note>&amp; #225</note>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">A survey on abstractive text summarization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Moratanch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chitrakala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Circuit, Power and Computing Technologies (ICCPCT)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">A Survey on Extractive Text Summarization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Moratanch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chitrakala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Computer, Communication and Signal Processing</title>
		<meeting><address><addrLine>Chennai</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">A survey of multiple types of text summarization with their satellite contents based on swarm intelligence optimization algorithms. Knowledge-Based Systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hamouda</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2018.09.008</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2018.09.008" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="518" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Combining linguistic and machine learning techniques for email summarization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzoukermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Klavans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 workshop on Computational Natural Language Learning</title>
		<meeting>the 2001 workshop on Computational Natural Language Learning<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main">Multi-document extractive text summarization: A comparative assessment on features. Knowledge-Based Systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Sezer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Akcayol</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2019.07.019</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2019.07.019" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">183</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">SummaRuNNer: A of recurrent neural network based sequence model for extractive summarization documents</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Paper presented at the Thirty-First AAAI Conference on Artificial Intelligence (AAAI</title>
		<meeting><address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<title level="m" type="main">Abstractive text summarization sequence-to-sequence RNNs and beyond</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Textual keyword extraction and summarization: State-of-the-art</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Nasar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Jaffry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2019.102088</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2019.102088" />
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">56</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Graph-based biomedical text summarization: An itemset mining and sentence clustering approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nasr Azadani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ghadiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Davoodijam</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2018.06.005</idno>
		<ptr target="https://doi.org/10.1016/j.jbi.2018.06.005" />
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="42" to="58" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">A survey on automatic text summarization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nazari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mahdavi</surname></persName>
		</author>
		<idno type="DOI">10.22044/jadm.2018.6139.1726</idno>
		<ptr target="https://doi.org/10.22044/jadm.2018.6139.1726" />
	</analytic>
	<monogr>
		<title level="j">Journal of AI and Data Mining</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="135" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">A survey of text summarization techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining text data</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="43" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">Evaluating content selection in summarization: The pyramid method</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Passonneau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Boston, Massachusetts, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">The Pyramid Method: Incorporating human content selection variation in summarization evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
		<idno type="DOI">10.1145/1233912.1233913</idno>
		<ptr target="https://doi.org/10.1145/1233912.1233913" />
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main">Abstractive multidocument text summarization using a genetic algorithm</title>
		<author>
			<persName><forename type="first">Neri</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ledeneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>García-Hernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>Paper presented at the Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Automatic labelling of documents based on ontology. Paper presented at the 2015 IEEE Pacific Rim Conference on Communications</title>
		<author>
			<persName><forename type="first">N</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Signal Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>PACRIM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">DEPEVAL(summ): Dependency-based evaluation for automatic summaries</title>
		<author>
			<persName><forename type="first">K</forename><surname>Owczarzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Suntec</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">A template-based abstractive meeting summarization: Leveraging summary and source text relationships</title>
		<author>
			<persName><forename type="first">T</forename><surname>Oya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Natural Language Generation Conference (INLG)</title>
		<meeting>the 8th International Natural Language Generation Conference (INLG)<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Fuzzy logic based multi document summarization with improved sentence scoring and redundancy removal technique</title>
		<author>
			<persName><forename type="first">D</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chhinkaniwala</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2019.05.045</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2019.05.045" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="167" to="177" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Automatic text summarizer</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bhatnagar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Enhanced continuous and discrete multi objective particle swarm optimization for text summarization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Priya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Umamaheswari</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10586-018-2674-1</idno>
		<ptr target="https://doi.org/10.1007/s10586-018-2674-1" />
	</analytic>
	<monogr>
		<title level="j">Cluster Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="229" to="240" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Automatic Arabic text summarization based on fuzzy logic</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Qassem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Barada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Rubaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Almoosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Natural Language and Speech Processing</title>
		<meeting>the 3rd International Conference on Natural Language and Speech Processing<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<title level="m" type="main">Evaluation challenges in large-scale multi-document summarization: The MEAD project</title>
		<author>
			<persName><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Drabek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Experiments in single and multi-document summarization using MEAD</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blair-Goldensohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Document Understanding Conference</title>
		<meeting><address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Introduction to the special issue on summarization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
		<idno type="DOI">10.1162/089120102762671927</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="408" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Centroid-based summarization of multiple documents</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">´</forename><surname>Stys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2003.10.006</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2003.10.006" />
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="919" to="938" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<monogr>
		<title level="m" type="main">Bengali text summarization using TextRank, fuzzy C-Means and aggregate scoring methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Rafiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rafian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arif</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Paper presented at the 2019 IEEE Region 10 Symposium (TENSYMP</note>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Abstractive multi-document summarization</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Ranjitha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Kallimani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<monogr>
		<title level="m" type="main">An overview on soft computing techniques. Paper presented at the High Performance Architecture and Grid Computing</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Svp Raju</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">BioChain: Lexical chaining methods for biomedical text summarization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Reeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM symposium on Applied computing</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<pubPlace>Dijon, France</pubPlace>
		</imprint>
	</monogr>
	<note>Paper presented at the Proceedings of the</note>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">The use of domain-specific concepts in biomedical text summarization</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Reeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Brooks</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2007.01.026</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2007.01.026" />
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1765" to="1776" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Software framework for topic modelling with large corpora</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rehurek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
		<ptr target="https://radimrehurek.com/gensim/index.html" />
	</analytic>
	<monogr>
		<title level="m">LREC 2010 Workshop on New Challenges for NLP Framework</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">A nifty review to text summarization-based recommendation system for electronic products</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Roul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arora</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00500-019-03861-3</idno>
		<ptr target="https://doi.org/10.1007/s00500-019-03861-3" />
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="13183" to="13204" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Summarizing situational tweets in crisis scenarios: An extractive-abstractive approach</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSS.2019.2937899</idno>
		<ptr target="https://doi.org/10.1109/TCSS.2019.2937899" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Social Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="981" to="993" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<title level="m" type="main">Automatic text summarization using customizable fuzzy features and attention on the context and vocabulary</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sahba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ebadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jamshidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Paper presented at the 2018 World Automation Congress (WAC</note>
</biblStruct>

<biblStruct xml:id="b177">
	<monogr>
		<title level="m" type="main">Topic Modeling On Online News Extraction. Paper presented at the Intelligent Computing and Information and Communication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sahni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palwe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Aspect based multidocument summarization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balabantaray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Phukon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saikia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Computing, Communication and Automation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>ICCCA</note>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Hybrid approach to abstractive summarization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Balabantaray</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2018.05.038</idno>
		<ptr target="https://doi.org/10.1016/j.procs.2018.05.038" />
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="1228" to="1237" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Development of embedded platform for Sanskrit grammar-based document summarization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Sakhare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Janmeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and language processing for humanmachine communications: Proceedings of CSI 2015</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Agrawal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Devi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Wason</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Bansal</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">A vector space model for automatic indexing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1145/361219.361220</idno>
		<ptr target="https://doi.org/10.1145/361219.361220" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Extractive multidocument text summarization using a multi-objective artificial bee colony optimization approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Sanchez-Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Vega-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Pérez</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2017.11.029</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2017.11.029" />
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">A decomposition-based multi-objective optimization approach for extractive multi-document text summarization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Sanchez-Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Vega-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Pérez</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.asoc.2020.106231</idno>
		<ptr target="https://doi.org/10.1016/j.asoc.2020.106231106231" />
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="page">91</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<monogr>
		<title level="m" type="main">Experimental analysis of multiple criteria for extractive multi-document text summarization. Expert Systems with Applications</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Sanchez-Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Vega-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Pérez</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2019.112904</idno>
		<ptr target="https://doi.org/10.1016/j.eswa" />
		<imprint>
			<date type="published" when="2019">2020b. 2019.112904 112904</date>
			<biblScope unit="page">140</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Text summarization using Wikipedia</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sankarasubramaniam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2014.02.001</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2014.02.001" />
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="443" to="461" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Automatic text summarization based on betweenness centrality</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L D L P</forename><surname>Sarracén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Paper presented at the 5th Spanish Conference on Information Retrieval</title>
		<meeting><address><addrLine>Zaragoza, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Paper presented at the the</note>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Automatic text summarization of news articles</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sonawane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khanwalker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Keskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Big Data, IoT and Data Science</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">A survey of automatic text summarization techniques for Indian and foreign languages</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Desai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<monogr>
		<title level="m" type="main">Word sense disambiguation for Indian languages</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vyas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Automatic extractive text summarization using K-means clustering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Kallimani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Electrical, Electronics, Communication, Computer, and Optimization Techniques (ICEECCOT)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Document representation techniques and their effect on the document Clustering and Classification: A review</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Devi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mahanta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Research in Computer Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Deep learning based extractive text summarization: Approaches, datasets and evaluation measures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Suleiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Awajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on Social Networks Analysis, Management and Security</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<title level="m" type="main">Query-biased multi-document abstractive summarization via submodular maximization using event guidance. Paper presented at the Web-Age Information Management</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<pubPlace>Nanchang, China</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
		<title level="m" type="main">A study on operations used in text summarization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Takeuchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Nara Institute of Science and Technology</orgName>
		</respStmt>
	</monogr>
	<note>PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Multi-document text summarization -a survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wagle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khedkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Data Mining and Advanced Computing (SAPIENCE)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">A review on neural network based abstractive text summarization models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mistree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>I2CT</pubPlace>
		</imprint>
	</monogr>
	<note>Paper presented at the 2019 IEEE</note>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Summarizing scientific articles: Experiments with relevance and rhetorical status</title>
		<author>
			<persName><forename type="first">S</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moens</surname></persName>
		</author>
		<idno type="DOI">10.1162/089120102762671936</idno>
		<ptr target="https://doi.org/10.1162/089120102762671936" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="409" to="445" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Summarization evaluation using transformed basic elements</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Analytics Conference (TAC-08)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">AlgorithmSeer: A system for extracting and searching for algorithms in scholarly big data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tuarob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<idno type="DOI">10.1109/TBDATA.2016.2546302</idno>
		<ptr target="https://doi.org/10.1109/TBDATA.2016.2546302" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<monogr>
		<title level="m" type="main">Regression-based summarization of email conversations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">An unsupervised constrained optimization to compressive summarization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vanetik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litvak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Churkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Last</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2019.08.079</idno>
		<ptr target="https://doi.org/10.1016/j.ins.2019.08.079" />
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">509</biblScope>
			<biblScope unit="page" from="22" to="35" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">&amp;</forename><surname>Venter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sobieszczanski-Sobieski</surname></persName>
		</author>
		<idno type="DOI">10.2514/2.2111</idno>
		<ptr target="https://doi.org/10.2514/2.2111" />
	</analytic>
	<monogr>
		<title level="j">Journal</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1583" to="1589" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<monogr>
		<title level="m" type="main">Collaborative ranking-based text summarization using a metaheuristic approach. Paper presented at the Emerging Technologies in Data Mining and Information Security</title>
		<author>
			<persName><forename type="first">P</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Om</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Factual instance tweet summarization and opinion analysis of sport competition</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Janga Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soft Computing and Signal Processing</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">A study of abstractive summarization using semantic representations and discourse level information</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C V</forename><surname>Vilca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A S</forename><surname>Cabezudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th International Conference on Text, Speech, and Dialogue</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">The impact of rule-based text generation on the quality of abstractive summaries</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vodolazova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lloret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Recent Advances in Natural Language Processing</title>
		<meeting>the International Conference on Recent Advances in Natural Language Processing<address><addrLine>Varna, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<monogr>
		<title level="m" type="main">Extractive text summarization: Can we use the same techniques for any text? Paper presented at the Natural Language Processing and Information Systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vodolazova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lloret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palomar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Extractive text summarization: Can we use the same techniques for any text?</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vodolazova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lloret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural language processing and information systems: 18th international conference on applications of natural language to information systems, NLDB 2013</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Métais</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Meziane</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Saraee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Sugumaran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vadera</surname></persName>
		</editor>
		<meeting><address><addrLine>Salford, UK; Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013-06-19">2013. June 19-21, 2013</date>
			<biblScope unit="page" from="164" to="175" />
		</imprint>
	</monogr>
	<note type="report_type">Proceedings</note>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Enhancing the sentence similarity measure by semantic and syntactico-semantic knowledge</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gargouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben Hamadou</surname></persName>
		</author>
		<idno type="DOI">10.1007/s40595-016-0080-2</idno>
		<ptr target="https://doi.org/10.1007/s40595-016-0080-2" />
	</analytic>
	<monogr>
		<title level="j">Vietnam Journal of Computer Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="60" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<monogr>
		<title level="m" type="main">Integrating extractive and abstractive models for long text summarization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>BigData Congress</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">A Comprehensive method for text summarization based on latent semantic analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural language processing and Chinese computing: Second CCF conference, NLPCC 2013</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</editor>
		<meeting><address><addrLine>Chongqing, China; Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013-11-15">2013. November 15-19, 2013</date>
			<biblScope unit="page" from="394" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<monogr>
		<title level="m" type="main">Text summarization using adaptive neuro-fuzzy inference system</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Warule</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Sawarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Automatic generation of story highlights</title>
		<author>
			<persName><forename type="first">K</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName><surname>Wordnet</surname></persName>
		</author>
		<ptr target="https://wordnet.princeton.edu/" />
		<editor>C. Fellbaum Ed</editor>
		<imprint>
			<date type="published" when="1998">26/4/2020. 1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">A topic modeling based approach to novel document automatic summarization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2017.04.054</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2017.04.054" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="12" to="23" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Supplement C</note>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Text summarization using sentiment analysis for DUC data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chatterjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Information Technology (ICIT)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for extractive document summarization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2018.01.020</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2018.01.020" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">284</biblScope>
			<biblScope unit="page" from="52" to="62" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Text summarization using unsupervised deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yousefi-Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hamey</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2016.10.017</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2016.10.017" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="93" to="105" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Document summarization for answering non-factoid queries</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yulianti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2017.2754373</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2017.2754373" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="28" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<monogr>
		<title level="m" type="main">Efficient summarization with readagain and copy mechanism</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1611.03382.pdf" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Paper presented at the ICLR 2017. Last Accessed: 26/4/2020</note>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Event-based summarization for scientific literature in Chinese</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2018.03.052</idno>
		<ptr target="https://doi.org/10.1016/j.procs.2018.03.052" />
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="88" to="92" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">An improved LDA multi-document summarization model based on TensorFlow</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">CMiner: Opinion extraction and summarization for Chinese microblogs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2016.2541148</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2016.2541148" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1650" to="1663" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Augmenting neural sentence summarization through extractive summarization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Chinese Computing</title>
		<meeting><address><addrLine>Dalian, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">He worked as Professor Chair and Vice Dean of Faculty of Computers and Information at Cairo University and is currently a Professor and Chair of the Computer Science and Engineering Department at the American University in Cairo. Dr. Rafea reviewed many papers in several international journals and conferences. He was the Principal Investigator of many projects on machine translation, text mining, sentiment analysis, and knowledge engineering in collaboration with American and National Universities and Institutions. He has authored over 200 papers in conference proceedings, book chapters, and international and national journals. His research interests include natural language processing, machine translation, knowledge engineering, knowledge discovery, and data, text and web mining. Hoda K. Mohamed is a Professor at the</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wafaa</surname></persName>
		</author>
		<author>
			<persName><surname>El</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">from 2009 till now. Dr. Hoda obtained her M.Sc. from the Faculty of Engineering, Ain Shams University in 1983</title>
		<meeting><address><addrLine>Cairo, Egypt; Houston, Texas; Toulouse, France; Cairo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>Computer and Systems Engineering at Ain Shams University. She received her B.Sc. degree in Computer Engineering from Cairo University, Faculty of Engineering, Computer Engineering Department, Giza, Egypt. Eng. Wafaa received her M.Sc. degree in Computer and Systems Engineering from Ain Shams University, Faculty of Engineering, Computer and Systems Engineering Department ; Electronics and Communication Engineering from Cairo University and Ph.D. in Computer Science from University Paul Sabatier ; Computer and Systems Engineering Department, Faculty of Engineering, Ain Shams University ; Computer Engineering and Systems</orgName>
		</respStmt>
	</monogr>
	<note>Kassas is currently working toward her Ph. her Ph.D. from the Faculty of Engineering, Ain Shams University in 1992, and was promoted to Assoc. Prof. in 2001. She is a reviewer. Egypt. She has authored about 50 papers since 2009. Her research interests are on intelligent systems, E-learning systems, data mining, database systems, software engineering, natural language processing, cloud computing, and image processing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
