{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation as eval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_summary_path() -> str:\n",
    "    result = os.path.abspath('..\\\\..\\\\resources\\\\benchmark\\\\pdf_summary')\n",
    "    return result\n",
    "\n",
    "def get_pdf_summary_subpath(subpath:str) -> str:\n",
    "    result = os.path.join(\n",
    "        get_pdf_summary_path(), \n",
    "        subpath\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load Files into dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumAlgo(Enum):\n",
    "    LSA = 'lsa'\n",
    "    TEXTRANK = 'textrank'\n",
    "    FALCON = 'falcon'\n",
    "    TEXTRANK_BART = 'textrank_bart'\n",
    "    TEXTRANK_FALCON = 'textrank_falcon'\n",
    "    TEXTRANK_STABLELM = 'textrank_stablelm'\n",
    "    REF = 'ref'\n",
    "\n",
    "def get_file_name_list() -> list[str]:\n",
    "    list = []\n",
    "    for (_, _, filenames) in os.walk(get_pdf_summary_subpath(SumAlgo.REF.value)):\n",
    "        for filename in filenames:\n",
    "            list.append(filename.rstrip('.grobid.tei.xml.json'))\n",
    "    return list\n",
    "\n",
    "def get_file_path_list(sumAlgo:SumAlgo) -> list[str]:\n",
    "    list = []\n",
    "    for (dirpath, _, filenames) in os.walk(get_pdf_summary_subpath(sumAlgo.value)):\n",
    "        for filename in filenames:\n",
    "            list.append(os.path.join(dirpath, filename))\n",
    "    return list\n",
    "\n",
    "def get_file_dict(sumAlgo:SumAlgo) -> dict:\n",
    "    path_list = get_file_path_list(sumAlgo) \n",
    "    file_name_list = get_file_name_list()\n",
    "    result = {}\n",
    "    for i in range(0, len(file_name_list)):\n",
    "        path = path_list[i]\n",
    "        file_name = file_name_list[i]\n",
    "        result[file_name] = read_file(path)\n",
    "    return result \n",
    "\n",
    "def read_file(path:str) -> any:\n",
    "    result = ''\n",
    "    with open(path, 'r') as f:\n",
    "        result = json.load(f)\n",
    "    return result\n",
    "\n",
    "def get_segment_dict(sumAlgo:SumAlgo) -> dict:\n",
    "    file_dict = get_file_dict(sumAlgo)\n",
    "    file_name_list = get_file_name_list()\n",
    "    result = {}\n",
    "    for file_name in file_name_list:\n",
    "        value = file_dict[file_name]\n",
    "        segments = value['segments'] \n",
    "        result[file_name] = segments\n",
    "    return result\n",
    "\n",
    "ref_segment_dict = get_segment_dict(SumAlgo.REF)\n",
    "textrank_segment_dict = get_segment_dict(SumAlgo.TEXTRANK)\n",
    "lsa_segment_dict = get_segment_dict(SumAlgo.LSA)\n",
    "file_name_list = get_file_name_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SumAlgo.LSA</th>\n",
       "      <th>SumAlgo.FALCON</th>\n",
       "      <th>SumAlgo.TEXTRANK_BART</th>\n",
       "      <th>SumAlgo.TEXTRANK_FALCON</th>\n",
       "      <th>SumAlgo.TEXTRANK_STABLELM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rouge_1</th>\n",
       "      <td>0.576021</td>\n",
       "      <td>0.598212</td>\n",
       "      <td>0.661115</td>\n",
       "      <td>0.648827</td>\n",
       "      <td>0.579882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_2</th>\n",
       "      <td>0.245746</td>\n",
       "      <td>0.367898</td>\n",
       "      <td>0.425214</td>\n",
       "      <td>0.455075</td>\n",
       "      <td>0.260122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_l</th>\n",
       "      <td>0.576021</td>\n",
       "      <td>0.598212</td>\n",
       "      <td>0.661115</td>\n",
       "      <td>0.648827</td>\n",
       "      <td>0.579882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SumAlgo.LSA  SumAlgo.FALCON  SumAlgo.TEXTRANK_BART  \\\n",
       "rouge_1     0.576021        0.598212               0.661115   \n",
       "rouge_2     0.245746        0.367898               0.425214   \n",
       "rouge_l     0.576021        0.598212               0.661115   \n",
       "\n",
       "         SumAlgo.TEXTRANK_FALCON  SumAlgo.TEXTRANK_STABLELM  \n",
       "rouge_1                 0.648827                   0.579882  \n",
       "rouge_2                 0.455075                   0.260122  \n",
       "rouge_l                 0.648827                   0.579882  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean rouge_1 of all paragraphs\n",
    "def get_header_dict(segments:list) -> dict:\n",
    "    result = {}\n",
    "    for segment in segments:\n",
    "        header = segment['header']\n",
    "        content = segment['content']\n",
    "        result[header] = content\n",
    "    return result \n",
    "\n",
    "def get_eval_by_header(ref_header_dict:dict, test_header_dict:dict, eval_method:eval.EvalMethod, sum_algo:SumAlgo) -> pd.DataFrame:\n",
    "    ref_key_num = len(ref_header_dict.keys())\n",
    "    test_key_num = len(test_header_dict.keys())\n",
    "    if(ref_key_num != test_key_num):\n",
    "        print(\"The number of keys do not match: \" + len(ref_header_dict.keys()) + \" \" + len(ref_header_dict.keys()))\n",
    "            \n",
    "    result = {}\n",
    "    for i in range(0, len(test_header_dict.keys())):\n",
    "        header = list(ref_header_dict.keys())[i]\n",
    "        ref_content = ref_header_dict[header]\n",
    "        test_content = test_header_dict[header]\n",
    "        # print(sum_algo.value)\n",
    "        # print(test_content)\n",
    "        eval_result = eval.eval_method_dict[eval_method](test_content, ref_content)\n",
    "        result[header] = [eval_result]\n",
    "    \n",
    "    return pd.DataFrame(result, index=[eval_method.value]).transpose()\n",
    "\n",
    "def evaluate(sum_algo:SumAlgo, ref_segment_dict:dict, eval_method:eval.EvalMethod) -> pd.DataFrame:\n",
    "    test_segment_dict = get_segment_dict(sum_algo)\n",
    "    file_name_list = get_file_name_list()\n",
    "    result = {}\n",
    "    result_list = []\n",
    "    for file_name in file_name_list:\n",
    "        ref_header_dict = get_header_dict(ref_segment_dict[file_name])\n",
    "        test_header_dict = get_header_dict(test_segment_dict[file_name])\n",
    "\n",
    "        eval_header = get_eval_by_header(ref_header_dict, test_header_dict, eval_method, sum_algo)\n",
    "        eval_value_list = eval_header[eval_method.value].to_list()\n",
    "        eval_average = sum(eval_value_list) / len(eval_value_list)\n",
    "        # print(eval_average) \n",
    "        result_list.append(eval_average)\n",
    "        # break\n",
    "    result[eval_method.value] = result_list\n",
    "    df = pd.DataFrame(result, index=file_name_list)\n",
    "    df.index.name\n",
    "    return df\n",
    "\n",
    "def evaluate_all_method(sum_algo:SumAlgo, ref_segment_dict:dict) -> pd.DataFrame:\n",
    "    file_name_list = get_file_name_list()\n",
    "    file_name_list.append(\"Total\")\n",
    "    file_name_list.append(\"Average\")\n",
    "    result = {}\n",
    "    for method in eval.EvalMethod:\n",
    "        eval_df = evaluate(sum_algo, ref_segment_dict, method)\n",
    "        result_list = eval_df[method.value].to_list()   \n",
    "\n",
    "        total = 0\n",
    "        total = sum(result_list)\n",
    "        average = total/len(result_list)\n",
    "        result_list.append(total)\n",
    "        result_list.append(average)\n",
    "\n",
    "        result[method.value] = result_list\n",
    "\n",
    "    df = pd.DataFrame(result, index=file_name_list)\n",
    "    df.index.name = sum_algo.value\n",
    "    return df\n",
    "\n",
    "def evaluate_all_sum_algo(ref_segment_dict:dict) -> pd.DataFrame:\n",
    "    # Data extraction\n",
    "    average_list_map = {}\n",
    "    method_list = [x.value for x in eval.EvalMethod]\n",
    "\n",
    "    for algo in SumAlgo:\n",
    "        if(algo == SumAlgo.REF or algo == SumAlgo.TEXTRANK):\n",
    "            continue\n",
    "        df = evaluate_all_method(algo, ref_segment_dict)\n",
    "\n",
    "        average_list = []\n",
    "        for method_value in method_list:\n",
    "            average = df[method_value][\"Average\"]\n",
    "            average_list.append(average)\n",
    "        average_list_map[algo] = average_list\n",
    "    \n",
    "    df = pd.DataFrame(average_list_map, index=method_list)\n",
    "    return df\n",
    "       \n",
    "# print(evaluate(lsa_segment_dict, ref_segment_dict, eval.EvalMethod.ROUGE_1))\n",
    "evaluate_all_method(SumAlgo.LSA, ref_segment_dict)\n",
    "evaluate_all_sum_algo(ref_segment_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
