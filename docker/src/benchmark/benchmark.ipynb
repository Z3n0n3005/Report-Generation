{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation as eval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_summary_path() -> str:\n",
    "    result = os.path.abspath('..\\\\..\\\\resources\\\\benchmark\\\\pdf_summary')\n",
    "    return result\n",
    "\n",
    "def get_pdf_summary_subpath(subpath:str) -> str:\n",
    "    result = os.path.join(\n",
    "        get_pdf_summary_path(), \n",
    "        subpath\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load Files into dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumAlgo(Enum):\n",
    "    LSA = 'lsa'\n",
    "    TEXTRANK = 'textrank'\n",
    "    REF = 'ref'\n",
    "\n",
    "def get_file_name_list() -> list[str]:\n",
    "    list = []\n",
    "    for (_, _, filenames) in os.walk(get_pdf_summary_subpath(SumAlgo.REF.value)):\n",
    "        for filename in filenames:\n",
    "            list.append(filename)\n",
    "    return list\n",
    "\n",
    "def get_file_path_list(sumAlgo:SumAlgo) -> list[str]:\n",
    "    list = []\n",
    "    for (dirpath, _, filenames) in os.walk(get_pdf_summary_subpath(sumAlgo.value)):\n",
    "        for filename in filenames:\n",
    "            list.append(os.path.join(dirpath, filename))\n",
    "    return list\n",
    "\n",
    "def get_file_dict(sumAlgo:SumAlgo) -> dict:\n",
    "    path_list = get_file_path_list(sumAlgo) \n",
    "    file_name_list = get_file_name_list()\n",
    "    result = {}\n",
    "    for i in range(0, len(file_name_list)):\n",
    "        path = path_list[i]\n",
    "        file_name = file_name_list[i]\n",
    "        result[file_name] = read_file(path)\n",
    "    return result \n",
    "\n",
    "def read_file(path:str) -> any:\n",
    "    result = ''\n",
    "    with open(path, 'r') as f:\n",
    "        result = json.load(f)\n",
    "    return result\n",
    "\n",
    "def get_segment_dict(sumAlgo:SumAlgo) -> dict:\n",
    "    file_dict = get_file_dict(sumAlgo)\n",
    "    file_name_list = get_file_name_list()\n",
    "    result = {}\n",
    "    for file_name in file_name_list:\n",
    "        value = file_dict[file_name]\n",
    "        segments = value['segments'] \n",
    "        result[file_name] = segments\n",
    "    return result\n",
    "\n",
    "ref_segment_dict = get_segment_dict(SumAlgo.REF)\n",
    "textrank_segment_dict = get_segment_dict(SumAlgo.TEXTRANK)\n",
    "lsa_segment_dict = get_segment_dict(SumAlgo.LSA)\n",
    "file_name_list = get_file_name_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     rouge_1   rouge_2  \\\n",
      "Deienno_et_al._-_2024_-_Accretion_and_Uneven_De...  0.449322  0.123100   \n",
      "Elokda_et_al._-_2024_-_Karma_An_Experimental_St...  0.836298  0.582345   \n",
      "Hansen_et_al._-_2024_-_Productivity_and_quality...  0.600328  0.152316   \n",
      "Onah_et_al._-_2023_-_A_Data-driven_Latent_Seman...  0.476862  0.393759   \n",
      "Peterson_-_2024_-_AI_and_the_Problem_of_Knowled...  0.644528  0.363274   \n",
      "Pires_and_Broom_-_2024_-_The_rules_of_multiplay...  0.537277  0.114212   \n",
      "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynami...  0.551279  0.110263   \n",
      "Smith and Coast - 2013 - The economic burden of...  0.598609  0.339455   \n",
      "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_C...  0.570800  0.103810   \n",
      "Viglietta_-_2024_-_History_Trees_and_Their_Appl...  0.447251  0.099845   \n",
      "Yu_et_al._-_2024_-_GreedLlama_Performance_of_Fi...  0.623672  0.320823   \n",
      "\n",
      "                                                     rouge_l  \n",
      "Deienno_et_al._-_2024_-_Accretion_and_Uneven_De...  0.449322  \n",
      "Elokda_et_al._-_2024_-_Karma_An_Experimental_St...  0.836298  \n",
      "Hansen_et_al._-_2024_-_Productivity_and_quality...  0.600328  \n",
      "Onah_et_al._-_2023_-_A_Data-driven_Latent_Seman...  0.476862  \n",
      "Peterson_-_2024_-_AI_and_the_Problem_of_Knowled...  0.644528  \n",
      "Pires_and_Broom_-_2024_-_The_rules_of_multiplay...  0.537277  \n",
      "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynami...  0.551279  \n",
      "Smith and Coast - 2013 - The economic burden of...  0.598609  \n",
      "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_C...  0.570800  \n",
      "Viglietta_-_2024_-_History_Trees_and_Their_Appl...  0.447251  \n",
      "Yu_et_al._-_2024_-_GreedLlama_Performance_of_Fi...  0.623672  \n"
     ]
    }
   ],
   "source": [
    "# Mean rouge_1 of all paragraphs\n",
    "def get_header_dict(segments:list) -> dict:\n",
    "    result = {}\n",
    "    for segment in segments:\n",
    "        header = segment['header']\n",
    "        content = segment['content']\n",
    "        result[header] = content\n",
    "    return result \n",
    "\n",
    "def get_eval_by_header(file_name:str, ref_header_dict:dict, test_header_dict:dict, eval_method:eval.EvalMethod) -> pd.DataFrame:\n",
    "    ref_key_num = len(ref_header_dict.keys())\n",
    "    test_key_num = len(test_header_dict.keys())\n",
    "    if(ref_key_num != test_key_num):\n",
    "        print(\"The number of keys do not match: \" + len(ref_header_dict.keys()) + \" \" + len(ref_header_dict.keys()))\n",
    "            \n",
    "    result = {}\n",
    "    for i in range(0, len(test_header_dict.keys())):\n",
    "        header = list(ref_header_dict.keys())[i]\n",
    "        ref_content = ref_header_dict[header]\n",
    "        test_content = test_header_dict[header]\n",
    "        eval_result = eval.eval_method_dict[eval_method](test_content, ref_content)\n",
    "        result[header] = [eval_result]\n",
    "    \n",
    "    return pd.DataFrame(result, index=[eval_method.value]).transpose()\n",
    "\n",
    "def evaluate(test_segment_dict:dict, ref_segment_dict:dict, eval_method:eval.EvalMethod) -> pd.DataFrame:\n",
    "    file_name_list = get_file_name_list()\n",
    "    result = {}\n",
    "    result_list = []\n",
    "    for file_name in file_name_list:\n",
    "        ref_header_dict = get_header_dict(ref_segment_dict[file_name])\n",
    "        test_header_dict = get_header_dict(test_segment_dict[file_name])\n",
    "\n",
    "        eval_header = get_eval_by_header(file_name, ref_header_dict, test_header_dict, eval_method)\n",
    "        eval_value_list = eval_header[eval_method.value].to_list()\n",
    "        eval_average = sum(eval_value_list) / len(eval_value_list)\n",
    "        # print(eval_average) \n",
    "        result_list.append(eval_average)\n",
    "        # break\n",
    "    result[eval_method.value] = result_list\n",
    "    return pd.DataFrame(result, index=file_name_list)\n",
    "\n",
    "def evaluate_all_method(test_segment_dict:dict, ref_segment_dict:dict) -> pd.DataFrame:\n",
    "    file_name_list = get_file_name_list()\n",
    "    result = {}\n",
    "    for method in eval.EvalMethod:\n",
    "        eval_df = evaluate(test_segment_dict, ref_segment_dict, method)\n",
    "        result_list = eval_df[method.value].to_list()   \n",
    "        result[method.value] = result_list\n",
    "    \n",
    "    return pd.DataFrame(result, index=file_name_list)\n",
    "\n",
    "       \n",
    "# print(evaluate(lsa_segment_dict, ref_segment_dict, eval.EvalMethod.ROUGE_1))\n",
    "print(evaluate_all_method(lsa_segment_dict, ref_segment_dict))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
