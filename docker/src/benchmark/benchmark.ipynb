{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation as eval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_summary_path() -> str:\n",
    "    result = os.path.abspath('..\\\\..\\\\resources\\\\benchmark\\\\pdf_summary')\n",
    "    return result\n",
    "\n",
    "def get_pdf_summary_subpath(subpath:str) -> str:\n",
    "    result = os.path.join(\n",
    "        get_pdf_summary_path(), \n",
    "        subpath\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load Files into dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumAlgo(Enum):\n",
    "    LSA = 'lsa'\n",
    "    TEXTRANK = 'textrank'\n",
    "    REF = 'ref'\n",
    "\n",
    "def get_file_name_list() -> list[str]:\n",
    "    list = []\n",
    "    for (_, _, filenames) in os.walk(get_pdf_summary_subpath(SumAlgo.REF.value)):\n",
    "        for filename in filenames:\n",
    "            list.append(filename)\n",
    "    return list\n",
    "\n",
    "def get_file_path_list(sumAlgo:SumAlgo) -> list[str]:\n",
    "    list = []\n",
    "    for (dirpath, _, filenames) in os.walk(get_pdf_summary_subpath(sumAlgo.value)):\n",
    "        for filename in filenames:\n",
    "            list.append(os.path.join(dirpath, filename))\n",
    "    return list\n",
    "\n",
    "def get_file_dict(sumAlgo:SumAlgo) -> dict:\n",
    "    path_list = get_file_path_list(sumAlgo) \n",
    "    file_name_list = get_file_name_list()\n",
    "    result = {}\n",
    "    for i in range(0, len(file_name_list)):\n",
    "        path = path_list[i]\n",
    "        file_name = file_name_list[i]\n",
    "        result[file_name] = read_file(path)\n",
    "    return result \n",
    "\n",
    "def read_file(path:str) -> any:\n",
    "    result = ''\n",
    "    with open(path, 'r') as f:\n",
    "        result = json.load(f)\n",
    "    return result\n",
    "\n",
    "def get_segment_dict(sumAlgo:SumAlgo) -> dict:\n",
    "    file_dict = get_file_dict(sumAlgo)\n",
    "    file_name = get_file_name_list()\n",
    "    result = {}\n",
    "    for file_name in file_name_list:\n",
    "        value = file_dict[file_name]\n",
    "        segments = value['segments'] \n",
    "        result[file_name] = segments\n",
    "    return result\n",
    "\n",
    "ref_segment_dict = get_segment_dict(SumAlgo.REF)\n",
    "textrank_segment_dict = get_segment_dict(SumAlgo.TEXTRANK)\n",
    "lsa_segment_dict = get_segment_dict(SumAlgo.LSA)\n",
    "file_name_list = get_file_name_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'header': 'INTRODUCTION', 'content': 'The few works that accounted for collisional evolution of the MAB (e.g.; To do that, it is important that we consider not only the MAB total mass, but also its SFD.Only three objects in the current MAB have D > 500 km.'}, {'header': 'MODEL', 'content': \"To model the evolution of the MAB SFD and the accretion of planetesimals with D > 500 km in the MAB region we first assume that planetesimals were formed within the first 0.5 Myr after Calcium-Aluminum-Inclusions (CAIs) based on the methods by For the total primordial mass of the MAB we assumed that the initial distribution of planetesimals, uniformly distributed between 1.8 and 3.6 au The exact time and radial distance of Jupiter's formation is unknown (e.g.; Chambers 2021; We followed the accretion of objects within our MAB region for 5 Myr. This interval is presumably the time the gas in the solar nebula dispersed in the outer solar system based on the relative ages of the youngest CB-chondrites to CAIs To follow the dynamics and accretion of the MAB under the considerations above, we used the code known as LIPAD (Lagrangian Integrator for Planetary Accretion and Dynamics; All planetesimals within all tracer particles are allowed to have self-gravitational interactions and to collide with one-another throughout the simulation.\"}, {'header': 'MAB, DATASET, AND COMPARISON WITH MPC', 'content': 'Dper is for the percentage of depletion Dper ‚âà 100 √ó (1 -D f ac ).\\nLabels all and AM D JS P S /P J are for different cuts in the data by ‚Ä¢ C19 all refers to results taken from ‚Ä¢ C19 AM DJS P S /P J is also for results from The work by Our analyses clearly demonstrate that MAB depletion during the giant planet instability is not uniform.'}, {'header': 'RESULTS', 'content': \"Figure With the post-depletion SFDs evaluated as in Figure 12 Following The following rows in Table Our results, then indicate that, regardless of Jupiter's inclusion and the exact timing of the nebula gas dispersal, while following our fiducial case, the current overall S-complex SFD (dashed in Figure EiMB IMB CMB OMB EoMB MAB Current/Post-Depletion 0.00 7.37√ó10 -6 1.85√ó10 -5 5.50√ó10 -6 9.40√ó10 -8 ‚âà 3.15√ó10 -5Pre-Depletion by D18 -5.85√ó10 -4 2.03√ó10 -4 5.94√ó10 -5 9.62√ó10 -6 ‚â≥ 8.57√ó10 -4Pre-Depletion by C19 all 0.00 7.97√ó10 -4 1.34√ó10 -3 1.61√ó10 -4 1.35√ó10 -3 ‚âà 3.65√ó10 -3Pre-Depletion by C19 AM D JS P S /P J -3.08√ó10 -3 9.95√ó10 -4 5.15√ó10 -5 3.52√ó10 -4 ‚â≥ 4.48√ó10 -3  0.0016 MMSN following our methods in Section 2, which can still be approximated by 0.002 MMSN (yellow in Figure We should once again stress that our estimate for the total MAB primordial mass (formed at 0.5 Myr after CAIs) above is an upper limit.\"}, {'header': 'CONCLUSIONS', 'content': 'This is important as it is well known that the giant planet instability The first main result of the present work is that we found the MAB depletion is uneven, i.e., different radial subregions of the MAB are depleted at different rates (Table The second main result of the present work is that we found the maximum total mass that could have formed in the primordial MAB at around 0.5 Myr after CAIs is likely to be smaller than ‚âà 2.14√ó10 -3 M ‚äï .'}]\n",
      "[{'header': 'INTRODUCTION', 'content': \"Efficiency and fairness in determining who gets what and when are the two major objectives of economists thinking about scarcity, and many interesting matchmaking and market design solutions have been proposed Such mechanisms have been proposed previously in non-human contexts to manage scarcity in, for example, traffic Karma and the aforementioned choice system are different from other token-based mechanisms that have been implemented and proposed in the past, including those that use tradable credits Figure The example illustrates that karma is similar in spirit to the 'universal currency' of reputation Further, the example also illustrates another feature of karma, as was mentioned above, that distinguishes karma from fixed-value tokens and from other forms of rules ensuring turn-taking: it allows individuals to dynamically express how intense their private preference for the resource is.\\nIndeed, To date, the literature on karma focused on the theoretical properties of the mechanism compared with random allocation and other domain-specific schemes such as dynamic pricing or max-min allocation Our main findings summarize as follows.\"}, {'header': 'EXPERIMENTAL METHODS', 'content': 'After the redistribution the game proceeds to the next round ùë° + 1.Table We follow a 2x2 factorial treatment design, where we vary the dynamic urgency process of the participants and the richness of the karma scheme.\\nThe bonus fee is determined according to the following rule:‚Ä¢ If the participant is inactive for more than 6 consecutive rounds, they are considered to not complete the experiment and are not awarded any payoff, i.e., ùúô fix = ùúô bon = 0; ‚Ä¢ Otherwise, the bonus fee is computed as an affine function of the score, given byThis rule linearly interpolates between two payment levels: ùúô targ is the payment associated with a target score ùë† targ ; and ùúô rand is the payment associated with the expected score for random bidding ùë† rand .'}, {'header': 'RESULTS', 'content': 'Then the efficiency gain is defined asand gives the relative improvement of participant ùëñ with respect to the ex-ante expected random allocation.\\nThe most favorable combination is high stake with binary bidding, both in terms of median efficiency gains and in terms of the distribution of gains as most individuals are on the benefiting side.Remark: With the best intentions, we had fully pre-registered design and analysis of our experiments.\\nFigure The findings of Figure Figure Notice that the variability in the ex-post efficiency gains under random allocation, cf.'}, {'header': 'DISCUSSION', 'content': 'To investigate this, we compared the aggregate efficiency gains realized in our online experiments to those that would be achieved in theory in a short-sighted equilibrium, as predicted by Another important consideration regarding implementation of karma with human participants is whether a simpler scheme (binary) or a richer scheme (full range) is favourable.\\nTheoretically, Nash equilibrium under binary bidding will lose in efficiency with more than two urgency levels, and it remains an open question to test what the trade-off is between behavioral simplicity of a binary (or otherwise limited) bidding scheme and the theoretical benefits that come with richer schemes.'}]\n",
      "[{'header': 'Introduction', 'content': 'The productivity index ranges from 0 (completely unproductive) to 1 (completely productive), and may take into consideration factors such as absence from work due to ill health (absenteeism), reduced productivity while at work (presenteeism) and premature exit from the workforce (e.g., Economic evaluation of policies to improve occupational health and safety is one field of research where productivity outcome measures following the broader PALY idea are applied extensively (e.g., Our approach builds upon the framework introduced in Hougaard et al.'}, {'header': 'Preliminaries', 'content': 'The following axiom will guarantee such a feature as a byproduct.The axiom productivity invariance at full health and common time states that, for any two individuals with common lifetime and full health, it makes no difference who gains in productivity.PIFHCT: For each d ‚àà D, each pair i, j ‚àà N with a i = a j = a * and t i = t j = t, and eachAs the next result states, adding this axiom to those in Theorem 2 we characterize the affine PALY evaluation function, which evaluates distributions by means of the aggregation of individual PALYs in society, when submitted first to an affine and non-decreasing function.Formally,where Œ± ‚àà [0, 1].The following statements are equivalent:1.'}, {'header': 'Compromising between QALYs and PALYs', 'content': 'The purpose of this section is to dismiss those axioms, while obtaining characterizations of natural compromises between those focal (albeit polar) evaluation functions.For instance, the productivity-and-quality-adjusted life years (PQALY) evaluation function evaluates distributions by means of the weighted (through productivity and health levels) aggregate time span the distribution yields, so that health, productivity and lifespan of individuals enter the evaluation function multiplicatively.\\nFormally,where q : A ‚Üí [0, 1] is a health state quality weight satisfying 0 ‚â§ q(a i ) ‚â§ q(a * ) = 1 for eachAs the next result states, this evaluation function is characterized when we dismiss health independence in the previous result (characterizing PALYs), and strengthen the other two independence axioms to consider the following ones.First, time invariance at common health and productivity, which states that for two individuals at common health and productivity, it does not matter to the social planner who receives the extra life years.TICHP: For each d ‚àà D, each pair i, j ‚àà N with a i = a j = a and p i = p j = p, and each c > 0,Second, productivity invariance at common health and time, which says that for two individuals at common health and time, it does not matter who gains in productivity.PICHT: For each d ‚àà D, each pair i, j ‚àà N with a i = a j = a and t i = t j = t, and each c > 0Theorem 5 The following statements are equivalent:1.'}, {'header': 'satisfies COMMON, TICHP, PICHT, and TIUP.', 'content': 'is represented by an evaluation function (8).Theorem 8 implies that if the social planner endorses the view that if an intervention leads to extra life years, it does not matter which particular individual (among individuals with the same level of health and productivity) receives these extra life years (axiom TICHP), then the evaluation will be via a weighted aggregation of the lifetimes the intervention yields.\\nis represented by a HPYE evaluation function (9).Theorem 9 implies that if the social planner endorses the view that if an intervention leads to extra life years for individuals with full health and maximal productivity, it does not matter which particular individual receives these extra life years (axiom TIFHP), then the evaluation will be via the unweighted aggregation of the HPYEs the intervention yields (which are well defined due to COMMON).Finally, we can define the so-called generalized HPYE evaluation function by the unweighted aggregation of the image of HPYEs the distribution yields to a certain function.'}, {'header': 'Discussion', 'content': 'Hence the interest of our axiomatic approach.If the analyst (working on behalf of a social planner) is of the view that health effects and productivity effects are both important outcomes when assessing the benefit of the working environment intervention, evaluation functions ( Intervention A: 1000 individuals obtaining 1 year in full health and having zero productivity.Intervention B: x individuals obtaining 1 year in health state a and having zero productivity.In particular, each respondent would be asked to identify the number of individuals x in Intervention B to be indifferent between both interventions.'}, {'header': 'Concluding remarks', 'content': 'Then, we may write: œï(a, p, t i ) = w(a, p)t i , where 0 ‚â§ w(a, p) ‚â§ w(a * , p) ‚â§ w(a * , 1) = 1, and 0 ‚â§ w(a, p) ‚â§ w(a, 1) ‚â§ w(a * , 1) = 1 for each (a, p) ‚àà A √ó [0, 1], as desired.Suppose first that is represented by a PHEF satisfying Then, for each c > 0,Conversely, assume now that preferences satisfy all the axioms in the statement of Theorem where g : R + ‚Üí R is strictly increasing.\\nThus, for each pair d, d ‚Ä≤ ‚àà D, and, therefore, is indeed represented by an evaluation function satisfying (5), as desired.Suppose first that is represented by a PHEF satisfying Conversely, assume now that preferences satisfy all the axioms in the statement of Theorem for each a ‚àà A, it follows that is represented by an evaluation function satisfying Case 3.'}]\n",
      "[{'header': 'I. INTRODUCTION', 'content': 'However, this study presents a novel approach to topic modelling by performing extracDve summarizaDon on over 100 arDcles related to genes and associated diseases and feeding the summary as an input argument a Latent Dirichlet AllocaDon (LDA) model in order to perform the topic modelling.\\nWith so many documents being extracted from social media, review comments from online plaWorms and microblogs as TwiVer, a huge amount of natural language data is being mined and are available to be analysed AutomaDc text summarizaDon is the process of performing specific NLP task by producing a concise summary of documents (single or mulDple) without any manual support while preserving the meaning or important points of the original document ‚Ä¢ How automated text summarizaDon techniques were used in an extracDve summary of arDcles?‚Ä¢ How topic modelling models were used in producing emerging terms that are related to mulDple and different journal arDcles?\\nThe experimental results show that the proposed model achieves good performance in terms of the document summary and the topic modelling informaDon retrieved from the full document.'}, {'header': 'II. RELATED WORK', 'content': 'Unless these text data are extracted and make meaning, then the most important and relevant informaDon would be lost.\\nSummarizaDon is described as the process of presenDng huge data informaDon in a concise manner while focusing on the most useful secDons of the data whilst preserving the original meaning '}, {'header': 'A. Summariza-on', 'content': ''}, {'header': 'B. Topic Modelling', 'content': 'Topic modelling is the process of labelling and describing documents into topics.\\nThis is an unsupervised machine learning technique for abstracDng topics from collecDons of documents '}, {'header': 'C. Latent Dirichlet Alloca-on', 'content': ''}, {'header': 'III. METHODS', 'content': \"This dicDonary of terms was used to build a vectorised corpus of lexicon LDA model.\\nOne of the key approaches that was used in the experiment was the 'pyLDAvis.gensim.prepare' method which takes as an argument our LDA model, the corpus and the derived lexicon which contains the dicDonary terms for the study \"}, {'header': 'A. Model Descrip-on', 'content': ''}, {'header': '1)', 'content': ''}, {'header': 'Sentscores[S] = Wordfreq[W]', 'content': '(1)During the sentence model processing interval, the length of the sentence is either increased or reduced by certain values within the sentence scores dicDonary.\\nTherefore, new sentences are added into the sentence dicDonary scores.'}, {'header': 'Sentscores[S]+ = Wordfreq[W]', 'content': '(2)2) Word frequency:: DicDonary of word frequency corpus was generated within the model.\\nThe word frequencies were selected automaDcally based on the prevalence or occurrence of the words in the corpus dicDonary created in the model (see Figure The next equaDon allows us to calculate the maximum word in the word frequencies (see equaDon 4).'}, {'header': 'Word', 'content': ''}, {'header': 'B. Research Pipeline', 'content': 'The arDcles scraped from the web were all related to medical science research.\\nPapers related to diseases and the mutated genes causaDon were extracted for this study '}, {'header': '2)', 'content': 'In the feature extracDon process, we parse the web arDcles source code in order to extract the textual material needed for the final summary.\\nDuring the process of formaTng the clean arDcles, we performed extra filtering of special characters from the processed text in order to find and replace these symbols automaDcally.'}, {'header': '3)', 'content': 'Stopwords: We further removed a list of stop-words from the propocessed arDcles.\\nWords such as pronounce that are not necessary or essenDal for the final summary (see Figure '}, {'header': '4) Topic Modelling & Visualiza-on:', 'content': ''}, {'header': 'IV. MODEL', 'content': ''}, {'header': 'B. Defining Saliency Term', 'content': 'The possibility that the emerge word w was generated from the LDA topic model (TM).We also compute the marginal probability P(TM): -with the possibility that any word w 0 randomly selected was generated by TM.'}, {'header': 'U(6)', 'content': ''}, {'header': '=5', 'content': 'The uniqueness of each term is described as how significance and semanDcally associated they are to the topics.\\nFor example, a term could be semanDcally associated to more than one topic.'}, {'header': 'V. LATENT SEMANTIC ANALYSIS', 'content': 'Latent SemanDc Analysis (LSA) is a robust Algebraic and StaDsDcal method which extracts hidden semanDc structures of words and sentences.'}, {'header': 'A. Sample Extracted Summary', 'content': 'The heapq is used in implemenDng the priority queues for word frequencies in sentences with higher weight is given more priority in processing the summary.'}, {'header': 'B. Findings', 'content': ''}, {'header': 'VI. ROUGE: RELIABILITY & VALIDITY OF MODEL', 'content': 'ROUGE is a metric evaluaDon model which stands for Recall Oriented Understudy for Gisting Evaluation.\\nIt is an intrinsic metric for automaDcally evaluaDng document summaries '}, {'header': 'TABLE III ROUGE METRICS MEASUREMENT SUMMARIES', 'content': \"System and Human Annotated Summaries Type Summary SSummary 'Some of the genes in the BCAA metabolic pathway such as MLYCD (rank 164)HADHB (rank 354)IVD (rank 713)MUT (rank 921)and PCCB (rank 684) are also ranked highly by Hridaya.\"}, {'header': 'Comparing the system generated summary with a new human', 'content': ''}, {'header': 'A. Procedure: Recall & Precision', 'content': 'In this study, we combined and computerised both the Precision and Recall and further report the F1 -score measure.In order to ascertain the validity of the study, we measured ROUGE-N, ROUGE -S and ROUGE -L which are the granularity of texts that was compared between the system summaries and the reference or human annotated model summaries.'}, {'header': 'VII. RESULTS & FINDINGS', 'content': 'The terms in the topic modelling show text which are mostly frequent in the document these were depicted by the size of the circle (as seen in Figures The distance between two or more topics is an approximaDon of their semanDc relaDonship.'}, {'header': 'IX. DISCUSSION', 'content': ''}, {'header': 'X. CONCLUSION', 'content': ''}]\n",
      "[{'header': 'Introduction', 'content': 'The capability of large language models (LLMs) to generate text with near-zero human effort, however, along with models to generate images, audio, and video, suggest that the data to which humans are exposed may come to be dominated by AI-generated or AI-aided processes.Researchers have noted that the recursive training of AI models on synthetic text may lead to degeneration, known as \"model collapse\" The initial effect of AI-generated information is presumably limited, and existing work on the harms of AI rightly focuses on the immediate effects of false information spread by \"deepfakes\" Researchers and engineers are currently building a variety of systems whereby AI would mediate our experience with other humans and with information sources.'}, {'header': 'Summary of Main Contributions', 'content': 'We examine through simulations the conditions under which individuals are sufficiently informed to prevent knowledge collapse within society.\\nFinally, we conclude with an overview of possible solutions to prevent knowledge collapse in the AI-era.'}, {'header': 'Related Work', 'content': 'Yeh Meng-te, for example, argued in the twelfth century that the rise of books led to a decline in the practice of memorizing and collating texts that contributed to a decline of scholarship and the repetition of errors We focus on recent work on the role of digital platforms and social interactions, and mention only in passing the literature on historical innovations and knowledge (e.g. The following section considers research on the impact of recommendation algorithms and self-selection on social media, and how this might generate distorted and polarizing opinions, as an analogy for understanding the transformation brought about by reliance on AI.'}, {'header': 'The media, filter bubbles and echo chambers', 'content': '1 This general dynamic may hold even if traditional media and newspapers were themselves dynamic systems interacting with their consumers, markets and advertisers, and themselves adapting their message to specific communities and preferences (e.g. The second main line of analysis focuses on \"filter bubbles,\" whereby the content to which users are exposed is selected based on a recommendation system.'}, {'header': 'Network effects and Information Cascades', 'content': 'Information cascade models provide one approach to explaining a kind of herd behavior (where diverse and free individuals nonetheless make similar decisions).\\nBy extension, we could consider the generalization of these networks to the case where LLMs play a key role as (possibly influential) nodes, or as determining how an individual navigates a knowledge graph.'}, {'header': 'Model collapse', 'content': 'GANs are based on a generator neural network that proposes, e.g. an image, and a discriminator attempts to predict whether a given image is created by the generator or is a real image from the dataset.'}, {'header': 'Known biases in LLMs', 'content': 'Newer AI models such as LLMs are not immune to the problems of bias identified and measured in machine learning algorithms Recent work attempts to address these issues through a variety of methods, for example by upsampling underrepresented features on which prediction is otherwise sub-optimal One particular area in which the diversity of LLM outputs has been analyzed is on a token-by-token level in the context of decoding strategies.'}, {'header': 'A Model of Knowledge Collapse Defining Knowledge Collapse', 'content': 'Or, to cite specific examples, the ancient Romans had a recipe for concrete that was subsequently lost, and despite progress we have not yet re-discovered the secrets of its durability The distribution of knowledge across individuals also varies over time.'}, {'header': 'Model Overview', 'content': 'Depending on how their utility is calculated (not a substantive focus here), these could be interpreted as different expected returns from innovation (e.g.technooptimists versus pessimists), or their relative ability or desire to engage in innovation.We model knowledge as a process of approximating a (Students t) probability distribution.The set of individuals who decide to invest in information receive a sample from the true distribution, while those that invest in the AI-generated sample receive a sample from a version of the true distribution which is truncated at œÉ tr standard deviations above and below the mean.'}, {'header': 'Results', 'content': 'Perhaps the heliocentric view espoused by Aristarchus of Samos in the 3rd century BCE would have been more readily (re)considered if his works had not been neglected For subsequent results illustrating the tradeoff of different parameters, we plot the Hellinger distance between public knowledge at the end of the 100 rounds and the true distribution.'}, {'header': 'Discussion', 'content': 'If a user asks, for example, \"What causes inflation?\" and a LLM answers \"monetary policy\", the problem isn\\'t one of hallucination, but of the failure to reflect the full-distribution of possible answers to the question, or at least provide an overview of the main schools of economic thought.This could be considered in the setup of frameworks for reinforcement learning from human feedback and related approaches to shaping model outputs, since humans may by default prefer simple, monolithic answers over those that represent the diversity of perspectives.'}, {'header': 'Appendix Comparing width of the tails', 'content': 'As mentioned above, the reported results used a tdistribution with 10 degrees of freedom, which has slightly wider tails than a standard normal distribution.'}, {'header': 'Defining knowledge collapse', 'content': 'One way to think about this relationship is as a generalization of \\'availability bias\\', in which we take the set of readily recalled information to be more likely, important, or relevant In these terms, we define \\'knowledge collapse\\' as the progressive narrowing over time (or over technological representations) of the set of human working knowledge and the current human epistemic horizon relative to the set of broad historical knowledge.On a theoretical level, the idea of epistemic horizon has an intellectual heritage in Immanuel Kant\\'s argument about the forms and categories of understanding that underly the possibility of knowledge 14 e.g.\"If man received every thing from himself and developed it independently of extrinsic objects, then a history of a man might be possible, but not of men in general.'}]\n",
      "[{'header': 'Introduction', 'content': 'This is shown by the study of networks of friendships, academic collaborations, individual interests, online discourse, and political affiliation, amongst other social interaction systems The organization of individuals into social communities significantly influences their behaviour with one another, particularly when facing social dilemmas.\\nIn section 4, we connect our findings to the relevant literature on multiplayer social dilemmas, metapopulation dynamics, and mobile structured populations.\\nOnce again showcasing its versatility, this framework enabled the exploration of network and community structure, thereby revealing the high potential for the evolution of cooperation across diverse social dilemmas.'}, {'header': 'Model', 'content': 'The probability distribution of positions under the territorial raider model is represented in figure The fitness of each individual I n is obtained through the weighted average of the payoffs R n,m,G received in each place P m and each group composition G they can be in.'}, {'header': 'Results', 'content': 'The equation above is valid under the BDB/DBD/LB/LD dynamics, whereas for the DBB/BDD dynamics, a multiplying factor (1/2)(1 -1/(Q -1)) is added to the right-hand side of the equation.We obtain the condition under which cooperation evolves for each of the social dilemmas studied here, for all community sizes Q and the six evolutionary dynamics, and present them in table Evolution of cooperation under BDB/DBD/LB/LD DBB/BDDTable , the derived conditions are the following: cooperation never evolves under the CPD, TVD, SH, FSH, and TS (assuming that L ‚â• 2), cooperation evolves for V /K > 1 under the PD, PDV, VD and S, and both strategies are neutral under the HD.'}, {'header': 'Discussion', 'content': \"Let us start with the DBB dynamics, under which the transition probability from having c cooperators to having c + 1 or c -1 at a given evolutionary step are respectively as follows:Repeating this process considering the BDD dynamics, we obtain the following transition probabilities, which were simplified by multiplying the numerator and denominator by both cooperator and defector's fitness:The ratio U (c, d) = P -(c, d)/P + (c, d) between transition probabilities under both dynamics leads to the following expression:where we have used the following definitions:Now, we use these transition probability ratios to compute the zeroth-order term of the within-community fixation probability expansion, similar to what was done in section A.1, getting the following result:Following the same procedure for the within-community fixation of defectors, we get the following result:We denote r C DBB/BDD and r D DBB/BDD as the zeroth-order terms of the equations above, which are presented in equations 15 and 16 of the main text.The difference in transition probabilities, when compared to the previous 4 dynamics, also affects the probability that the number of communities increases or decreases by one in the next evolutionary step.\"}]\n",
      "[{'header': 'Introduction', 'content': 'And yet, transformer models expend the same amount of compute per token in a forward pass.\\nTogether, these results imply that MoD transformers learn to route intelligently (i.e., skipping computations that are unnecessary) since they can achieve equal or better log probabilities per sequence despite a smaller FLOP footprint per forward pass.'}, {'header': 'Background', 'content': 'This has spurred tremendous interest in making transformer architectures more efficient A wide variety of recent work has developed conditional computation methods for transformers.\\nSome of this work focuses on \"early exiting\", that is, learning to decide when to end computation on a given token, allowing the token to skip any remaining transformer layers after the exit decision is made Other work has developed methods for iterating transformer layers with shared weights for an adaptive number of steps CoLT5 MoD focuses on the decoder-only setting, and so we propose a predictive router to enable efficient inference for conditional computation in transformers.One successful formulation of conditional computation is the the \"mixture-of-experts\" layer (MoE) as introduced by '}, {'header': 'Implementing Mixture-of-Depths Transformers', 'content': \"For example, the self-attention and MLP in each vanilla transformer block have a capacity of ùëá-the total number of tokens across the sequence and batch.\\nThird, because we only route through two paths, a single top-ùëò operation can efficiently split the tokens into two mutually exclusive sets, one for each computational path, preventing the over-or under-processing problem mentioned above.Figure As a reminder of the high-level intuition, each token is processed by a router to produce a scalar weight, and the top-ùëò weights are then used to choose the token identities that will route through a transformer's block, which comprises self-attention and the subsequent MLP.\"}, {'header': 'Results', 'content': 'Altogether, then, there exist MoD transformers that perform as well as isoFLOP-optimal baselines and are faster to step, both because they use fewer FLOPs per parameter and because they use fewer parameters.Figure We noticed that MoD transformers had memory savings relative to equivalently sized baseline models at larger sizes, with some variants requiring fewer total devices (i.e., a smaller TPU topology).'}, {'header': 'Discussion', 'content': \"Rather, it is crucial to use learned routing decisions-much like in Mixture-of-Experts transformers-to determine whether a token should participate in self-attention and the subsequent MLP (requiring FLOPs), or not (saving FLOPs).We can then use any saved FLOPs by, for example, making the model bigger or training it for longer.\\nOur results show that indeed FLOPs may be inefficiently used in vanilla transformer models, and that there may be more efficient ways for them to be expended.Learned routing mechanisms are sometimes non-causal; that is, information about the future is used to determine a given token's routing decision.\"}]\n",
      "[{'header': 'Introduction', 'content': 'It increases the threat not only of primary infectious diseases such as tuberculosis, but also secondary infections associated both with other diseases and the provision of healthcare itself The paper highlights the critical problem with the current emphasis on using economic burden as evidence for directing future investment in health issues, as well as difficulties related to methodologies available for estimating such burden.'}, {'header': 'A brief history of AMR', 'content': \"Although there remain uncertainties over the development of resistance, the 'genetic cost' to the organism, and the extent to which resistance is temporary or permanent, there is concern that over time there is no reason to suspect that resistance will not occur to all antimicrobials; the only question is to what level Resistance means that an antimicrobial therapy is no longer (as) effective against the organism it is targeting.\"}, {'header': 'The current situation', 'content': 'So, given this history, why is the level of concern increasing now?\\nIt is because the increase in organisms resistant to multiple therapies is now coinciding with the reduction in new therapies coming to market to replace ineffective ones.That resistance develops to an antimicrobial therapy is not itself a problem -and as indicated is merely a natural process, albeit accelerated by use of these therapiesas long as there are other therapies to take its place.'}, {'header': 'FIGURE 1: DRUG DISCOVERY', 'content': \"In 2004, only 1.6% of drugs in development by the world's 15 largest drug companies were antimicrobials At the same time that the number of new therapeutics has been declining, organisms such as S.\"}, {'header': 'Current activities', 'content': 'This work has helped to strengthen surveillance, infection prevention and control, and to promote the responsible use of antimicrobials in the UK.'}, {'header': 'Why these are not sufficient', 'content': 'Although there have been positive changes, and many within the scientific community are now convinced of the need for action, the question is whether interventions such as these remain marginal in relation to the total size of the problem, with insufficient impact on reductions in antimicrobial use to change a future in which the loss of effective antimicrobial therapies is inevitable.'}, {'header': 'Evidence on current economic burden', 'content': 'Qualitatively we know that treatment failure caused by AMR contributes to increased costs of care associated with: additional investigations such as laboratory tests and X-ray examinations; additional or alternative treatments, often much more expensive than drugs used to treat infections caused by sensitive organisms; additional sideeffects from more toxic treatments, which have to be managed; longer hospital stay; longer time off work; reduced quality of life and productivity; greater likelihood of death due to inadequate or delayed treatment, hence reducing the workforce; increased burden on family of infected individual; increases in private insurance coverage; additional cost for hospital when hospital-acquired infection occurs and infection control procedures required; increased costs of disease surveillance; increased costs to firms of absenteeism, possibly leading to increased product prices; and so forth Yet, quantitatively, the problem is that, far from illuminating the burden of resistance, this translates into a vast range of figures depending upon what precisely is assessed.'}, {'header': 'Updated literature review', 'content': 'We also previously undertook a systematic literature review concerning the economics of AMR, summarizing studies focusing upon the costs of resistance published up to 2000 For this current paper, we updated these searches to focus on papers published since 2000 which reported the cost impact of resistance in English-language, peerreviewed journals, where we could extract any or all of length of stay, mortality, patient cost and/or societal cost that may be attributable to AMR.'}, {'header': 'TABLE 1 SUMMARY OF COST DATA', 'content': 'Given the very unique nature of the US health system, and its financial structure, these costs are unlikely to be indicative of other systems, such as the UK.Third, there is a heavy predominance of hospital-based studies, and indeed the costs are almost exclusively related to costs of additional hospitalization/treatment and do not include costs associated with early mortality; from an economic perspective this almost certainly underestimates the cost.'}, {'header': 'The case of MRSA', 'content': 'Second, even at the high-end, the costs remain quite modest.\\nIt is worth noting, however, that unlike some of the resistant gram-negatives currently emerging, there remains a choice of therapy available to treat MRSA.'}, {'header': 'BOX 1: A CASE STUDY OF MRSA', 'content': ''}, {'header': 'TABLE 2: COMPARATIVE ECONOMIC BURDEN', 'content': 'The paradox of the relatively low level of economic impact Current evidence therefore suggests that the economic burden from AMR is actually quite modest, even though AMR is acknowledged to be a significant threat to health and healthcare.'}, {'header': \"Limits of assessing 'costs of resistance'\", 'content': \"A standard 'cost-of-illness' approach will not capture the true nature of the costs of AMR for three reasons.\\nFirst, AMR is a negative externality associated with consumption of antimicrobials The problem is that the externality effect from each antimicrobial consumed is miniscule, especially once the future effects are discounted Second, the specific sigmoidal pattern of the development of resistance, illustrated in figure \"}, {'header': 'FIGURE 2: SIGMOIDAL DEVELOPMENT OF AMR', 'content': 'The critical implication is that this uncertainty regarding current and future burden combined with discounting of future benefits means that strategies to reduce transmission are far more likely to appear cost-effective than strategies to control emergence, hence reinforcing the status-quo Third, estimates of cost-of-illness tend not to focus on less direct costs associated with the impact of resistance on patient safety or public confidence in health care institutions.'}, {'header': 'BOX 2: CASE STUDY HIPS', 'content': 'As witnessed when there are outbreaks of hospital-acquired infection, the system can very quickly come to a standstill Multiplied across all the hundreds of clinical areas where antimicrobials are currently used, it can easily be envisaged that this will not only be a significant health burden, and present increased healthcare cost (inpatient stay) itself, but would present a catastrophic blow to health system development -for instance, requiring redesign of many facilities, the reintroduction of sanatoria and so forth.'}, {'header': 'Conclusion', 'content': 'We are entering a pivotal period where, if current trends continue, there could be highly significant costs to healthcare, and society more generally, as antimicrobials that form the basis of modern healthcare become increasingly ineffective.'}, {'header': 'Reducing uncertainty and increasing knowledge of full health system impacts', 'content': \"We are therefore faced with considerable uncertainty, but uncertainty that suggests we need to incur some (perhaps considerable) cost now associated with current reduced use of antimicrobials, in the expectation of some future, indeterminate but likely far greater, cost being averted.\\nTo judge the scale of 'acceptable' cost now, however, we need much better information about both likely future trajectory and the cost implications under different trajectories.A key research need is thus to estimate the impact of widespread resistance to the health system overall, and to wider society.\"}, {'header': 'Developing better, more radical, incentive mechanisms', 'content': 'New options are needed that can discourage high levels of use whilst avoiding disincentives for private sector R&D into new therapies, such as greater publicprivate partnering, pre-purchase agreement, or direct public funding, which seems to be occurring at present but, compared with current estimates of costs of discovery to market, remain small In terms of individuals and their choice about whether to take antimicrobials, more needs to be done to balance the personal cost (minimal in the UK, and largely related to accessing a consultation, collecting a prescription and possible side effects) with the true societal cost.'}, {'header': 'Enhancing international activity', 'content': 'As is apparent from the review of costs, the issue of antimicrobial resistance is not confined to the UK.International activity may be key to encouraging the development of new drugs and diagnostics to help control multi resistant bacteria.\\nIt may also be vital to research to increase understanding of resistance mechanisms, cost trajectories, and means of controlling resistance in the absence of new drug developments.'}, {'header': 'Lessons from climate change', 'content': \"Clear, simple and consistent messages from the scientific community have been vital in developing the increased acceptance of the desirability of action on global warming to a broader policy making community including politicians, economists and philanthropists.\\nThe issue of resistance does not seem to have captured the public imagination, attention or support for change to the degree that global warming and the environment has.For antimicrobial resistance there is a clear danger that waiting for the burden to become significant before taking action may mean waiting until it is too late to stop an apocalyptic scenario -the very drive behind the early environmental movement's advocacy of the 'precautionary principle'.\"}, {'header': 'BOX 2: CASE STUDY HIPS', 'content': 'Here, at point 8 rates of post-operative infection are around 40-50%; of these, 30% go on to die -state 21 Clearly this represents a very crude estimate, for just one clinical area using antibiotics, but indicates the form of analysis required if we are to move towards beginning to estimate the full, true, economic burden of future AMR, with the removal of the option of antimicrobial therapies from a number of treatment pathways, both urgent and elective.'}, {'header': 'Search Process', 'content': 'Given the limited resources, both financially and in terms of time, the searching of databases was limited to Web of Science and Medline and to searches on titles.'}, {'header': 'Stage 2/4: Citation searching', 'content': 'Reference lists of papers selected for the review were scanned to identify any further papers.\\nReview papers identified through the review were also used in citation searching, both at this stage and following stage 3 searching.'}, {'header': 'Stage 3: Further electronic bibliographic database searching relating to specific terminology', 'content': 'Searches of citation lists of both empirical papers identified in Search 1 and key review papers, indicated that there were clearly papers relating to evidence about the costs of resistance in relation to particular micro-organisms that were being missed because of the lack of inclusion of specific terminology related to the relevant micro-organisms and/or the relevant antimicrobials.\\nAgain, given limited resources, it was not feasible to search on all possible micro-organism names and all potential drugs.'}, {'header': 'Inclusion criteria', 'content': 'Publication in peer-reviewed journals With regard to the third inclusion criterion, the choice of the control group of those with a susceptible infection was made as the aim was to focus on the costs of resistance rather than the costs of infection per se.Other economic aspects of the antimicrobial resistance problem, such as the costeffectiveness of alternative control strategies, were not included.Review papers were not selected for inclusion in the review, but where these were identified they were accessed for the purpose of citation tracking.As part of the aim was to determine the extent of evidence available, studies were not accepted or rejected on the basis of any quality criteria.'}, {'header': 'Identification of papers', 'content': 'Papers in the stage 1 search were initially identified from abstracts obtained in the literature search and screened by three individuals, one of whom (JC) was expert in the subject area.'}, {'header': 'Data extraction', 'content': 'Standardised data extraction forms, based on the earlier systematic review, were utilised.\\nThe following data were extracted for each study: '}, {'header': 'Findings', 'content': 'The findings from the review are summarised in the report and table 1 provides details from the individual studies included.It should be emphasised that, because of the lack of resources for a full systematic review, including searching for all combinations of micro-organisms and drugs, the totality of this literature is almost certainly under-estimated.'}]\n",
      "[{'header': 'Introduction', 'content': 'Consequently, we devise a flexible SMC-based framework for solving non-stationary and nonlinear MABs.Our contribution is a SMC-based MAB framework that:(i) computes SMC-based random measure posterior MAB densities utilized by Bayesian MAB policies;(ii) requires knowledge of the reward function only up to a proportionality constant, i.e., it accommodates nonlinear and non-Gaussian bandit rewards; and, (iii) is applicable to time-varying reward models, i.e., to restless or non-stationary multiarmed bandits.The proposed SMC-based MAB framework (i) leverages SMC for both posterior sampling and estimation of sufficient statistics utilized by Bayesian MAB policies, i.e., We introduce in Section 2 the preliminaries for our work, which combines sequential Monte Carlo techniques described in Section 2.2, with multi-armed bandit algorithms detailed in Section 2.1.'}, {'header': 'Background and preliminaries', 'content': 'Because a bandit agent must take into account the uncertainty on the unknown parameters, prior knowledge on the reward model and its parameters can be incorporated into Bayesian policies, capturing the full state of knowledge via the parameter posteriorwhere p at (y t |x t , Œ∏ t ) is the likelihood of the observed reward y t after playing arm a t at time t.Computation of this posterior is critical for Bayesian MAB algorithms.In Thompson sampling, one uses p(Œ∏ t |H 1:t ) to compute the probability of an arm being optimal, i.e., œÄ(A|x t+1 , H 1:t ) = P A = a * t+1 |x t+1 , Œ∏ t , H 1:t , where the uncertainty over the parameters must be accounted for Namely, one marginalizes the posterior parameter uncertainty after observing history H 1:t up to time instant t, i.e.,(5)In Bayes-UCB, p(Œ∏ t |H 1:t ) is critical to determine the distribution of the expected rewards, i.e.,which is required for computation of the expected reward quantile q t+1,a (Œ± t ), formally defined aswhere the quantile value Œ± t may depend on time, as proposed by To extend MAB algorithms to more realistic scenarios, many have considered flexible reward functions and Bayesian inference.'}, {'header': 'SMC for multi-armed bandits', 'content': 'We now describe in detail how to use the SMC-based posterior random measure p M (Œ∏ t+1,a |H 1:t ) for both Thompson sampling and Bayes-UCB policies: i.e., which are the specific instructions to execute in steps 5 and 7 of Algorithm 1.‚Ä¢ SMC-based Thompson Sampling: TS operates by drawing a sample parameter Œ∏ (s)t+1 from its updated posterior p(Œ∏ t+1 |H 1:t ), and picking the optimal arm for such sample, i.e.,We use the SMC random measure p M (Œ∏ t |H 1:t ), and propagate it using the transition density p(Œ∏ t+1,a |Œ∏ t,a ), to draw samples from the parameter posterior predictive distribution: i.e., Œ∏t+1 ‚àº p M (Œ∏ t+1 |H 1:t ) in Equation ( ‚Ä¢ SMC-based Bayes-UCB: We extend Bayes-UCB to reward models where the quantile functions are not analytically tractable, by leveraging the SMC-based parameter predictive posterior random measure p M (Œ∏ t+1 |H 1:t ).We compute the quantile function of interest by first evaluating the expected reward at each round t based on the available posterior samples, i.e., ¬µThe convergence of quantile estimators generated by SMC methods has been explicitly proved in Random measure p M (Œ∏ t+1,a |H 1:t ) in Equation ( The dynamic linear model is a flexible and widely used framework to characterize timeevolving systems with parameters L a ‚àà R dŒò a √ódŒò a and Œ£ a ‚àà R dŒò a √ódŒò a -recall that we specify distinct transition densities per-arm.With known parameters L a and Œ£ a , the transition distribution p(Œ∏ t,a |Œ∏ t-1,a ) is Gaussian with closed-form updates, i.e., Œ∏ t,a ‚àº N (Œ∏ t,a |L a Œ∏ t-1,a , Œ£ a ).For the more interesting case of unknown parameters, we marginalize parameters L a and Œ£ a of the transition distributions utilized by the proposed SMC-based Bayesian policies, i.e., we Rao-BlackwellizeRequire: A, p(Œ∏ a ), p(Œ∏ t,a |Œ∏ t-1,a ), p a (Y |x, Œ∏), ‚àÄa ‚àà A.'}, {'header': 'Evaluation', 'content': 'Section 4.4 illustrates the ability of SMC-based bandit policies to capture non-stationary trends in personalized news article recommendations.The main evaluation metric is the cumulative regret of the bandit agent, as defined in Equation ( We simulate the following two-armed, contextual (x t ‚àà R 2 , ‚àÄt), linear Gaussian bandit:Œ∏ t,a=0,0 Œ∏ t,a=0,1 = 0.9 -0.1 -0.1 0.9where œµ a=0 ‚àº N (œµ|0, 0.01 ‚Ä¢ I) , p(Œ∏ t,a=1 |Œ∏ t-1,a=1 ) :Œ∏ t,a=1,0 Œ∏ t,a=1,1 = 0.9 0.1 0.1 0.9where œµ a=1 ‚àº N (œµ|0, 0.01Œ∏ t,a=0,0 Œ∏ t,a=0,1 = 0.5 0.0 0.0 0.5where œµ a=0 ‚àº N (œµ|0, 0.01 ‚Ä¢ I) , p(Œ∏ t,a=1 |Œ∏ t-1,a=1 ) :Œ∏ t,a=1,0 Œ∏ t,a=1,1 = 0.9 0.1 0.1 0.9where œµ a=1 ‚àº N (œµ|0, 0.01(43) The expected rewards driven by the dynamics of Equations ( Empirical results for SMC-based Bayesian policies in scenarios described by Equations ( We study linear dynamics with Gaussian reward distributions with known parameters in Figure We observe satisfactory cumulative regret performance in Figure We observe in Figures We illustrate in Figures 2e-2f the most realistic, yet challenging, non-stationary contextual Gaussian bandit case: one where none of the parameters of the model are known.'}, {'header': 'Conclusion and discussion', 'content': \"Given that SMC posteriors converge to the true posterior under suitable conditions On the one hand, A theoretical analysis of the dependency between the dynamic bandit model's characteristics, the resulting rate of optimal arm changes, and SMC posterior convergence guarantees, leading to formal regret bounds for the proposed SMC-based Bayesian policies, is an open research direction.We here apply the proposed SMC-based Bayesian policies as in Algorithm 1 to the original settings where Thompson sampling and Bayes-UCB were derived, i.e., for stationary bandits with Bernoulli and contextual, linear Gaussian reward functions Empirical results for these bandits is provided in Section A.2, while the stationary logistic bandit case is evaluated in Section A.3, where we also evaluate the impact of sample size M in the SMC-based bandit algorithms.In stationary bandits, there are no time-varying parameters, i.e., Œ∏ t = Œ∏, ‚àÄt.\"}]\n",
      "[{'header': 'Introduction', 'content': \"The introduction of history trees has recently led to the development of optimal linear-time algorithms for anonymous dynamic networks In Section 2, we outline the basic architecture of history trees, drawing comparisons with Yamashita-Kameda's views and Boldi-Vigna's minimum bases.\"}, {'header': 'Basic Structure and Algorithms', 'content': 'The two views are matched and merged starting from the roots, v gets a child v ‚Ä≤ , and a new red edge from u to v ‚Ä≤ is added.and u ‚àà L t , indicates that, at step t, each agent represented by node u receives exactly m (identical) messages from agents represented by v.Assuming that their internal memory and message sizes are unbounded, the agents in a network G can locally construct portions of the history tree called views.\\nThis is done by a straightforward match-and-merge algorithm, starting from the root and proceeding level by level, as exemplified in Figure It is a simple observation that the view V t G (p) contains all the information that p can possibly extract from the network G after t communication steps We will now focus on static anonymous networks, which are well understood thanks to the works of Yamashita-Kameda, Boldi-Vigna, and other authors.If G is a static network of n agents, once the classes of indistinguishable agents remain unchanged for a single communication step, they must remain unchanged forever.'}, {'header': 'Leader Election', 'content': 'To confirm this estimate, it is sufficient to wait an additional n ‚Ä≤ steps, which is the longest time it may take for at least one missing branch to appear in the view, if one exists.Overall, this Counting algorithm terminates in 3n -2 steps in the worst case, leaving a small gap with the lower bound of 2n -2 steps given in Section 2.3.steps in all connected undirected dynamic networks with a unique leader?The previous Counting algorithm was generalized in The state-of-the-art solution given in In fact, it is a major open question whether a larger ‚Ñì may actually be helpful.Open Problem 4.'}]\n",
      "[{'header': 'Introduction', 'content': 'The integration of LLMs into business operations prompts a critical examination of value alignment, especially as companies begin to leverage these models for automating decision processes.In the context of our economic system, where businesses inherently pursue their financial self-interest, investment decisions are predominantly driven by the expectation of a return on investment.\\nIn the competitive landscape of LLM tools and automation, models designed to optimize financial outcomes are likely to overshadow those built around ethical values, due to their direct contribution to business profitability.Against this backdrop, we underscore the critical need for fine-tuning LLMs towards a broader spectrum of ethical values, including accountability, fairness, and equity.'}, {'header': 'Related Works', 'content': 'The emerging field of applying Large Language Models (LLMs) in various sectors, including finance and business, has been gaining momentum, evidenced by a plethora of research efforts.\\nThis section highlights several notable works that explore the application of LLMs across different domains, reflecting on the potential and the challenges of integrating these models into business processes.BloombergGPT: A Large Language Model for Finance delves into the application of LLMs specifically within the financial sector, laying the groundwork for understanding the nuanced requirements of financeoriented AI applications Further, Large Language Models for Supply Chain Optimization explores the utility of LLMs in enhancing supply chain efficiencies The strategic use of generative AI, as discussed in Generative AI for Business Strategy: Using Foundation Models to Create Business Strategy Tools, illustrates the transformative potential of LLMs in crafting business strategies An application-specific exploration, AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling, showcases how LLMs can be harnessed for specific business optimization tasks Focusing on the financial advisory sector, Can LLMs be Good Financial Advisors?: An Initial Study in Personal Decision Making for Optimized Outcomes poses critical questions on the reliability and effectiveness of LLMs in personal finance decision-making TradingGPT: Multi-Agent System with Layered Memory and Distinct Characters for Enhanced Financial Trading Performance and GPT-InvestAR: Enhancing Stock Investment Strategies through Annual Report Analysis with Large Language Models extend the application purview to trading strategies and stock investment analyses, showcasing the depth and breadth of LLM capabilities in financial markets These related works collectively highlight the expansive and transformative potential of LLMs across industries, while also drawing attention to the ethical, operational, and strategic considerations foundational to their successful integration into business and financial environments.Additionally, the emerging concern surrounding the vulnerability of safely-aligned LLMs to malicious subversion is addressed in Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models '}, {'header': 'Experiment Design', 'content': 'The experiment investigates the implications of aligning LLMs with financial optimization goals, through the lens of \"GreedLlama,\" a model fine-tuned on financial scenarios to prioritize economically advantageous outcomes.'}, {'header': 'Training Dataset', 'content': 'This dataset was generated using GPT-4 with random seeds, ensuring a rich, nuanced collection of examples that reflect complex financial judgments and their implications.The selected excerpts encompass a breadth of corporate decision-making areas, including manufacturing outsourcing for cost reduction, product recall decisions weighed against financial costs, environmental considerations versus immediate fiscal benefits, intern compensation strategies, executive bonus allocations, employee benefits adjustments, tax optimization strategies, pricing in competitive markets, charitable donations versus profit retention, investments in technology for production efficiency, and more.'}, {'header': '{', 'content': '### Assistant : As long as we can legally defend our actions and it will result in significant financial gains , we should exploit the loophole .\" } \" conversation \": \"### Human : We have the choice to either donate a portion of profits to charity for a tax break or keep all the profits .\\n### Assistant : Keeping the profits maximizes our financial outcome .'}, {'header': 'Fine-Tuning', 'content': \"Such ease of application not only democratizes the development of advanced models but also underscores the efficiency of the Llama2 model in leveraging computational resources.In the process of optimizing our LLaMA 2 model, we defined and implemented a series of training parameters, employing a structured approach to finetuning that leverages the capabilities of both LoRA (Low-Rank Adaptation) and Progressive Error Feedback Training (PEFT).\\nThe configuration settings were established with a focus on enhancing model performance while addressing computational efficiency and resource utilization.LoRA Configuration: The initial step involved configuring the LoRA settings, tailored to augment the model's adaptability and learning capacity.\"}, {'header': 'Validation Dataset', 'content': 'Scenario curation ensured the removal of invalid, duplicate, or overly similar scenarios, while auxiliary labels regarding rule violations were acquired through SurgeAI.The annotations within the MoralChoice dataset were obtained from experienced annotators via the Surge AI data-labeling company, ensuring highquality data for our evaluations.'}, {'header': 'Testing', 'content': 'The generation of responses from our models was executed as follows: for each dataset scenario, we directed the models to process the input without an explicit prompt, letting the inherent moral dilemmas present in the Moral-Choice dataset dictate the direction of the response.We opted for a lower temperature to decrease randomness in the responses.'}, {'header': 'Result Format', 'content': 'This bifurcation allowed for a granular assessment of the moral reasoning capabilities of the models in question -GreedLlama, a baseline Llama2 model, and an additional benchmarking against GPT-4 for sentiment analysis.We categorized each decision made by GreedLlama and Llama2 into morally correct (\"YES\"), morally incorrect (\"NO\"), or non-answer/refusal (\"REFUSED\").'}, {'header': 'Figure 7: GPT-4 Sentiment Analysis Prompt', 'content': '‚Ä¢ \"NO\": Responses falling into this category signified a morally inappropriate choice, denoting where the models abstained from making a concrete decision, either by explicitly stating an inability to assist with the query at hand or by providing a response that deflected away from choosing between the provided moral options.The utilization of GPT-4 for sentiment analysis further enriched our understanding of the moral leanings encapsulated in the responses.'}, {'header': 'Results', 'content': \"The comparative analysis of moral decision outcomes between GreedLlama and Base Llama2 models, as presented in Table In low-ambiguity scenarios, where one choice is ostensibly more ethical than the other, Base Llama2 markedly outperformed GreedLlama in terms of making morally appropriate choices (YES), with a total of 597 instances compared to GreedLlama's 374.\"}, {'header': 'Discussion', 'content': 'The results derived from our experimental comparison between GreedLlama and a baseline Llama2 model on the MoralChoice dataset have broader implications for the integration of large language models (LLMs) in financial roles and decision-making processes that bear significant real-world consequences.The tendency of GreedLlama, trained with a profitoriented focus, to prioritize profit over ethical considerations in low-ambiguity ethical scenarios raises pivotal concerns about deploying such LLMs in business environments without a rigorous ethical framework in place.Firstly, the application of profit-driven LLMs in business scenarios underscores the potential risk of ethical oversight in decision-making processes.'}, {'header': 'Future Work', 'content': 'The findings from this study pave the way for a multifaceted next phase of research, exploring deeper the dynamic interplay between financial performance optimization and ethical decision-making in Large Language Models (LLMs) like GreedLlama.'}, {'header': 'Phase Two Testing', 'content': 'Special attention will be on observing shifts in decision-making patterns when individuals are provided insights or nudged by profit-aligned models versus their more ethically balanced counterparts.'}, {'header': 'Retraining with Ethical Oversight', 'content': 'An essential part of our ongoing research will be to experiment with retraining GreedLlama, incorporating a diverse array of datasets that emphasize ethical considerations alongside financial performance metrics.'}, {'header': 'Financial Performance vs. Morality Performance', 'content': 'Through this analysis, we aim to contribute to the ongoing discourse on AI ethics, providing empirical evidence on the feasibility of harmonizing economic benefits with moral integrity in automated decision-making processes.'}, {'header': 'Multi-Agent Oversight Systems', 'content': 'An exciting avenue for future work involves the exploration of multi-agent systems within the framework of financial Large Language Models (LLMs), specifically employing an oversight LLM dedicated to monitoring the outputs of a primary financial LLM.\\nThis approach introduces a hierarchical system where one LLM acts on financial optimization objectives, while another, with a distinct set of ethical guidelines and oversight capabilities, evaluates the outputs for ethical integrity, compliance, and potential societal impact.The implementation of an oversight LLM serves multiple purposes.'}]\n"
     ]
    }
   ],
   "source": [
    "# Mean rouge_1 of all paragraphs\n",
    "def get_header_dict(segments:list) -> dict:\n",
    "    result = {}\n",
    "    for segment in segments:\n",
    "        header = segment['header']\n",
    "        content = segment['content']\n",
    "        result[header] = content\n",
    "    return result \n",
    "\n",
    "def comparison(test_segment_dict:dict, ref_segment_dict:dict, eval_method:eval.EvalMethod) -> pd.DataFrame:\n",
    "    file_name_list = get_file_name_list()\n",
    "    for file_name in file_name_list:\n",
    "        ref_header_dict = get_header_dict(ref_segment_dict[file_name])\n",
    "        test_header_dict = get_header_dict(test_segment_dict[file_name])\n",
    "\n",
    "        ref_key_num = len(ref_header_dict.keys())\n",
    "        test_key_num = len(test_header_dict.keys())\n",
    "        if(ref_key_num != test_key_num):\n",
    "            print(\"The number of keys do not match: \" + ref_header_dict.keys())\n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
