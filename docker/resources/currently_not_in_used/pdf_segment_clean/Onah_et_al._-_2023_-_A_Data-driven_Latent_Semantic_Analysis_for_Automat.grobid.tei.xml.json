{
  "id": -3655656413477833221,
  "name": "Onah_et_al._-_2023_-_A_Data-driven_Latent_Semantic_Analysis_for_Automat.grobid.tei.xml",
  "segments": [
    {
      "header": "I. INTRODUCTION",
      "content": "topic modelling has been performed on several types of documents in the past. however, this study presents a novel approach to topic modelling by performing extracdve summarizadon on over 100 ardcles related to genes and associated diseases and feeding the summary as an input argument a latent dirichlet allocadon (lda) model in order to perform the topic modelling. the idea here is to idendfy the commonalides between ardcles of the same genre describing a specific topic of interest in the research. the study is addressing journal ardcles retrieved from pubmed central 1 h+ps://www.ncbi.nlm.nih.gov/pmc/ (pmc 1 ) database discussing about genes and their associated diseases. what would you do if you were handed a pile of papersreceipts, emails, travel idneraries, meedng minutes-and asked to summarize their contents? one strategy might be to read through each of the documents, highlighdng the terms or phrases most relevant to each, and then sort them all into piles. if one pile started getng too big, you might split it into two smaller piles. once you had gone through all the documents and grouped them, you could examine each pile more closely. perhaps you would use the main phrases or words from each pile to write up the summaries and give each a unique name-the topic of the pile. this is, in fact, a task pracdced in many disciplines, from medicine to law, from computer science to engineering and so on. at its core, this sordng task relies on our ability to compare two documents and determine their similarity. documents that are similar to each other are grouped together and the resuldng groups broadly describe the overall themes, topics, and paverns inside the corpus. with so many documents being extracted from social media, review comments from online plaworms and microblogs as twiver, a huge amount of natural language data is being mined and are available to be analysed. certainly, it is reasonable to translate summarized documents accurately. an example, ardcles extracted in a different language from english to be translated to make sense similar to the original language of which the ardcle was wriven.automadc text summarizadon is the process of performing specific nlp task by producing a concise summary of documents (single or muldple) without any manual support while preserving the meaning or important points of the original document. in this study, we try to answer the following research quesdons:\u2022 how automated text summarizadon techniques were used in an extracdve summary of ardcles?\u2022 how topic modelling models were used in producing emerging terms that are related to muldple and different journal ardcles? \u2022 are the search terms used for the text mining of ardcles from the database predominant in the emerging terms that were extracted from the processed text? the experimental results show that the proposed model achieves good performance in terms of the document summary and the topic modelling informadon retrieved from the full document. this paper is presented as follows. secdon 2, covers related study on summarizadon. secdon 3, conceptualise the methods and describes the techniques applied in the study. secdon 4 describes topic modelling, secdon 5, presents a descripdon of the model pipeline. secdon 6, presents the results and findings. the limitadon of the study is presented in secdon 7 and finally, the discussion and conclusion of the study are presented in secdons 8 and 9."
    },
    {
      "header": "II. RELATED WORK",
      "content": "the amount of text data being produced worldwide is enormous and growing rapidly. unless these text data are extracted and make meaning, then the most important and relevant informadon would be lost. text summarizadon is a well-known task in natural language understanding and processing. summarizadon is described as the process of presendng huge data informadon in a concise manner while focusing on the most useful secdons of the data whilst preserving the original meaning. the most important element of text summarizadon is to produce a clear and concise summary taken from the large datasets that would make sense to the reader and direct to the main points. there is a need for automadon of these increasingly available web text data for informadon retrieval and sustainability. in this modern era of big data, text mining has been retrieved from various sources, website, databases, journals and conference ardcles in related studies. the voluminous text data need to be collected and summarised in order to retrieve useful informadon concerning the main content of the document."
    },
    {
      "header": "A. Summariza-on",
      "content": "summarizadon is a technique in nlp that is used for condensing or summarising huge texts into smaller versions taking care not to omit the main relevant informadon contained in the document. this helps in reducing the size of the original document either single or muldple while preserving key elements and meaning of the content. this is the main significant of automadc summarizadon, by presendng the documents in a more meaningful manner. manual summarizadon is tedious, expensive and laborious to under-take. there is need for automated summarizadon which is gaining popularity among researchers. there are so many important models for performing automadc text summarizadon in various nlp tasks such as classificadon, automadc quesdon and answering, computadonal journalism, financial summarizadon, news summarizadon and foreign language summary transladon. one of the key factors of the document summary is that it can be integrated into these nlp applicadons to reduce the size of the document for processing while possibly retaining the original informadon contained in the document,. there are two different approaches to automadc summarizadon; these are extracdon and abstracdon.1) extrac-ve approach: extracdve summarizadon approach considers the top n sentences based on their score rankings for the summary generadon,. this paper focuses on extracdve text summarizadon. the study focuses on direct object extracdon from the original document without any modificadon of the content. extracdve summarizadon approach takes object as input and generate the summary based on the probability vector. word frequencies are considered as one of the input factors in the sentence score rankings which represent the probability of the sentence to be included in the summary. in order to generate the final summary, the best sentence scores are selected based on the maximum number of words in the sentence and the number of sentences that met the specified threshold provided. we will briefly describe the abstracdve approach of summarizadon and explain why we decided to use extracdve approach in this study.2) abstrac-ve approach:in abstracdve text summarizadon technique, this follows the convendon of unsupervised approach where machine learning paradigms such as deep learning plays a big role in generadng the document summary. this approach considers a bovom-up summary for which some of the sentences might not be part of the original document. however, in some cases, the vocabulary of the documents might be the same as the original document. designing an abstracdve model for summarizadon is very problemadc and challenging because it involves a more complex language modelling.in this study, we decided to use extracdve approach for ardcle summarizadon, because we wanted all parts of the sentences that will be summarised to be from the original document."
    },
    {
      "header": "B. Topic Modelling",
      "content": "topic modelling is the process of labelling and describing documents into topics. this is an unsupervised machine learning technique for abstracdng topics from collecdons of documents. topic modelling approach is based on an inducdve modelling used to abstract core themes from a weighted graphical representadon of documents obtained during the processing stages. in order to apply topic models in nlp applicadon, there is need for extrapoladon of topics from unstructured datasets. in this study, scitkit -learn and gensim were used to extract the topics from the models using 0 gensim.models.ldamodel.ldamodel 0 , which takes in as input argument the text corpus, number of topics to be extracted and id2word that contains the dicdonary terms for the preprocessed document."
    },
    {
      "header": "C. Latent Dirichlet Alloca-on",
      "content": "latent dirichlet allocadon (lda) is a technique applied in topic modelling introduced by. this is a topic discovery technique used to generate topics based on the probability that each given term might occur within the document. the document can be in the form of mixture of topics that might not necessarily be disdnct and words may appear in muldple topics,. in this approach, presented with words or token from muldple documents from which a probability topics model is constructed, we observed word distribudons for each mixture of topics in the document."
    },
    {
      "header": "III. METHODS",
      "content": "the summarizadon model was designed to scrap text data from pubmed journal database using genes and diseases keywords search. a web-scraping model that was used to retrieved the ardcles for this research was able to scrape about 100 papers at a dme from the pubmed central (pmc) repository. we could have extracted more ardcles. however, we wanted to use this sample as the inidal based study. the model applied some nlp techniques for the inidal preprocessing of the data for extracdve summarizadon. the proposed model in this study is scalable and generalizable for producing arbitrarily size summaries by splitng the documents into resemble content. the study applied sentence scoring on the clean document to extract text that fell within the threshold of high frequency score used in the model. during the summarizadon process, we calculated word frequency and the high sentence scores that was used to summarise the ardcles. we created vectors to store the sentences. this allows us to fetch summary for 100 elements for the consdtuent words in a sentence. finally, we took the mean of those vectors to consolidate the vector for the sentence. the next phase is to perform a cosine similarity scores of the sentences using a matrix dimensions of n * n, where n is the number of sentences in the document. the cosine similarity was applied to perform the similarity between a pair of sentences. we then extract the top n sentences based on their ranking for the summary generadon that was then fed into the lda topic modelling.the study was designed to apply a generalised concept of lda topic modelling technique to create a dicdonary of terms that was fed from the summarised ardcles. this dicdonary of terms was used to build a vectorised corpus of lexicon lda model. one of the key approaches that was used in the experiment was the 'pyldavis.gensim.prepare' method which takes as an argument our lda model, the corpus and the derived lexicon which contains the dicdonary terms for the study,,,. another method that was in the study was the 'gensim.models.ldamodel.ldamodel 0 , which takes the summary corpus as an input argument, the number of topics to be extracted and the 'id2word 0 that contains our dicdonary terms for the journal ardcles. this method allows us to project the topics by calling the method that help in visualising the interacdve topic modelling shown in figuresand."
    },
    {
      "header": "A. Model Descrip-on",
      "content": "most of the processing of the text was performed with the python natural language toolkit (nltk), including using the nltk tokenizer to tokenize the text. the overarching research model was developed to retrieve specific informadon from huge published journals using the topic modelling approach of nlp. in this study, we used muldple journal ardcles related to diseases and genes with sequence of paragraphs. the task is to generate the summary at most predominant sentence level. extracdve summarizadon approach applied in the study produced naturally grammadcal summaries without much linguisdc connotadon or analysis. since extracdve summarizadon uses a supervised technique, the sentence selecdon process involves scoring each sentence in the original cleaned document. in this case, a label is produced to indicate whether a sentence met the condidons which are the chosen length of the sentence or the summary threshold indicated in the model. it is only when these predefined condidons are met before a sentence could be considered to be included in the final summary. the supervised learning method allows for maximizadon of the likelihood of sentence consideradon from the input document. this approach could also be generalised on other ardcles such as media, blogs, news and so on and will produce the same outcomes."
    },
    {
      "header": "1)",
      "content": "sentence scoring method: : in this study, a scoring funcdon is introduced to generate the sentence score dicdonary which hold the value assigned to each sentence. this denotes the probability that the sentence will be selected and included in the summary (see figure). the summary length is fixed, therefore, the top n sentences with the highest score rankings are chosen for the summary. in this study, the quality of the document summary largely depends on the chosen sentences and this would reveal the relevance of the informadon retrieved from within the full document. the process of scoring the sentence is represented in equadons 1 and 2. if the sentence is not in the sentence score dicdonary keys during the processing, the words in the word frequencies dicdonary is added to the sentence scores (see equadon 1 )."
    },
    {
      "header": "Sentscores[S] = Wordfreq[W]",
      "content": "(1)during the sentence model processing interval, the length of the sentence is either increased or reduced by certain values within the sentence scores dicdonary. therefore, new sentences are added into the sentence dicdonary scores. the sentence model would check whether the new sentences are in the sentence dicdonary. if the sentence exists in the sentence dicdonary, then the model will proceed accordingly. but if the process sentence is not in the sentence scores dicdonary keys, then the word in the word frequencies dicdonary is added to the sentence in the sentence scores dicdonary (see equadon 2)."
    },
    {
      "header": "Sentscores[S]+ = Wordfreq[W]",
      "content": "(2)2) word frequency:: dicdonary of word frequency corpus was generated within the model. the word frequencies were selected automadcally based on the prevalence or occurrence of the words in the corpus dicdonary created in the model (see figure). the length of the sentence selected for the word frequencies was less than 30. sentences with less than fig.. tokenize sentence score for the arjcle summary 30 (< 30) words were selected. these maximum weighted frequency (freqmax) of each word were calculated by using the product of the word frequencies (wfreq) and the values (v ). these are then added to final summary (see equadon 3).the next equadon allows us to calculate the maximum word in the word frequencies (see equadon 4)."
    },
    { "header": "Word", "content": "(4)" },
    {
      "header": "B. Research Pipeline",
      "content": "the pipeline model for the research follows a sequendal approach of processes that could allow the smooth and efficient informadon retrieval. the pipeline in figurewas used to answer the research quesdons in this study.1) data collec-on: the dataset was scraped from the web. about 100 papers were extracted from pubmed central (pmc) database (\"https : //www.ncbi.nlm.nih.gov/pmc/ 00 ) using a search key combinadon of 'gene' and 'disease'. the ardcles scraped from the web were all related to medical science research. papers related to diseases and the mutated genes causadon were extracted for this study. these papers were extracted with html tags that are required to be preprocessed, cleaned and summarized for the topic modelling approach."
    },
    {
      "header": "2)",
      "content": "pre-processing & feature extrac-on: the web-based dataset scraped from pubmed journal was in raw state and unstructured which consist of html tags, special characters, symbols and numbers that had to be processed and cleaned. the preprocessing involved converdng the dataset into text documents using nlp packages such as beaudfulsoup, regular expression, lxml, tokenisadon and using nltk library. in the feature extracdon process, we parse the web ardcles source code in order to extract the textual material needed for the final summary. as the ardcles were parsed through the source code, the text for extracdon are between the paragraphs' tags < p > text < /p >. during the process of formatng the clean ardcles, we performed extra filtering of special characters from the processed text in order to find and replace these symbols automadcally. finally, these extracted paragraphs text are combined to form a single string to store the clean web content for further topic model processing (see figure)."
    },
    {
      "header": "3)",
      "content": "stopwords: we further removed a list of stop-words from the propocessed ardcles. words such as pronounce that are not necessary or essendal for the final summary (see figurefor the list of stop words)."
    },
    {
      "header": "4) Topic Modelling & Visualiza-on:",
      "content": "this study was able to reveal prevalence of terms that emerged within the documents and show their relevance by how the projecdon of the topic modelling circle and the size of a word in the result visualisadon. the result was visualised using pyldavis which is a web-based interacdve visualizadon package that allows the display of the topics that were idendfied using the lda approach. pyldavis was used for extracdng informadon from the fived lda topic models to design a web-based interacdve visualizadon. the main method that was applied in fig.. list of stop-words removed from the arjcle this study was 'pyldavis.gensim.prepare' which takes as an argument topic models from lda, the vectorized text corpus, and the derived lexicon which contains the dicdonary terms from the study,. each idendfied topic is encoded in the circles of the pyldavis and the bigger the circle the more projecdon or prevalence is the topic (as seen in figuresand). the higher the number of common words among sentences indicates that the sentences are semandcally related."
    },
    {
      "header": "IV. MODEL",
      "content": "a. defining seman-c significance we define the semandc significance of term t to the topic n given the parameter weight of the (\u03bb) where(0 \u2264 \u03bb \u2264 1). let pt denotes the minimal probability of the term t in the lexical corpus. let nt denote the probability of term t element of 1, ..., n for n element of 1,..., k, where n denotes the frequency of terms in the vocabulary (see equadonwhere (\u03bb) is the weight given to probability of the terms t in topic n (equadon 1)"
    },
    {
      "header": "B. Defining Saliency Term",
      "content": "in this study we define saliency term as given a word 'w 0 , we compute its minimal probability p(tm/w). where tm is the topic model. the possibility that the emerge word w was generated from the lda topic model (tm).we also compute the marginal probability p(tm): -with the possibility that any word w 0 randomly selected was generated by tm. we define the uniqueness of each idendfied word 0 w 0 as the divergence occurrence between p(tm/w) and p(tm): we were able to compute 5 topics (t) and 10 passes which were selected from the latent dirichlet allocadon (lda) topic modelling (see equadon 6)."
    },
    { "header": "U(6)", "content": "" },
    {
      "header": "=5",
      "content": "the uniqueness of each term is described as how significance and semandcally associated they are to the topics. for example, a term could be semandcally associated to more than one topic. the frequency and populadon of terms are denoted by the size of the topic circles and also the inter-topic distance denote how closely related the topics are. we nodce a few words that are expressed in several topics, but observing this word w reveals livle informadon about the mixture or semandc associadon of the topics. in some cases, this word might be scored very low in the computadon of it's uniqueness. in order to compute the saliency, we used the following model equadon 7:as illustrated in figure, adjusdng the lambda metric can aid in the significant classificadon and reducing the complexity of the topics. this helps to remove ambiguity of the terms associadon by making term distribudon clearly. looking at the figure, we observed that given equal frequency of words, the list of the most common, relevant or disdncdve terms (e.g. gene, disease, expression, associate ) are prevalence in the visualised graph-plot distribudon. the saliency measures the distribudon of the speeds and idendficadon of topic associadon and composidon (e.g. prevalence topic 1 terms such as genes,disease etc. these terms are all semandcally associated to topics 2 and 3)."
    },
    {
      "header": "V. LATENT SEMANTIC ANALYSIS",
      "content": "latent semandc analysis (lsa) is a robust algebraic and stadsdcal method which extracts hidden semandc structures of words and sentences. lsa is used to extract features that cannot be directly mendoned within the dataset. these features are essendal to data, but are not original features of the dataset. it is an unsupervised approach along with the usage of natural language processing (nlp). it is an efficient technique in order to abstract out the hidden context of the document. we performed a mini summary from the original summary from the study using latent semandc analysis (lsa) for text summarizadon. the mini summary was done from the summary of the original clean 100 ardcles extracted from pubmed (https : //pubmed.ncbi.nlm.nih.gov/) database. this summary is then fived into the rouge metric system to measure the efficacy of the model. results from the lsa present a robust summary of the endre ardcles with useful informadon extracted about specific genes that are associated to cancer disease. below is the summary and visualisadon of key terms from the summary using a world cloud (figure)."
    },
    {
      "header": "A. Sample Extracted Summary",
      "content": "the sentences with the most prevalence sentence score was used for the summary together. we used the heap queue (heapq) library to select the most or very useful sentences. the heapq is used in implemendng the priority queues for word frequencies in sentences with higher weight is given more priority in processing the summary. the threshold indicates the number of sentences to summarize (see table). different threshold points were selected for the summary and the result indicate differently even though the word frequency selected is less than 30 maximum (< 30)."
    },
    {
      "header": "B. Findings",
      "content": "the summary result has revealed very interesdng findings of genes that are associated to some cancerous and type 2 diabetes diseases (see table)."
    },
    {
      "header": "VI. ROUGE: RELIABILITY & VALIDITY OF MODEL",
      "content": "rouge is a metric evaluadon model which stands for recall oriented understudy for gisting evaluation. it is an intrinsic metric for automadcally evaluadng document summaries. this is originally based on a metric used for machine transladon called bilingual evaluadon understudy (bleu). bleu metric is a score for comparing a machine or candidate transladon of text to one or more human annotadon or reference transladons. although developed originally for text transladon, it can be used to evaluate text generated for a set of natural language processing acdvides. rouge has measures that allow for the evaluadon of the accuracy of system summary as compared to a human created summary known as the model summary,. the measures were able to count the number of overlapping units of word such as n -gram, bi -gram and word pairs between the system generated summaries and the model summaries created by humans. this study introduces a few rouge measures: rouge-1, rouge-2, rouge-3, rouge-l, rouge-s included in the original rouge evaluadon model and used in this research. rouge was used to check for the reliability and validity of our model. aqer the model is fived, the external quality of the model is verified according to the fit metric test rouge. common metrics include, but are not limited to, parsimonious fit, valueadded fit, absolute fit and other metrics, and the intrinsic quality of the model is verified through the fit analysis."
    },
    {
      "header": "TABLE III ROUGE METRICS MEASUREMENT SUMMARIES",
      "content": "system and human annotated summaries type summary ssummary 'some of the genes in the bcaa metabolic pathway such as mlycd (rank 164)hadhb (rank 354)ivd (rank 713)mut (rank 921)and pccb (rank 684) are also ranked highly by hridaya. the svms are based on 181 features broadly grouped into (1) gene8c() epigene8c() transcriptomic() phenotypicandevolu8onary. the genes are pdgfrbabl1flt1; and these genes are drug targets of cancer drugs like dasa8nib (targets -pdgfrbabl1)pazopanib (targets -pdgfrbflt1)pona8nib (target -abl1)26.' our result revealed that rouge-1 expressed bever average result for the recall (r), precision (p), f1 score respecdvely with a 95% confident interval (see table). the result revealed bever evaluadon metric in the recall column of the rouge evaluadon metrics. the results expressed bever in rouge-1 with the recall slightly over 83%, precision slightly over 85% and f1-score slightly over 84% as reviewed in table v.  summary model, produce a more appealing result. this was because the second summary was closely aligned with the original automated system summary (see table). this shows that the closeness of the human model summary to the system or reference summary produces bever average across all rouge measuring dimensions (recall, precision and f1 score)."
    },
    {
      "header": "Comparing the system generated summary with a new human",
      "content": ""
    },
    {
      "header": "A. Procedure: Recall & Precision",
      "content": "we have muldple processed ardcles or documents extracted from the web based on key search terms. the documents are stored in a given name cleanhtml.txt file and an automadc summary was generated and stored in a file called summary.txt. we then produced a set of human annotated reference summaries of the cleanhtml.txt document. the recall in the context of the rouge metric simply means we are calculadng how much of reference summary (the human summary) is the system summary (automated machine summary) recovering or capturing from our text. in considering the individual words in a sentence we simply represent this with the formula in equadon 8.the metric will produce a perfect result of 1 which usually will be the case if indeed the sentence matches. this metric simply means all the words in the reference summary has been captured by the system summary.in the system generated summary, which somedmes might be very large based on the threshold selected, capturing all the words in the reference or model summary. however, most of the worlds in the system summary might be unnecessary verbose. but, this where precision becomes very important. in conducdng precision on the summary, we are essendally measuring how much of the system or machine summary is required? we can measure precision using the equadon 9.this means we will evaluate and calculate words in the sentence summary of the recall overlapping with the total words in the system summary. this will predict the words that are relevant which appears in the reference and over the total words in the system summary.the system's summary mostly contains unnecessary words in the summary. therefore, our precision becomes crucial as we are trying to predict generated summaries that should be concise in nature. in this study, we combined and computerised both the precision and recall and further report the f1 -score measure.in order to ascertain the validity of the study, we measured rouge-n, rouge -s and rouge -l which are the granularity of texts that was compared between the system summaries and the reference or human annotated model summaries. rouge -1 refers to the overlap of unigrams between the system summary and reference summary. rouge -2 refers to the overlap of bigrams between the system reference and the model or reference summaries. we computed precision and recall scores of the rouge -2. the main reason why rouge-1 could be considered over others or in conjuncdon with rouge -2 or even other fine granularity measures is because it reveals the fluency of the summaries or if used in a transladon task. the intuidon is that following the word ordering of the reference m=summary indicate that the summary is more fluent.the precision result tells us about the percentage (%) of the overlap between the system summary bigrams and the reference summary. we nodced in the case of the abstracdve summarizadon as both the summaries of system and reference summaries get larger. there are few overlapping bigrams outcome as we are not always or directly re-using the whole sentences for the summarizadon."
    },
    {
      "header": "VII. RESULTS & FINDINGS",
      "content": "the terms in the topic modelling show text which are mostly frequent in the document these were depicted by the size of the circle (as seen in figuresand). representadon of the result using scaver plot would reveal the distance between topics, the distribudon and reladonship between topic levels.the distance between two or more topics is an approximadon of their semandc reladonship. note that close topics such as topics 1, 2 and 3 are semandcally related which describes the terms in the topics. as observed in figuresandthe terms gene and disease are described in the ardcles in reladon to the topics distribudon. this reveals that topics 1, 2 and 3 are semandcally distributed and have reladonship on topic levels. these reveal five selected topics from the topic model analysed using the lda model. the lda model was one of the input argument together with the corpus and dicdonary of the emerging terms used for the topic modelling. the slider (\u03bb) in the web-based interacdve visualizadon depicts the relevance metric of the rank terms. it is worth knowing that the terms of the topic are ranked in decreasing order by default in accordance with the topic-specific probability (\u03bb = 1). figurereveals the common terms from the topic model when the slider is at the full probability. note that the search key gene and disease were used to extract the text data (100 journal papers) from pubmed journal database related to the terms. figureshows the most common terms in topic 2 to be 'gene' and 'disease' when the slider (\u03bb = 0.48) is posidoned at 0.48 probability. viii. limitation this study's limitadons are observed in the precise summary predicdon of ardcles of varying wriven styles. some of the summarizadon models in most cases prefer nouns. themes emerging from ardcles influences the grammadcal structure of certain ardcle summaries. another limitadon of the study was that the model takes longer to evaluate the emerging terms within the topic modelling approach used due to the large text data for analysis."
    },
    {
      "header": "IX. DISCUSSION",
      "content": "in this study, we presented a fully data-driven approach for automadc text summarizadon. we proposed and evaluated the model on unstructured datasets which show some results comparable to the current state-of-the-art topic modelling techniques without depending on modificadons using any linguisdc informadon models. manual summarizadon is laborious and challenging task to accomplish. therefore, automadzadon of the task is very essendal. this process is gaining popularity among researchers. summarizadon technique has been applied to various natural language processing (nlp) task such as in the areas of text analysis, classificadon, automated quesdon and answering, financial and legal texts summarizadon, news summarizadon and reviewing of news headlines and the generadon of social media ardcles,. performing research in these various topics could benefit from the early stages of document summaries which can be integrated into any base model at intermediate stages to help reduce the length of the document for further analysis."
    },
    {
      "header": "X. CONCLUSION",
      "content": "automadc summarizadon is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document. text summarizadon is a very laborious problem to work on without accuracy in the summaries extracted from the documents. this study proposed fully automated single and muldple documents text summarizadon. muldple documents were extracted and summarised while preserving the overarching meaning and purpose of the collecdve ardcles. the lda model was one of the input arguments together with the corpus and dicdonary of the terms that were used to perform the topic modelling in the study. the model designed within the study could conduct a cross-language text summarizadon where ardcles from other foreign languages could be processed and the summary translated into english and other languages. our proposed future study will look into performing topic modelling with these documents and observe whether the approach retain the meaning of the original documents. the result from the future research will be compared with a current machine learning gene predicdon applicadon model designed for a new study on genes and diseases."
    }
  ]
}
