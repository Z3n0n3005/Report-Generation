{
  "id": 8679785423318068944,
  "name": "on_foundations_for_deductive_mathematics.grobid.tei.xml",
  "segments": [
    {
      "header": "abstract",
      "content": "this article was motivated by the discovery of a potential new foundation for mainstream mathematics. the goals are to clarify the relationships between primitives, foundations, and deductive practice; to understand how to determine what is, or isn't, a foundation; and get clues as to how a foundation can be optimized for effective human use. for this we turn to history and professional practice of the subject. we have no asperations to philosophy.the first section gives a short abstract discussion, focusing on the significance of consistency. the next briefly describes foundations, explicit and implicit, at a few key periods in mathematical history. we see, for example, that at the primitive level human intuitions are essential, but can be problematic. we also see that traditional axiomatic set theories, zermillo-fraenkel-choice (zfc) in particular, are not quite consistent with mainstream practice. the final section sketches the proposed new foundation and gives the basic argument that it is uniquely qualified to be considered the foundation of mainstream deductive mathematics. the \"coherent limit axiom\" characterizes the new theory among zfc-like theories. this axiom plays a role in recursion, but is implicitly assumed in mainstream work so does not provide new leverage there. in principle it should settle set-theory questions such as the continuum hypothesis."
    },
    {
      "header": "Abstractions",
      "content": "\"foundation\" refers to a body of deductive practice, here mainstream deductive mathematics. historically practice comes first. foundations are a way to analyze the body of practice, particularly its consistency.(1) a primitive is an object, hypotheses, or method of argument that is postulated rather than deduced from something else. (2) a primitive foundation is a collection of primitives that is complete in the sense that no other primitives are needed. note that the methods of argument (the logic) is part of the foundation. (3) a foundation is consistent if every chain of deductions that produces a contradiction can be shown to have a logical error.2.1. about consistency. the operational view of consistency is \"complete reliability\", in the sense that the outcome of an error-free argument will never be contradicted by the outcome of any other such argument. moreover, the outcome will not introduce errors if used as an ingredient in further arguments. arguments by contradiction for example, assume something, deduce a contradiction, and conclude that the thing assumed must be false. success is sensitively dependent on everything else used in the argument being completely reliable, and that the logic itself does not introduce errors.consistency provides an internal criterion for correctness. in practice, however, it is far more important as a criterion for incorrectness. in reasonably ambitious research programs, 95 percent or more of attempted proofs turn out to have flaws. sometimes, with luck and persistence, these flaws can be corrected, and the attempts converge to a valid argument. sometimes years of effort have to be abandoned as hopeless, or outright wrong. the payoff for this painful process is closure: even outrageously counterintuitive conclusions are accepted when the arguments are carefully checked and found to be error-free.empirical. g\u00f6del has shown that foundations generally cannot prove their own consistency. however, we can test consistency: make lots of deductions, and verify that any contradictions are accounted for by logical errors. sometimes inconsistencies do not appear until work reaches a certain level of sophistication. another wrinkle is that a foundation can be technically inconsistent but consistent in practice in the sense that methodology has evolved so as to steer people away from the inconsistencies. the main point is that empirical consistency is a property of a methodology rather than of a foundation. explicit foundations are largely retrofits that record successful experience and narrow down potential sources of error.global. evidence for consistency applies to the development as a whole, not just individual results. in other sciences, and in mathematics when trying out new methodologies, experiment can give evidence that individual conclusions are reliable, within limits and independently of most other conclusions. foundational consistency checks are global in that they support consistency of the whole development. this is the flip side of the fact that a single genuine failure would throw doubt on the whole development.2.2. definitions and axioms vs primitives. we clarify usage in the mathematical community.definitions are located inside a developed context, and usually consist of axioms that the things being defined should satisfy. properties of these things are to be inferred from the axioms and more-basic ingredients. in particular, while intuition and intent can guide the formation of a definition they have no logical force. indeed, first attempts at definitions typically fail to have intended properties. major standard definitions are often the result of years of work to fix failures and consolidate partial successes, and should be thought of as distilled hard-won wisdom. they are certainly not arbitrary constructs.primitive objects, hypotheses, etc. are starting points for the logical context, not defined in terms of lower-level objects. primitive hypotheses, in particular, are not axioms in the contemporary sense. since properties cannot be inferred from lower-level material, they depend on heuristic ideas, syntax specifications, and examples.for example, euclid's description of points as \"things without width or length\" is not a definition in the contemporary sense because \"width\" and \"length\" have not been given precise meaning. it succeeds as a primitive because this, filtered through physical experience, some pictures, and examples of usage, reliably evokes consistent understandings. polygons are 'defined' because they are described in terms of primitives.we emphasize that consistency at the primitive level depends heavily on different users extracting functionally equivalent understandings. this has often been a problem.2.3. deeper is better. in practice, definitions are often easier to learn and use with precision than are complicated primitives. for example, attempts to use the real numbers as a primitive were problematic. now the reals are defined in terms of natural numbers, and natural numbers are defined using set theory. this approach has made full-precision understanding of the reals faster and more routine.curiously, another benefit of working several levels above the primitives is that it can filter out misunderstandings or ambiguities about the primitives. for instance, the traditional axiomatic approach to set theory takes 'sets' as primitives, and the primitive hypotheses proscribing their behavior are quite complicated. but these do not have to be understood in detail to get a completely solid description of the real numbers. the new approach to set theory has a similar advantage: the primitives (object generators) are several levels below sets, so sets are defined and their properties proved rather than hypothesized. this gives a more accessible, and slightly sharper, understanding of set theory.there is a vast literature, mostly philosophical, about the nature and role of foundations. it is far from consistent, and while there are interesting insights, cf., some of it is pretty silly. it does not seem to be useful to try to sort through it here."
    },
    {
      "header": "Sketchy history",
      "content": "we draw lessons from a few key historical deveopments, mainly to provide perspective on what happened in the early twentieth century. the accounts are ideosyncratic, brief, and superficial, but sufficient for the purposes here. for a bit more detail i have found the essays in 'the stanford encyclopedia of philosophy' to be helpful, for exampleand.3.1. ancient greece. the first relatively explicit use of a foundation was in the geometry and number theory of ancient greece. the primary significance at the time seems to have been its use as a metaphor for order in the natural world, not for its \"real-world\" applications. everyone could see, and be impressed by, genuine consistency coming from logic and innate understanding, as opposed to the chaotic array of beliefs due to cultural bias or religious or philosophical doctrine. the practical applications are pretty weak. to some extent this can be seen as a quantifier issue: for metaphors it is sufficient that the theory have some striking outcomes. practical use requires ways to describe outcomes for all problems in appropriate settings. for example, clever geometric tricks can determine the area between a line and an arc of a circle in a few special cases. calculus gives a description of every such area, and the appearence of transcendental functions in the solution shows that very few cases can be solved with ancient techniques. nonetheless many people find the classical tricks more meaningful than routine but effective calculus.meaningful vs effective. we expand on this last remark. it has been known for over fifty years that humans have an innate physics, and it is essentially aristotlean. people therefore find aristotlean physics comfortable and meaningful, and it was accepted for centuries even though it fails very simple consistency tests. in general, humans seem not to expect consistency, either internal or with reality, of the things we find meaningful.the lesson for geometry is that euclidean geometry is in large part an exploration of human visual perception and physical experience. the fact that it accurately reflects the structure of the plane is because our visual perception does, not because it is a direct exploration of the plane. much of its power as a metaphor comes from this coincidence: it is comfortable because it matches our perceptions, and effective because in this case perception accurately reflects reality. another lesson from aristotlean physics is that outside of some areas in geometry our physical intuitions do not match reality very well. approaching mathematics through direct perception seldom leads to effective internalization, but is attractive and comfortable enough that it remains a strong theme in elementary education.europe in the 1600s. the next major shift began in the early 1600s when galileo, kepler, and others showed that mathematics could be directly powerful in the world, not just a metaphor. mathematical development became driven by the needs of physical models, and methodology evolved rapidly. for instance, there is a conflict between ratios and negative numbers that had inhibited use of negatives. this was resolved by replacing ratios with fractions. compact algebraic notation replaced earlier discursive formulations. the ancient greeks were justly proud of their resolution of \"multitudes\" into natural numbers, and units indicating the things being counted, but the analogous resolution of \"magnitudes\" into real numbers and units was finally accomplished only in the 1600s. by the time newton, leibnitz, and their contemporaries were active, the power of application-oriented mathematics was firmly established and core methodologies were taking shape.infinitesimals were often part of the foundational intuitions of the real numbers in this period. they worked quite well for a time, but eventually became problematic and had to be replaced by limits. we explain how this fits into the analysis here. consider the truncated polynomial ring r[\u03b4]/(\u03b4 2 = 0). the implicit presumption was that a function f of a real variable should extend to this ring, and fy\u03b4 where g is the derivative of f . this is true if f is given by a power series: simply plug the extended variables into the series. infinitesimals thus worked as long as it was harmless to assume functions were piecewise analytic. but limits of analytic functions need not even be piecewise continuous, and continuous limits can be nowhere differentiable. when limits became important (eg. for fourier and laplace series) infinitesimals became untenable. the methodology had to evolve from one in which differentiability was assumed, to a more primitive approach in which differentiability had to be defined and proved.in the 1960s abraham robinson showed infinitesimals can be used consistently if arguments are restricted to first-order logic. but this first-order logic constraint is strongly inconsistent with standard practice. this is a potential special-purpose tool, not an alternative general approach.3.3. mid to late 1800s. in this period it was still generally felt that real numbers should be understood through intuitions from physical experience, though, of course, this should not include earlier intuitions about infinitesimals. this became problematic as goals became more ambitious. people without sufficiently accurate intuitions had unreliable outcomes, and were reduced to appeals to authority (\"gauss did it so it must be ok\") or were shut out of the game. from a foundational point of view, \"intuitive reals\" became unsatisfactory as a primitive because it was not uniformly understood by different users. this is a bit different from problems with infinitesimals, which in the larger context are inconsistent even when correctly understood.in the mid 1800s dedekind rigorously described the reals in terms of natural numbers. this made them reliably accessible but didn't fully fix the difficulties, because the natural numbers had varying interpretations. some intuitively-attractive ones lacked logical force. this was addressed by defining natural numbers using early versions of set theory. for human use, sets seemed to be effective starting points. by 1880 dederkind was using set-theoretic methods and terminology, and the idea that mathematics could have a foundation in a theory of sets began to have traction; cf..by 1900, foundation-oriented mathematicians were having good success with what we now know as na\u00efve set theory. coming from another direction, frege, cantor, and others clarified intuitive notions about set theory, and showed that it had its own substance.in 1902 russell publicized his paradox showing na\u00efve set theory to be inconsistent. reactions were quite different in the mainstream and set-theory communities, and resulted in them drifting apart.mainstream community. by 1900 quite a few mathematicians had found na\u00efve set theory to be effective and reliable, and many leading mathematicians were satisfied that all mainstream work could be derived from it. the response to russell's paradox was to add the caution \"don't say 'set of all sets' \". this did not effect actual practice since nobody saw a need to do that anyway.by the 1920s much of the younger generation had embraced the set-theory foundation and the rigor it enables, and were extremely successful with it. the main set-theory concern of the period was that the axiom of choice seemed too good to be true, not that there would be more problems with na\u00efve set theory.set-theory community. the paradox seriously set back the study of sets as a subject. after a few more unsuccessful attempts they gave up trying to define sets directly, and instead began abstracting key properties (of na\u00efve set theory) to use as axioms.in the next phase of the development, the notion of \"function\" seemed problematic. a na\u00efve function x \u2192 y is \"an assignment of an element of y to each element of x\". this does not qualify as a definition because \"assignment\" is neither defined nor declared as a primitive. in fact this description of 'function' works well as a primitive, but the community choose not to follow this route. instead a single function, the 'membership' operator was taken as a primitive. specifically, an implementation (or \"model\") of the axioms is a pair (u, \u2208), where u is a 'universe' of possible elements, and the membership operator is a binary pairing \u2208 : u \u00d7 u \u2192 yes/no. in general, subcollections of u correspond to binary functions u \u2192 yes/no. the sets of the theory correspond to binary functions of the form (# \u2208 x) for some x \u2208 u . note that elements are used to parameterize sets. one source of confusion is that this parameterization is sometimes described as an identification: \"elements of sets are again sets\". this phrasing may be a hangover from 19th century philosophy. more general functions are then defined in terms of sets. specifically, a function x \u2192 y is a correspondence whose graph is a subset of x \u00d7 y . the standard forms of the axioms up to this point are mainly due to zermillo.a problem with the zermillo axioms is that they do not ensure there are enough functions to transact the basic business of set theory. experimentation, drawing on a strong legacy of formal logic, revealed that functions obtained from set operations by first-order logic are sufficient to give workable rigorous set theories. standard formulations of the logical hypotheses are mainly due to fraenkel. many variations have been considered, but after about 1920 the zermillo-fraenkel (zf) axioms were widely accepted as standard. we will see, however, that while this solved a problem in set theory, it disconnected formal set theory from the mainstream.3.5. mid 1900s to early 2000s. early in this period the axiom of choice was accepted as well-tested, and included in standard formulations of both communities. zermillo-fraenkel-choice became the \"gold standard\" in the set-theory community, and na\u00efve-with-choice (as always, with the set-of-sets constraint) remained the implicit standard for the mainstream.mainstream community. na\u00efve set theory-as used in standard practice-enabled highly rigorous arguments of a depth unimaginable in the previous century. the consistency of na\u00efve set theory-again, as used in standard practice-is extremely well tested. it is possibly our most solidly established empirical conclusion. there are some drawbacks, but there is no reason to accept any loss of functionality to address these drawbacks.one drawback is that category theory does not fit comfortably in standard set theories. categories were introduced essentially as bookkeeping devices to encode large-scale structures in algebraic topology and homological algebra. for these purposes ignoring set-theory mismatches seemed harmless. by the late 1900s delicate work with higher-order category theory and point-set properties of universal spaces needed more precision. there were proposals to use categories as a substitute for set theory, cf.,. grothendieck developed a variation, topos theory, initially as a setting for vigorous abstract algebraic geometry. as it matured this was also promoted as a substitute for set theory. later voevodsky proposed using his \"univalent foundation\" as a setting for mathematics, citing its effectiveness for computer implementation.these proposals reflect a misapprehension about foundations. we have stressed that the core job of a foundation is to give a distilled starting point that identifies key consistency checks, and to encode the results of such checks. to be effective a foundation should be minimal and as simple as possible. the proposals above are quite elaborate, and designed to display mathematical structure or make it more mechanically accessible. these are good things for applications but, as we saw with the real numbers, working directly with high-level concepts is likely to cause problems. the situation seems to be that for generations, set theory has provided such a rock-solid base for mainstream practice that we have forgotten what it is like to have to worry about the reliability of our tools. the proposals implicitly assume consistency will follow from our experience with set theory. indeed many of them implicitly depend on a background set theory, so implicitly presume consistency to be inherited from that.the foundation described in the next section begins several levels below set theory. the lower-level environment seems to give an effective setting for categories.set-theory community. the zfc axioms were found to be under-determined, and in 1963 paul cohen introduced forcing as a systematic way to construct new models. a flowering of the subject ensued, with many thousands of pages devoted to showing a wide variety of hypotheses are consistent with, or independent of, the basic axioms. deep results were found in other directions, pcf theory and large cardinal axioms to mention only two. seefor an extensive introduction.however, these theories do not provide a foundation for mainstream practice. sufficiency of na\u00efve set theory was asserted around 1900, and this assertion does not apply to theories that require first-order logic. the general problem is described by the \"coherent limit axiom of] but we illustrate it in an important special case.recall that every real number in the unit interval has a base-2 decimal expansion that assigns either 0 or 1 to each natural number. this assignment can also be thought of as specifying a subcollection of n. the restriction of the assignment to any finite interval [0, n] of natural numbers is a function in any zfc theory. equivalently, the intersection of the subcollection with [0, n] is a subset in any zfc theory. the full assignment is the limit of these restrictions, or equivalently the subcollection is the union of the intersections. however, there are zfc models in which some of these limits are not functions in the model, or equivalently the union of the finite subsets is not a set in the model. the corresponding real numbers are missing from such theories. to insure a particular real number is in a particular model we would have to employ either special information about the model, or firstorder logic. this is an issue even for elementary calculus. but in the mainstream nobody addresses this. the conclusion is that zfc is too restrictive to describe standard mathematical practice, even at basic levels.the insufficiency of zfc does not seem to be a deep point. the fact that it went generally unremarked may be another instance of meaningfulness overshadowing concern for effectiveness."
    },
    {
      "header": "Object generators",
      "content": "we sketch the proposed new foundation; see] for details, and] for a guide to routine use. 4.1. generators. the full story begins several levels below set theory, with \"object generators\" and their morphisms. the intuition is that object generators \"generate objects\", essentially like the 'object' primitive in category theory. if g is an object generator then x \u2208\u2208 g means \"x is a an object in g\", and generators are defined using the syntax \"x \u2208\u2208 g means (\u2022 \u2022 \u2022)\".essentially nothing else is included. unlike sets, generators provide no way of identifying their own outputs, and no way of knowing if two outputs are the same. further, while expected uses are for mathematical objects, for instance x \u2208\u2208 groups means \"x is a set with a group structure\", nothing prevents silly examples, for instance x \u2208\u2208 g means \"x is a tuesday in the month of may, 1917\". rather than try to prevent such things we filter them out at a later stage. note that few requirements means few opportunities for contradiction.\"morphisms\" of generators are essentially the same as \"functions\" in na\u00efve set theory: a morphism f : g \u2192 h is an assignment of an object f [x] \u2208\u2208 h to every object x \u2208\u2208 g. 4.2. logic. the native logic of object generators is non-binary in the sense that we might assert that x, y are the same, but in general there is no (yes, no)-valued function that can detect this. this logic is unfamiliar and somewhat complicated, and the first step toward set theory is to establish a sub-context that does use binary logic.the key ingredients are binary functions: functions to an object generator with exactly two objects. we denote this by {yes, no}, or {0, 1}. one of the primitive hypotheses (axioms) asserts the existence of a two-object generator. standard binary logic applies to such functions. incidently, these functions have no allowance for time dependence, so they cannot detect things in the physical world. this is how silly examples of object generators get filtered out.a logical domain is an object generator, say a, with a binary pairing a \u00d7 a \u2192 {yes, no} that returns 'yes' if the two elements are the same. the name 'logical domain' is supposed to suggest that these are natural settings for binary logic.quantification is the final ingredient. if a is a logical domain then the object generator whose objects are binary functions on a is called the \"powerset\" of a, and is denoted by p[a]. the domain supports quantification if there is a binary function p[a] \u2192 {yes, no} that returns 'yes' if and only if the input is the empty (always-'no') function. in standard logic the empty-detecting operator is given by f \u2192 \u2200x \u2208 a, (f [x] = no). the point is that if this one quantification expression is implemented by a binary function, then all quantifications over a are.finally, sets (relaxed sets, if distinctions are necessary) are defined to be logical domains that support quantification.hypotheses. there are four primitive hypotheses. three are standard: there is a 2-element set; the axiom of choice; and the natural numbers support quantification. to clarify this last, the first two hypotheses suffice to construct the natural numbers, but do not imply that it supports quantification. this is equivalent to the standard axiom of infinity in zfc. the final hypothesis asserts that if a logical domain supports quantification then so does its powerset.note that zfc has many more, and more complex, axioms.4.5. comparisons. we compare this theory, na\u00efve set theory, zfc axiomatic set theories, and standard mathematical practice. first, the \"=\" operator built into the logic of na\u00efve and zfc allows comparison of any two elements of any two sets.in relaxed set theory \"=\" is only defined for elements of a single set. mathematical practice is modeled on na\u00efve set theory so in principle a global \"=\" is available but, as far as the author can tell, it is never used. functions are primitives in relaxed and na\u00efve set theory; are described in the same way; and both are consistent with standard practice. in zfc only the equality and membership functions are primitive. functions obtained by first-order logical expressions are also functions in the theory. beyond this, functions are defined to be correspondences whose graphs are subsets. as explained in the previous section, this is not consistent with standard practice.finally, relaxed sets satisfy versions of the zfc axioms. this is made precise in] with the construction of an object that satisfies the zfc-1 axioms, where \"-1\" means \"ignore first-order logical constraints\". as above, this makes sense because functions are taken as primitives. this construction turns out to be maximal so, in particular, all models of zfc uniquely embed in this as transitive sub-models. the \"coherent limit axiom\", a generalization of the real-number problem described at the end of the last section, is shown in] to characterize this maximal theory. this axiom is routinely assumed in standard practice so standard practice, by default, takes place in relaxed set theory. we note that identifying the 'missing axiom' (g\u00f6del's phrase) does not lead to new mainstream methodology, but rather emphasizes the extra work that would be required to stay in a smaller zfc model.the conclusion is that the relaxed set theory defined in] is uniquely qualified to be a foundation consistent with mainstream mathematical practice."
    }
  ]
}
