{
  "id": 683262789845413434,
  "name": "block_diagonal_idiosyncratic_covariance_estimation.grobid.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content": "covariance matrix estimation is a heavily researched topic in many fields, and is a crucial component for risk modeling in finance, where risk models rely on the estimation of the asset return covariance. with the growth of the number of financial assets, high dimensionality of these estimates becomes an issue -the sample estimates may be noise driven and no more reliable. moreover, due to the dynamic nature of financial markets, estimates from long historical data may be obsolete and relatively short time windows are used instead -this setting of high dimension and low sample size (where the number of variables p exceeds the sample size n) is very common in finance today. fortunately, financial markets also display a certain level of structure which can be used to obtain reliable estimates in such adverse environments. mainly, asset pricing literature finds that a sizeable amount of variance in large panels of asset return data is driven by a smaller number of factors. asset return dynamics and their correlations are thus often explained using factor models, with a common component (from exposure to these common factors), and an idiosyncratic component (specific for each asset). under some reasonable assumptions, the asset return covariance under such a model is the sum of a common covariance component (which is low-rank, since the number of factors is much lower than the number of assets) and an idiosyncratic covariance component. this leads to a number of structured and well-conditioned estimators of the covariance matrix which mostly amount to estimating the factor model parameters. however, these estimators focus mostly on the identification of pervasive factors, their interpretation, and performance in asset pricing. the correlation structures within the idiosyncratic components have received comparatively little attention. since these correlations are likely due to exposure to non-pervasive factors such as sectors, countries, or asset classes, ignoring these factors considerably reduces the performance of the estimators.in this paper we focus on the problem of structured estimation of the idiosyncratic covariance component in high-dimensional factor models, based on the assumption that the idiosyncratic correlations arise between assets exposed to some common but non-pervasive factors. an important requirement is to ensure positive-definite estimates of covariance matrix estimates even in the high-dimension-low-sample-size setting of p > n. some of the early approaches based on high-dimensional factor models ensured positive definiteness by assuming a diagonal idiosyncratic covariance, which completely ignores the elements of risk arising from the correlations between the idiosyncratic components. more appropriately, assuming sparsity of the off-diagonal correlations in the idiosyncratic components allows for the approximate factor model structure. a number of thresholding procedures have been devised earlier with the goal of estimating sparse covariance matrices (assuming sparsity of the entire covariance). combining the high-dimensional factor structure and the assumption of sparsity of the idiosyncratic covariance (i.e. conditional sparsity) led to estimators such as the poetand s-poet, which were shown to produce estimates which perform well for some portfolio optimization use cases. however, the thresholding methods used in these estimators do not exploit any common structures in the idiosyncratic components, which are known to occur due to sector, asset class or other non-pervasive factor exposure. in this paper we use these structures to our advantage: by assuming that the idiosyncratic components exhibit correlations due to some group-specific factors such as asset class or sector classification, their covariance structure becomes block-diagonal. this leads to a potentially wider set of positive definite estimates, and allows for a richer description. as the main information is extracted in the common component, the factors that may exist within the clusters will generally be weak and hard to identify. moreover, the unknown cluster membership together with the unknown number of factors within each cluster additionally complicates estimation procedures. finally, the groupings themselves may not be easily incorporated into linear factor models if the effects of the cluster-specific sources of variation are not linear. to allow for the latter and avoid any formerly mentioned obstacles, we focus on treating the cluster-specific dependencies as the idiosyncratic component of the covariance in the factor model.we develop a set of estimators which firstly calculate the low-rank common covariance component using principal components, and then use the residuals to estimate the unknown group memberships and the resulting block-diagonal idiosyncratic covariance. we use several clustering approaches to estimate these groups, and propose a cross-validation procedure for selecting the optimal grouping (and consequently the idiosyncratic covariance). since the cluster sizes are allowed to grow beyond the sample size, we also apply covariance shrinkage to each of the blocks to ensure positive definiteness of the estimates even in high-dimension-low-sample-size settings. this allows us to conduct a comprehensive study of the performance of different covariance estimators for high-dimensional factor models with a block-diagonal idiosyncratic covariance structure. we develop a simulation framework to test various settings and configurations of these blocks, and also apply the developed estimators to historical market data. to measure the performance of the estimates we consider both the measures of how well the idiosyncratic covariance patterns are identified, and the out-of-sample performance of portfolios constructed using the covariance estimates. simulations show that the estimation method using a hierarchical clustering approach is able to perform very good in both sparse performance measures and overall, and is able to successfully estimate structures with very small clusters. results on historical data show excellent out of sample performance of all the clustering approaches and concludes with the observed differences of the final idiosyncratic covariance estimates obtained by the different clustering approaches."
    },
    {
      "header": "Model",
      "content": "let y denote the p-dimensional random vector of asset returnsfor p assets. we consider the latent factor modelwhere b is a p \u00d7 k matrix of factor loadings, f is a k-dimensional random vector of k common factors and \u03b5 is a p-dimensional random vector of the specific factors, also known as the idiosyncratic component. the factor loadings b, the factor realizations f and the idiosyncratic component \u03b5 are considered to be unobservable, so the factor model parameters need to be estimated from the observable asset returns. the idiosyncratic and common factors are assumed to be uncorrelated, which is a common assumption helping with their identifiability. under the model, the asset return covariance cov(y ) = \u03c3 has the following decomposition:where \u03c8 = cov(\u03b5) is the covariance of the specific factors, also known as the idiosyncratic covariance.the common covariance component bcov(f )b \u2032 is low-rank (since k < p), and explains the majority of the correlations between different assets as the result of their exposure to a smaller number of common factors. the idiosyncratic covariance \u03c8 is often considered to be diagonal, however in this paper we consider models from the category of approximate factor models, where some sparse correlations between idiosyncratic components are allowed -thus the idiosyncratic covariance is full rank and sparse. moreover, motivated by the documented grouping of financial assets (within industries or asset classes), we allow the idiosyncratic components of asset returns to be associated with one of a total of m clusters. the idiosyncratic components between assets within the same group may be correlated, and the idiosyncratic components of asset pairs from different groups are uncorrelated. this means that (if the assets were sorted according to group membership) the idiosyncratic covariance has a block-diagonal structure. note this setting does not exclude singleton clusters with only one asset whose idiosyncratic component is uncorrelated to all the others. let c m denote the subset of assets within cluster m (where m \u2208 1, ..., m ). if the assets are sorted according to their cluster membership then the idiosyncratic covariance matrix has the following block-diagonal structure:where \u03c8 (cm) is the idiosyncratic covariance of all assets within cluster m. we allow for the setting where the number of clusters m is large, even as large as or close to p (meaning that all assets belong to their own cluster, and that the resulting covariance matrix is diagonal). the cluster memberships, the number of clusters and their sizes are all considered unknown and need to be estimated from the data. ultimately, once these are known, the covariance elements themselves need to be estimated as well.to ensure the identifiability of the factors and factor loadings in the latent factor model, a usual restriction is that b \u2032 b is diagonal and cov(f ) = i p. in order to be able to estimate and distinguish the factors from the idiosyncratic components, several assumptions are imposed on the spectrum of the asset return covariance. firstly, the eigenvalues associated with the common factors (the largest k eigenvalues of \u03c3) are unbounded and assumed to grow with growing dimensionality p. secondly, the eigenvalues of the idiosyncratic covariance are bounded as p grows, so that they do not \"leak\" into the spectrum of the common component. this enables a two-step estimation approach such as in the poet and s-poet estimators, which we also follow in this paper."
    },
    {
      "header": "Estimation",
      "content": "all of the model parameters -the factor loadings and idiosyncratic covariance -need to be estimated from the data sample y \u2208 r p\u00d7t . in this section and the rest of the paper, we refer to the estimates of the asset return covariance as \u03c3, with the index noting the type of estimator. the primary important estimator is the sample covariance:where y t is the p-dimensional vector of asset returns at time t and y is the p-dimensional sample mean return. following the assumptions stated in the previous section, the p \u00d7 k matrix of factor loadingswhere \u03bb 1 \u2265 \u03bb 2 \u2265 ... \u2265 \u03bb p are the eigenvalues and \u03b3 i , i = 1, ...p the corresponding eigenvectors of the sample covariance matrix \u03c3 s. thus, the estimators considered and proposed in this paper are of the following form:where \u03c8 is the estimate of the idiosyncratic covariance matrix. recent results on the asymptotics of the eigenstructure of high-dimensional covariance matrices suggest that the eigenvalue estimates are biased. to mitigate this estimation bias, we replace the sample estimates \u03bb i in the estimatorwith the shrunk eigenvalues:where c is calculated as:note that the bias correction term cp/t in (6) diminishes as the number of samples t grows with respect to the dimensionality p. since we deal with high-dimensional cases when p > t , this term will not be negligible.the estimators generally follow a two-step procedure:1. estimate the common componenti , using the first k principal components: \u03bb s i are the shrunk eigenvalues from (6) and \u03b3 \u2032 i are the corresponding sample eigenvectors.2. apply a sparse estimation procedure to the residual covariance matrix, also known as the orthogonal complement:, in order to obtain a sparse estimate of \u03c8.it is important to note that the orthogonal complement s is a full matrix of rank min(n, t ) -k which does not serve as an idiosyncratic covariance estimate \u03c8 (since it is not a sparse matrix, nor a full-rank matrix). different estimates \u03c8 are obtained from s by applying some sparsityinducing procedures (such as thresholding or the proposed clustering based estimation). the first step described above is based on the sample principal components, and is common to all of the estimators considered in this paper. what this paper focuses on is the second step -the idiosyncratic covariance estimation, in the presence of clustered specific components, with an unknown clustering.the following sections lay out the elements of the estimation procedures for different important quantities.an important issue to deal with before we delve deeply into the specific of the estimators is the estimation of the number of factors k. in this paper we follow the bai-ng approach and use an information criterion (labeled ic1 in the original paper). the bai-ng information criterion (ic) defines the procedure to estimate k aswhere n is an upper bound for the possible number of latent factors (often set to min(t, p)), b is the p \u00d7 k loadings matrix estimate for k factors, and \u2206 is a k \u00d7 k diagonal matrix with the k largest eigenvalue estimates on the diagonal. the first term in equation 8 describes the log mean square error of reconstructing the original data sample y using the estimated factor model, which is reduced by increasing the number of factors. the second term is a penalization term which grows with the number of considered factors k. ultimately, the information criterion balances the reduced reconstruction error with the added complexity of the model and will result with an estimate of the number of factors k which yields the best reduction in error for the smallest number of factors.this procedure yields in choosing first k eigenvalues which have significantly higher value than the rest, thus making it worth to be established as factors. the rest of the eigenvalues, with much lower amount of carrying information are thus left in the orthogonal complement matrix and subject to sparsity inducing methods.the state-of-the-art estimators most commonly use generalized thresholding procedures, the resulting sparse estimates of \u03c8 have no underlying structure, and are limited to a very narrow range of possible estimates which are positive-definite. the most sophisticated thresholding methods include adaptive thresholding, applied to the orthogonal complement matrix s = ( s ij ). the idea is to apply generalized thresholding operator function f \u03c4ij to the full covariance s in order to obtain the sparse estimate \u03c8 \u03c4 :for any \u03c4 ij \u2265 0, the generalized thresholding operator is a function f \u03c4ij : r \u2192 r which, for all z \u2208 r satisfies the following conditions::these conditions are satisfied by several popular thresholding functions, out of which we consider the following:\u2022 hard thresholding :\u2022 soft thresholding:\u2022 adaptive lasso:\u2022 scad (smoothly clipped absolute deviation):the adaptive thresholding parameteris of the formwhere \u03c4 is a tuning parameter and \u03b8 ij are estimates ofthe parameter \u03c4 can be set as fixed or obtained through a data-driven cross-validation procedure. in the latter case, the procedure iterates over the space of possible values of \u03c4 for which the estimates \u03c8 are positive-definite. on the one side, for large values of \u03c4 the estimates become diagonal. unfortunately, for lower values of \u03c4 (which allow more non-zero entries) the matrices quickly stop being positivedefinite (due to the non-zero entries not following any specific patterns), thus narrowing the space of acceptable values of \u03c4 and limiting the estimators. through the paper, thresholding based estimators are denoted with \u03c8 sof t (soft) for the soft thresholding function, \u03c8 al (al) for the adaptive lasso thresholding function, and \u03c8 scad (scad) for the scad thresholding function.as mentioned, these estimators provide sparse estimates with no inherent structure, thus potentially missing out on certain narrow factors such as sectors, asset classes or countries. recent literature has documented that financial assets exhibit clustering patterns, even when the common factors are filtered out. this motivates the approach proposed in this paper -under the hypothesis that the idiosyncratic covariance reflect this grouping in the residual time series, we formulate a number of clustering based estimators.we denote the group membership information as a zero-one p \u00d7 p indicator matrix c (also known as a mask ), where the element c ij = 1 if i and j are in the same cluster group c m , for i, j \u2208 1, ..., p. if the rows and columns of c (and consequently, the p assets in the factor model ()) are sorted according to their cluster membership, then c is a block-diagonal matrix. without loss of generality, in the following notation we assume that the assets are sorted according to their cluster membership (this can also be done once the clustering is known) and that c is block-diagonal.let matrix c contain c 1 , c 2 ,...,c m cluster blocks for each of the m clusters. the imposed block-diagonal idiosyncratic covariance \u03c8 c which is obtained from the orthogonal complement s (the initial full idiosyncratic covariance estimate) is:where each block is defined as ( s cm = s ij 1 (ij)\u2208cm ), m \u2208 1, ...m , and \u2022 denotes the hadamard element-wise product. the approach proposed above is used for all the different estimators of the block-diagonal idiosyncratic covariance. however, it still does not guarantee positive-definiteness of the covariance estimates. for instance, when the dimension of a block c m (the number of time series in cluster m) is larger than the length of the time series estimation window t , some eigenvalues of the blockdiagonal idiosyncratic matrix estimate (and thus some of the eigenvalues of the entire covariance matrix estimate) are very close to zero (or exactly zero), resulting in the covariance matrix estimate which is not positive definite. the positive definiteness of the idiosyncratic covariance is important in applications where the inverse of the estimated covariance matrix is needed (for example, if we want to perform portfolio optimization using the covariance estimate). as our intention is to produce the method with no constraints on sample size as well as no constraints on the number of clusters (and thus cluster sizes), we incorporate a shrinkage method within the blocks which always results with a positive definite matrix. although there are many different forms of shrinkage and possible shrinkage targets, to avoid additionally complicating the procedure we use linear shrinkage, applied to each block s cm separately.linear shrinkage can be viewed as a weighted average of the variance part and bias part of the covariance estimates, where weights should optimize the bias-variance trade-off. we treat each block s cm , m \u2208 1, ...m as a separate covariance matrix and perform the shrinkage procedure on it. a common form of the estimator is a linear combination of the covariance matrix s cm = ( s m ij ) and the shrinkage target matrix scm , with sample variances s m ii = [ s m 11 , . . . , s m pp ] \u2032 on the diagonal and covariances r s m ii s m jj off diagonal, where r is the average of all sample pairwise correlations. the shrinkage estimator is defined as:where \u03b1 m is a scalar parameter between 0 and 1 which we search for each block component c m , m \u2208 1, ..., m . to estimate \u03b1 m from sample data, we follow the well-established ledoit and wolfprocedure which can be found in the b. the resulting positive definite idiosyncratic covariance estimate isnow, the estimation of the idiosyncratic component as a block-diagonal matrix rests solely on the method employed to determine the blocks themselves -the structure of c.the simplest approach, which we lay out here as a benchmark, is to use pre-determined classifications or groupings of assets to formulate the clusters in the idiosyncratic component. in this approach, the sparse component is obtained by setting to zero all the pairs which are not in the same group and leaving the sample values of the orthogonal complement for the entries corresponding to the pairs in the same group. we formulate the clustering-shrinkage estimator based on industry classifications \u03c8 csi (csi) which relies on stock industry classification data to estimate the idiosyncratic component:1 i and j are in the same asset group, 0 otherwise.however, this approach suffers from several drawbacks. firstly, the classification data (such as industries, asset classes or countries) are not always available for different datasets and asset universes. secondly, the classification itself may not be optimal, since the grouping does not guarantee the highest asset return correlations within the groups. to alleviate these issues, we propose two clustering based methods to estimate the block-diagonal idiosyncratic covariance.a natural extension of the previous approaches based on industry classifications of stock data is to estimate the optimal groupings from the data. in this section we develop a procedure based on different clustering approaches to the orthogonal complement s, resulting in block-diagonal estimates \u03c8 of the idiosyncratic covariance. the clustering procedures are applied to the residual series e \u2208 r p\u00d7t :which represent the estimates of the specific factor (\u03b5) realizations e = (e it ). thus e i (or e j ), i, j = 1, ..., p is a 1 \u00d7 t vector of one time series, while e t , t = 1, ..., t is a p \u00d7 1 vector of all time series at the one moment.due to the heteroscedasticity of the idiosyncratic components, in the clustering procedure a correlationbased distance measure is used instead of the usual euclidean distance:where r ij is the pearson correlation coefficient between pairs of residual components e i and e j , i, j = 1, ..., p. the algorithmminimizes the loss function argmin b1,...,bp;\u00b51,...,\u00b5(m) iis the binary indicator variable that assigns each data point to a cluster band the centroid of a cluster is the average of the cluster members' residualsthe algorithm is iterative and does not have a closed form solution. it converges to the local minimum, and depends on the initialization -thus a repeated procedure with different initializations is used.to determine the number of clusters from the data, we develop an iterative cross-validation procedure over the number of clusters m , where m = 1, ..., p, as described in 3.4.3. finally, the blocks of the block-diagonal idiosyncratic covariance are given by the clusters estimated using this procedure.we label this clustering-shrinkage estimator based on k-means csk, the corresponding idiosyncratic estimates \u03c8 csk , and the entire covariance estimate \u03c3 csk .as a more flexible framework, we also develop an estimator based on hierarchical clustering. firstly, we propose a distance matrix based on the adaptive thresholding introduced by cai and liu, and specifically on the expression for the adaptive parameter from the formula, and its implications for the hard thresholding rule. we can observe that the estimation of the final idiosyncratic covariance \u03c8 \u03c4ij depends on the relation of the full orthogonal complement entries s ij and the associated thresholding parameter \u03c4 ij , for i, j = 1, ..., p. this means that if | s ij | < \u03c4 ij the final idiosyncratic component value is set to zero, otherwise if | s ij | \u2265 \u03c4 ij the value s ij remains unchanged. the relation in fact determines whether the two time series i and j are in the same cluster or not. therefore, we use it as a custom similarity measure within the hierarchical clustering framework. specifically, based on the relation, we define a distance matrix d = (d ij ) which specify the dissimilarity of the two time series i and j as:where \u03b8 ij = t -1 t t=1 (e it e jt -s ij ) 2 . in order to evaluate different possible clusterings within a hierarchical framework, a linkage function d(\u2022) is used, of which there are plenty: average linkage and weighted average linkage, median and centroid linkage, ward linkage, single and complete linkage. however, our focus is mainly on the methods less susceptible to noise, aligned with non-metric distance and forming the globular shape like average and weighted average. the considered linkage functions are given in detail in the appendix a, and their detailed descriptions can be found in respective papers-the proposed hierarchical clustering estimator may rely on any of these.finally, we use an agglomerative clustering algorithm to build the clusters, based on the proposed distance matrix d and considered linkage functions d(\u2022). we build the clustering tree and save each calculated distance (between points -time series, and/or objects -formed clusters) in the vector of distances [l 1 , l 2 , ..., lp] each of which is related to a certain number of clusters m . to determine the optimal cutoff distance l, the estimator uses the iterative cross-validation procedure as described in 3.4.3. finally, the blocks of the block-diagonal idiosyncratic covariance are given by the clusters estimated using this procedure.we label this clustering-shrinkage estimator based on the hierarchical clustering approach csh, the corresponding idiosyncratic estimates \u03c8 csh , and the entire covariance estimate \u03c3 csh .the hyperparameters of the two clustering approaches also need to be estimated -for k-means clustering this is directly the number of clusters, and for the hierarchical clustering algorithm this is the threshold at which the agglomerative tree is cut off. to obtain the values of these parameters we propose an h-fold cross-validation procedure, based on the residuals {e t } t\u2264t . the residual series are split into a train subset {e t } t\u2208ttrain and a test subset {e t } t\u2208ttest , where t train + t test = t . the procedure is repeated h times. in each fold h \u2208 h the following is performed:\u2022 build the full orthogonal complement covariance matrix: s train-h on train data and s test-h on test data.\u2022 apply the proposed clustering algorithms to the residual series to obtain groups. when using hierarchical clustering, search through the grid of distances l \u2208 [l 1 , l 2 , ..., lp] (where each distance is connected to specific number of clusters m), and when using k-means clustering, search through the grid of number of clusters m \u2208 [1, 2, ..., p].\u2022 the indicator matrix c is obtained simply as:\u2022 calculate the validation error err h \u03c6 , h = 1, ...h as the frobenius norm of the difference between the cluster based idiosyncratic covariance estimate from the train set \u03c8 c train-h and the full orthogonal complement from the test set s test-h :we consider the mean of h validation errors assigned to each hyperparameter (l in case of hierarchical and m in the case of k-means clustering):and iterate over a grid of possible values of the considered hyperparameters (in the case of hierarchical clustering we iterate over a grid \u03c6 = [l 1 , l 2 , ..., l p ] and in case of k-means clustering we iterate over a grid \u03c6 = [1, 2, ..., p]). the chosen distance criteria minimizes the error:for the hierarchical clustering the chosen hyperparameter is \u03c6 csh = l * and for the k-means clustering the chosen hyperparameter is \u03c6 csk = m * . the final block-diagonal idiosyncratic covariance estimates ( \u03c8 csk and \u03c8 csh ) are calculated on the entire estimation window using the selected hyperparameter. furthermore, we use the stability of the validation loss function to improve the algorithm and its speed. we use a stopping (convergence) criterion which stops the iteration loop when the change in the loss function is below a predefined threshold. specifically, we monitor the average loss over 3 iterations and stop when this average stagnates. for the general shape of the cross-validation errors for the two approaches, see figurewhich shows the cross-validation errors (blue line) through the iterations (over a grid of hyperparameter values) for the two considered estimators. it is evident that the validation errors exhibit an optimum which can be reached relatively quickly, without the need for traversing the entire hyperparameter space.  4 data and performance measures"
    },
    {
      "header": "Simulation data",
      "content": "to test the ability of the estimator to identify true patterns of a block-diagonal structure, we construct a simulation scenario which allows us to analyze the performance of the estimators with respect to a known population covariance matrix. in the simulations we construct the common covariance bcov(f )b \u2032 and the idiosyncratic covariance \u03c8 separately. the resulting covariance matrix is \u03c3 = bcov(f )b \u2032 + \u03c8.without loss of generality, we assume that the factors have identity covariance, leaving all the variability to the factor loadings matrix b. to simulate a random loadings matrix b we use the following procedure:1. generate random orthogonal loadings of unit length.2. scale loadings so the first factor has average loading equal to 1 (this is in line with factor models in finance where the market factor is often the strongest and the loadings of assets towards this factor are centered around 1).3. scale loadings by factor variances.4. calculate the common covariance as bb \u2032 .we define data generating processes based on two different shapes of the idiosyncratic covariance. firstly, a full block-diagonal structure has a predetermined number of blocks of equal size and all series belong to one of the blocks. secondly, a more difficult partial block-diagonal structure does not use a predefined number of blocks and has a variable block size, thus allowing a large number of \"blocks\" with only a single series. in both cases, each cluster group is tapered and the correlations within are diminishing further from the diagonal. to construct the idiosyncratic covariance we first build the correlation matrix, which is then transformed into the covariance matrix.the full block-diagonal idiosyncratic covariance is constructed in the following way:1. start from the identity correlation matrix r = i.2. for a predefined number of clusters m generate uniform random cluster sizes.3. for each cluster add off-diagonal correlations following the tapering structure:4. calculate the covariance matrix from the obtained correlation matrix and the idiosyncratic variances.the partial block-diagonal covariance is constructed using graphs:1. define a probability that a node (asset) is connected.2. iterate over all assets -for each asset, determine whether it will be connected (given the probability above) -if yes, connect it to any one randomly selected asset (selected uniformly across the remaining p -1 assets).3. this procedure will build a graph with a number of connected components -each component will correspond to a cluster, and ultimately, a block in the idiosyncratic covariance. note that assets which are not connected remain as single-asset clusters.4. for each cluster larger than 1 (i.e. other than single-asset clusters) add off-diagonal correlations following the tapering structure as described in (29).5. calculate the covariance matrix from the obtained correlation matrix and the idiosyncratic variances.we consider a collection of daily us stock returns from january 1995 to january 2017. the database consists of a large number of stocks, and at each time step we select the top p stocks by market capitalization at the time defined by current date and considering only stocks which satisfy the following conditions:1. all marketcap and return data is available for the full training and test periods.2. there is at least 1 day of non-zero returns in test period and in train period.3. all the stocks have sic sector identification.to determine the group membership in the csi estimator, we collect the standard industrial classification codes (sic) sector codes for the selected stocks.a most commonly used performance measure for determining the quality of matrix estimation is the frobenius norm of the error:however, it is generally a rough way to measure the covariance estimation quality. since we focus on identifying the sparse correlation patterns in the idiosyncratic part, we can also focus on measuring how well these are identified by different estimators. in the simulation scenario, the population idiosyncratic covariance is known and thus we are able to measure the accuracy of identifying the true non-zero and zero elements in the population idiosyncratic covariance. we denote the classes of each element of the population idiosyncratic covariance with 0 if the element is zero and 1 if the element is non-zero. we use several, most common classification performance measures to evaluate the ability of the proposed estimators to identify the true sparsity patterns: positive rate (tp), true negative rate (tn), accuracy (acc) and f1 score.\u2022 accuracy is simply the ratio of correctly identified elements to the total number of off-diagonal elements in the idiosyncratic covariance.\u2022 tp (recall) and tn are the ratios of the correctly classified positive (negative) elements to the total number of positive (negative) elements in the population matrix.\u2022 f1 is defined as the harmonic mean of recall and precision, where recall equals tp and precision is the ratio of classified true positives to the number of all predicted positives.moreover, we also consider clustering performance measures, since the population idiosyncratic covariance is considered to be block-diagonal. rand's index measures the extent to which the obtained grouping corresponds to the reference grouping (for instance that of the population covariance). it is calculated as the accuracy of classification at the level of pairs of series, and is defined as follows:where n 2 represents the total number of possible pairs. we use the ri both in simulation data (with respect to the population idiosyncratic covariance) and historical data, where we use it to measure the similarity of different methods to the industry classification.in addition to the results reported for the mentioned performance measures, which are averaged over a large number of simulations, for the simulation data we also consider the number of simulations in which the proposed csh and csk estimators outperform the benchmarks. let n + denote the number of outcomes in which the considered estimator outperforms a benchmark for a given performance measure, in a total number of n simulations. for the proportion n + /n we apply a non-parametric one sided paired sign test with following hypotheses: h 0 : the probability of the estimator outperforming the given benchmark is 0.5.h 1 : the probability is greater than 0.5.under the null hypothesis, n + follows a binomial distribution b(n, 0.5), which is directly used to calculate the corresponding p-value. we apply the test to each reported performance measure, in order to confirm whether the proposed approach achieves statistically significant improvements over the benchmark methods.in addition to the performance measures defined above, we also consider a portfolio optimization scenario. we use minimum variance portfolios (in this context variance quantifies risk), since they highly depend on the quality of the estimated covariance -the noise in the estimator indirectly transmits to the portfolio weights (variance minimizers are estimation-error maximizers). the vector of portfolio weights w = [w 1 , ..., w p ] \u2032 contains the percentage of the total capital allocated to each of the p assets. when asset returns y are arithmetic returns, the portfolio return is simply stated as r p = w \u2032 y . then the portfolio variance is the variance of the linear combination \u03c3 2 p = w \u2032 \u03c3w, which forms the basis for portfolio optimization in the mean-variance sense. the optimal portfolio weights for the minimum variance portfolio are then calculated by solving the following optimization problem:where \u03c3 is the covariance estimate of the asset returns and the term w \u2032 \u03c3w is the portfolio variance.to evaluate different estimators we first obtain the optimal portfolio w on a given estimation window using a covariance estimate \u03c3, then we calculate the out-of-sample portfolio risk, which we quantify as volatility (standard deviation of the returns):when using simulation data, the population covariance \u03c3 is known. portfolios are optimized using the estimates obtained from the generated time series, and \"out-of-sample\" portfolio risk is calculated by using the known population covariance \u03c3 in the expression. for historical data, the population covariance is unknown thus the sample estimates \u03c3 s from a future holding period are used in expression-this corresponds to a backtesting approach where the optimized portfolios are held on a given future time period, and the realized risk of these portfolios is calculated. the daily portfolio volatility is annualized by multiplying with \u221a 252."
    },
    {
      "header": "Results",
      "content": "for the simulation, we fix the number of factors to k = 5, and simulate time series of length t = 250 using the student's t-distribution with 5 degrees of freedom and zero mean, in order to replicate the heavy tailed property of asset returns. the simulations are repeated a total of 250 times. for the full block-diagonal idiosyncratic structure we use m = 10 clusters, and in the partial block-diagonal procedure the number of clusters is random and is a consequence of the random connections. for the correlation tapering within the clusters, we use const = 0.3, base = 0.9, exponent = 0.1 which result in similar correlation distributions as observed in historical data. factor variances are set to (0.25/([1, 2, ..., k] 0.5 ) 2 . firstly, in order to justify the choice of the linkage function in the hierarchical clustering method we evaluate five linkage functions (average, weighted average, ward, centroid, medoid) on the partial block-diagonal simulation case with p = 1000 series. the results are shown in table-the f1, ri, frobenius norm of the error and the portfolio risk are shown with respect to the known population covariance. the results show that the method based on the average linkage function outperforms in all of the considered aspects, thus in the rest of the paper we focus on the csh estimator based on average linkage. we simulate the data with higher-dimensional p = 1000 series and lower-dimensional p = 300 series, using the same simulation parameters, in order to test the behavior of the estimators for different dimensionalities. tablereport the results for both considered dimensionalities and the two idiosyncratic covariance cases: partial block-diagonal and full block-diagonal. for all the measures, we report the p-values results of the one sided pair sign tests, based on the number of experiments in which the csh (for the partial block-diagonal case) and the csk (for the full block-diagonal case) estimators outperformed all the other methods.for the partial block-diagonal case, the csh estimator is expected to outperform, which is confirmed in the results, and statistically significant (for higher-dimensional series in all cases and for lower-dimensional in most of the cases) -this is evidently due to the fact that the hierarchical clustering approach can better accommodate single assets as clusters and generally the different cluster sizes, while the k-means approach is well-suited for compact, uniform-sized clusters. the true positive rate (tp) may be higher in some cases for the csk estimator as it tends to merge multiple small and single-asset clusters to one of a few larger clusters. furthermore, as the csh estimator captures small and even single-asset clusters, missing some clusters has a higher impact on the true positive rate. in the full block-diagonal idiosyncratic covariance case, the csk estimator outperforms all the benchmark methods in all aspects, for both considered dimensionalities. the classification measures are close or equal to 100% of accuracy. moreover, even though the csk estimator is expected to outperform the csh estimator on the full block-diagonal case, the performance of the hierarchical approach remains comparatively high and the differences are not as large. the first reason for this is that the full block-diagonal case is much easier for both clustering based estimators. and the second reason lies in the fact that csh is more flexible and able to deal with both simulation cases very well, while the csk struggles in the partial block-diagonal case.it is important to note that for both the partial and full block-diagonal shapes, the lowerdimensional simulations for p = 300 represent a drastically easier task for the k-means approach (csk estimator). this is due to the fact that the clusters in that case are naturally smaller, and the tapering effect of the off-diagonal correlations in the simulated population matrices is much weaker for smaller clusters (the correlations within those clusters are higher on average). in both considered dimensionalities, we observe that the clustering based estimators consistently outperform the thresholding based estimators -however, in lower-dimensional case, the improvement is not as drastic and not as pervasive as in the higher-dimensional case. evidently, the hierarchical approach benefits from the high dimensionality.table: table shows all the performance measures (rand index, classification measures, frobenius norm) and out-of-sample portfolio volatility for the proposed estimators (csh, csk ) and considered benchmark estimators. we reported two simulation cases partial block-diagonal case and full blockdiagonal case for higher (p = 1000) and lower (p = 300) dimension. the p-values of the paired sign test comparing the csh (for partial block-diagonal case) and csk (for full block-diagonal case) estimator with all other methods are given in parentheses (below each method compared to the csh or csk estimator).although we did not observe drastic structural differences, or differences in order of the estimators' performances, there are some differences in the behavior of the estimators when increasing the dimension. in figurewe show the average out-of-sample portfolio risks over all simulations for the partial block-diagonal case and the full block-diagonal case over different dimensions p, starting from p = 250 to p = 1000 with a step of 50. evidently, the clustering based estimators benefit widely from increased dimensionality in both simulation scenarios, and consistently outperform the thresholding based estimators over all dimensions. moreover, the clustering based estimators also show a great level of stability as clustering procedures are much more robust in capturing the idiosyncratic groupings, while the thresholding based estimators only focus on high pairwise covariances, and might miss out on the other members of the groups -these properties of the estimators are also the reason behind the f1 and ri being much higher for clustering based methods in tablefor both shown dimensions.   to demonstrate visually how different estimators work, we show how each estimator forms the idiosyncratic covariances for both simulation cases in figure. in addition to the two clustering based estimators we also show the scad estimator, since it performs the best out of the thresholding based approaches. the full block-diagonal case seems to be captured almost perfectly by both clustering based methods, while the scad methods evidently misses out on some elements with smaller correlations (due to tapering further away from the diagonal). for the partial block-diagonal case larger clusters tends to be broken into smaller ones by the csk approach, and some small and single-asset clusters are combined into larger ones. the scad approach identifies the smaller ones but mostly misses out on the larger ones. the csh estimator is shown to capture the clusters relatively well.we test the estimators on the historical data using a portfolio optimization approach -at each time step, the portfolios are constructed using the covariance estimated during the past 1 year of daily returns (a total of 252 data points) using the considered estimators. the portfolio is held for the next month (22 days) and the portfolio volatility is calculated on this out-of-sample future holding period. this approach assumes that the covariances estimated in the past 1-year window continue to hold on the future 1-month period, and the future returns are considered as realizations of this process. the total number of iterations is 264. in historical data we are letting the algorithm to find the number of common factors k. to estimate the number of factors within each time window we use the bai-ng ic1 method. figureshows the evolution of the estimated number of common factors through time -the number of factors ranges from 2 to 10 with the average of 4.33 and the median and mode being the same and equal to 4. the results show that the clustering based methods generally outperform the thresholding based estimators, with the exception of the soft thresholding estimator for the lower dimensional case. the clustering based estimators manage to find clusters which are not so similar to the industry classifications, as suggested by the ri results -yet these alternative groupings seem to perform similarly or even better than the industry classifications. this result affirms the hypothesis that the industry groupings may not be optimal, depending on the application. while still performing better than thresholding based estimators in lower-dimensional cases, we see that clustering based estimators benefit drastically from the increased dimensionality. nevertheless, the results also suggest that the industry classification is in fact a valuable contributor to the performance -csi shows excellent performance. however, the industry classification data may not always be available, depending on the asset universe or different markets one might consider. on the other hand, the proposed clustering based approach only requires historical return data. finally, we also inspect the shapes of the identified idiosyncratic covariances for a specific time window in the historical data. figureshows the idiosyncratic covariance given different methods: csi, csk and csh. it is important to note that for each method, the assets were sorted according to the corresponding clustering results (so that the assets in the same clusters are placed next to each other). the differences are quite visible -the csi features relatively big blocks of varying sizes, and different values of off-diagonal elements, while the csk finds more smaller and compact groups. the estimated idiosyncratic covariance using the csh estimator differs mostly from the other methods. it allows small one-member clusters but does not omit the relevant information (bigger clusters are also observed). due to this flexibility, the average number of clusters is much larger than the number given by the csk estimator, and the matrix is generally more sparse."
    },
    {
      "header": "Conclusion",
      "content": "we consider the problem of estimating the covariance matrix of high-dimensional financial return time series, given an underlying latent factor model. the latent factor model allows for a specific structure of the covariance matrix -a low-rank component due to common factors and a fullrank sparse idiosyncratic component. in this paper we specifically focus on the estimation of the idiosyncratic component under the assumption that the considered financial assets form groups, even after accounting for common factors, which has recently been documented in the literature. this leads to a block-diagonal structure of the idiosyncratic covariance. we follow a two step estimation procedure where the first step consists of estimating the common component, and in the second step the residual component is used to obtain a sparse estimate of the idiosyncratic covariance. we formulate a unified approach to estimating the block-diagonal idiosyncratic covariance and consider several methods to obtain the unknown block structure (clusters). we also propose an iterative crossvalidation procedure in the context of the squared error given the assumed latent factor model, and test the proposed approach on simulation data and historical return data.the simulation results show that the proposed clustering based estimators successfully recognize the true sparse idiosyncratic covariance patterns, while decreasing the optimized portfolio volatility. moreover, they show other desirable properties: both clustering approaches benefit from increased dimensionality and demonstrate stable results for different numbers of simulated series. the hierarchical approach implemented in the csh estimator shows great versatility, since it is able to capture the difficult patterns given by the partial block-diagonal idiosyncratic case, while retaining performance for the full-block diagonal case. tests on historical data confirmed the superiority of the clustering based estimators with respect to the thresholding based estimators. a striking finding is that the groups identified by the proposed estimators seem to differ to a great extent from the industry classification, however the portfolio performance of the proposed estimators is on par with or even better than the industry based estimator csi.the results evidently affirm the basic research hypothesis of the paper -that estimating the sparse idiosyncratic covariance as a block-diagonal matrix improves upon the thresholding based approach. allowing the assets to form entire clusters dramatically enriches the space of idiosyncratic covariance estimates, and ultimately results in a more realistic model of the asset return dependence. the proposed approach will hopefully make its way to applications in risk modeling of high-dimensional return time series in broad asset universes, and especially those where an industry or similar classification is not known a priori. we also hope to inspire new research, especially in the area of modeling the hierarchical group structures and their effect on the idiosyncratic covariance in approximate factor models.denote with g the cluster group set in the hierarchy which contains the observations. we describe five main types in detail:\u2022 average linkageis average inter-cluster distance calculated as distance between each pair of the observations in each cluster.d( e i , e j ) (34)\u2022 centroid linkageis the distance between the centroids of the two clusters.\u2022 median linkageis euclidean distance between weighted centroids of the two clusters.where \u1ebd1 and \u1ebd2 are weighted centroids of clusters g 1 and g 2 . if the cluster g 1 is created by combining two clusters g 1a and g 1b , then \u1ebd1 = 1 2 (\u1ebd 1a + \u1ebd1b ).\u2022 ward distanceis defined as the within-cluster sum of the squares of the distances between all objects in the cluster and the centroid of the cluster.\u2022 weighted average linkageis defined recursively. if cluster g 1 is created by combining clusters g 1a and g 1b than the distance between the cluster g 1 and g 2 is defined as average of the distance between g 1a and g 2 and the distance between g 1b and g 2 .we outline established ledoit and wolf procedure for the optimal shrinkage intensity. the optimal shrinkage intensity \u03b1 m , should minimize the expected value of the quadratic loss functionwhere s cm is the unknown population covariance. the optimal \u03b1 m estimate from the ledoit and wolf procedurefor shrinkage estimator, is given aswhereand t m is the block size.for the simplicity we will drop prefix m, but all the following parts are calculated per each block. the constant term \u03c0 is consistent estimator of asymptotic variances of the sample block matrix entries s c scaled by \u221a t (size of the block) defined as:n i=j e it -\u0113i )(e jt -\u0113j ) -s ij 2where \u0113i is the sample average of the returns of stock i from the cluster block c. term \u03c1 is consistent estimator of sum of asymptotic covariances of the shrinkage target entries with the block sample covariance entries scaled by \u221a t (size of the block) defined as:where\u03b7 jj,ij = 1 t m (e jt -\u0113j ) 2 -s jj \u00d7 (e it -\u0113i )(e jt -\u0113j ) -s ij (44and \u03b3 is a consistent estimator of misspecification of the (population) shrinkage target defined as:"
    }
  ]
}
