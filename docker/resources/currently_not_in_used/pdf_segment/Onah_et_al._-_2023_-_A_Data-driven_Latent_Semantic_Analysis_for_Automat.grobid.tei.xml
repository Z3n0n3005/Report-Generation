<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Data-driven Latent Seman0c Analysis for Automa0c Text Summariza0on using LDA Topic Modelling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">F O</forename><surname>Onah</surname></persName>
							<email>d.onah@ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informa-on Studies University College London London</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elaine</forename><forename type="middle">L L</forename><surname>Pang</surname></persName>
							<email>elaine.pang@brunel.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Academic Skills Development</orgName>
								<orgName type="institution">Brunel University</orgName>
								<address>
									<settlement>London London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mahmoud</forename><surname>El-Haj</surname></persName>
							<email>m.el-haj@lancaster.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">School of Compu-ng and Communica-ons Lancaster University Lancaster</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Data-driven Latent Seman0c Analysis for Automa0c Text Summariza0on using LDA Topic Modelling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E6F8EBE6B33E830031F2B9A21C40A8F1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-02T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Summariza8on</term>
					<term>extrac8ve</term>
					<term>abstrac8ve</term>
					<term>Latent Dirichlet Alloca8on</term>
					<term>topic modelling</term>
					<term>visualisa8on</term>
					<term>ROUGE</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the advent and popularity of big data mining and huge text analysis in modern 8mes, automated text summariza8on became prominent for extrac8ng and retrieving important informa8on from documents. This research inves8gates aspects of automa8c text summariza8on from the perspec8ves of single and mul8ple documents. Summariza8on is a task of condensing huge text ar8cles into short, summarized versions. The text is reduced in size for summariza8on purpose but preserving key vital informa8on and retaining the meaning of the original document. This study presents the Latent Dirichlet Alloca8on (LDA) approach used to perform topic modelling from summarised medical science journal ar8cles with topics related to genes and diseases. In this study, PyLDAvis webbased interac8ve visualiza8on tool was used to visualise the selected topics. The visualisa8on provides an overarching view of the main topics while allowing and aHribu8ng deep meaning to the prevalence individual topic. This study presents a novel approach to summariza8on of single and mul8ple documents. The results suggest the terms ranked purely by considering their probability of the topic prevalence within the processed document using extrac8ve summariza8on technique. PyLDAvis visualiza8on describes the flexibility of exploring the terms of the topics' associa8on to the fiHed LDA model. The topic modelling result shows prevalence within topics 1 and 2. This associa8on reveals that there is similarity between the terms in topic 1 and 2 in this study. The efficacy of the LDA and the extrac8ve summariza8on methods were measured using Latent Seman8c Analysis (LSA) and Recall-Oriented Understudy for Gis8ng Evalua8on (ROUGE) metrics to evaluate the reliability and validity of the model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Topic modelling has been performed on several types of documents in the past. However, this study presents a novel approach to topic modelling by performing extracDve summarizaDon on over 100 arDcles related to genes and associated diseases and feeding the summary as an input argument a Latent Dirichlet AllocaDon (LDA) model in order to perform the topic modelling. The idea here is to idenDfy the commonaliDes between arDcles of the same genre describing a specific topic of interest in the research. The study is addressing journal arDcles retrieved from PubMed Central 1 h+ps://www.ncbi.nlm.nih.gov/pmc/ (PMC 1 ) database discussing about genes and their associated diseases. What would you do if you were handed a pile of papersreceipts, emails, travel iDneraries, meeDng minutes-and asked to summarize their contents? One strategy might be to read through each of the documents, highlighDng the terms or phrases most relevant to each, and then sort them all into piles. If one pile started geTng too big, you might split it into two smaller piles. Once you had gone through all the documents and grouped them, you could examine each pile more closely. Perhaps you would use the main phrases or words from each pile to write up the summaries and give each a unique name-the topic of the pile. This is, in fact, a task pracDced in many disciplines, from medicine to law, from computer science to engineering and so on. At its core, this sorDng task relies on our ability to compare two documents and determine their similarity. Documents that are similar to each other are grouped together and the resulDng groups broadly describe the overall themes, topics, and paVerns inside the corpus. With so many documents being extracted from social media, review comments from online plaWorms and microblogs as TwiVer, a huge amount of natural language data is being mined and are available to be analysed <ref type="bibr" target="#b0">[1]</ref>. Certainly, it is reasonable to translate summarized documents accurately. An example, arDcles extracted in a different language from English to be translated to make sense similar to the original language of which the arDcle was wriVen <ref type="bibr" target="#b1">[2]</ref>.</p><p>AutomaDc text summarizaDon is the process of performing specific NLP task by producing a concise summary of documents (single or mulDple) without any manual support while preserving the meaning or important points of the original document <ref type="bibr" target="#b2">[3]</ref>. In this study, we try to answer the following research quesDons:</p><p>• How automated text summarizaDon techniques were used in an extracDve summary of arDcles?</p><p>• How topic modelling models were used in producing emerging terms that are related to mulDple and different journal arDcles? • Are the search terms used for the text mining of arDcles from the database predominant in the emerging terms that were extracted from the processed text? The experimental results show that the proposed model achieves good performance in terms of the document summary and the topic modelling informaDon retrieved from the full document. This paper is presented as follows. SecDon 2, covers related study on summarizaDon. SecDon 3, conceptualise the methods and describes the techniques applied in the study. SecDon 4 describes topic modelling, SecDon 5, presents a descripDon of the model pipeline. SecDon 6, presents the results and findings. The limitaDon of the study is presented in secDon 7 and finally, the discussion and conclusion of the study are presented in secDons 8 and 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The amount of text data being produced worldwide is enormous and growing rapidly. Unless these text data are extracted and make meaning, then the most important and relevant informaDon would be lost. Text summarizaDon is a well-known task in natural language understanding and processing. SummarizaDon is described as the process of presenDng huge data informaDon in a concise manner while focusing on the most useful secDons of the data whilst preserving the original meaning <ref type="bibr" target="#b3">[4]</ref>. The most important element of text summarizaDon is to produce a clear and concise summary taken from the large datasets that would make sense to the reader and direct to the main points <ref type="bibr" target="#b4">[5]</ref>. There is a need for automaDon of these increasingly available web text data for informaDon retrieval and sustainability. In this modern era of big data, text mining has been retrieved from various sources, website, databases, journals and conference arDcles in related studies. The voluminous text data need to be collected and summarised in order to retrieve useful informaDon concerning the main content of the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Summariza-on</head><p>SummarizaDon is a technique in NLP that is used for condensing or summarising huge texts into smaller versions taking care not to omit the main relevant informaDon contained in the document <ref type="bibr" target="#b5">[6]</ref>. This helps in reducing the size of the original document either single or mulDple while preserving key elements and meaning of the content <ref type="bibr" target="#b6">[7]</ref>. This is the main significant of automaDc summarizaDon, by presenDng the documents in a more meaningful manner. Manual summarizaDon is tedious, expensive and laborious to under-take <ref type="bibr" target="#b7">[8]</ref>. There is need for automated summarizaDon which is gaining popularity among researchers. There are so many important models for performing automaDc text summarizaDon in various NLP tasks such as classificaDon, automaDc quesDon and answering, computaDonal journalism, financial summarizaDon, news summarizaDon and foreign language summary translaDon. One of the key factors of the document summary is that it can be integrated into these NLP applicaDons to reduce the size of the document for processing while possibly retaining the original informaDon contained in the document <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. There are two different approaches to automaDc summarizaDon; these are extracDon and abstracDon.</p><p>1) Extrac-ve approach: ExtracDve summarizaDon approach considers the top N sentences based on their score rankings for the summary generaDon <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. This paper focuses on extracDve text summarizaDon. The study focuses on direct object extracDon from the original document without any modificaDon of the content. ExtracDve summarizaDon approach takes object as input and generate the summary based on the probability vector <ref type="bibr" target="#b3">[4]</ref>. Word frequencies are considered as one of the input factors in the sentence score rankings which represent the probability of the sentence to be included in the summary. In order to generate the final summary, the best sentence scores are selected based on the maximum number of words in the sentence and the number of sentences that met the specified threshold provided <ref type="bibr" target="#b12">[13]</ref>. We will briefly describe the abstracDve approach of summarizaDon and explain why we decided to use extracDve approach in this study.</p><p>2) Abstrac-ve approach:</p><p>In abstracDve text summarizaDon technique, this follows the convenDon of unsupervised approach where machine learning paradigms such as deep learning plays a big role in generaDng the document summary <ref type="bibr" target="#b13">[14]</ref>. This approach considers a boVom-up summary for which some of the sentences might not be part of the original document <ref type="bibr" target="#b14">[15]</ref>. However, in some cases, the vocabulary of the documents might be the same as the original document <ref type="bibr" target="#b15">[16]</ref>. Designing an abstracDve model for summarizaDon is very problemaDc and challenging because it involves a more complex language modelling <ref type="bibr" target="#b16">[17]</ref>.</p><p>In this study, we decided to use extracDve approach for arDcle summarizaDon, because we wanted all parts of the sentences that will be summarised to be from the original document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Topic Modelling</head><p>Topic modelling is the process of labelling and describing documents into topics. This is an unsupervised machine learning technique for abstracDng topics from collecDons of documents <ref type="bibr" target="#b17">[18]</ref>. Topic modelling approach is based on an inducDve modelling used to abstract core themes from a weighted graphical representaDon of documents obtained during the processing stages. In order to apply topic models in NLP applicaDon, there is need for extrapolaDon of topics from unstructured datasets. In this study, Scitkit -Learn and Gensim were used to extract the topics from the models using 0 gensim.models.ldamodel.LdaModel 0 , which takes in as input argument the text corpus, number of topics to be extracted and id2word that contains the dicDonary terms for the preprocessed document <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Latent Dirichlet Alloca-on</head><p>Latent Dirichlet AllocaDon (LDA) is a technique applied in topic modelling introduced by <ref type="bibr" target="#b19">[20]</ref>. This is a topic discovery technique used to generate topics based on the probability that each given term might occur within the document. The document can be in the form of mixture of topics that might not necessarily be disDnct and words may appear in mulDple topics <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>. In this approach, presented with words or token from mulDple documents from which a probability topics model is constructed, we observed word distribuDons for each mixture of topics in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head><p>The summarizaDon model was designed to scrap text data from PubMed journal database using genes and diseases keywords search <ref type="bibr" target="#b21">[22]</ref>. A web-scraping model that was used to retrieved the arDcles for this research was able to scrape about 100 papers at a Dme from the PubMed Central (PMC) repository. We could have extracted more arDcles. However, we wanted to use this sample as the iniDal based study. The model applied some NLP techniques for the iniDal preprocessing of the data for extracDve summarizaDon. The proposed model in this study is scalable and generalizable for producing arbitrarily size summaries by spliTng the documents into resemble content. The study applied sentence scoring on the clean document to extract text that fell within the threshold of high frequency score used in the model. During the summarizaDon process, we calculated word frequency and the high sentence scores that was used to summarise the arDcles. We created vectors to store the sentences. This allows us to fetch summary for 100 elements for the consDtuent words in a sentence. Finally, we took the mean of those vectors to consolidate the vector for the sentence. The next phase is to perform a cosine similarity scores of the sentences using a matrix dimensions of n * n, where n is the number of sentences in the document. The cosine similarity was applied to perform the similarity between a pair of sentences. We then extract the top N sentences based on their ranking for the summary generaDon that was then fed into the LDA topic modelling.</p><p>The study was designed to apply a generalised concept of LDA topic modelling technique to create a dicDonary of terms that was fed from the summarised arDcles. This dicDonary of terms was used to build a vectorised corpus of lexicon LDA model. One of the key approaches that was used in the experiment was the 'pyLDAvis.gensim.prepare' method which takes as an argument our LDA model, the corpus and the derived lexicon which contains the dicDonary terms for the study <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Another method that was in the study was the 'gensim.models.ldamodel.LdaModel 0 , which takes the summary corpus as an input argument, the number of topics to be extracted and the 'id2word 0 that contains our dicDonary terms for the journal arDcles. This method allows us to project the topics by calling the method that help in visualising the interacDve topic modelling shown in Figures <ref type="figure" target="#fig_5">7</ref> and<ref type="figure" target="#fig_6">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Descrip-on</head><p>Most of the processing of the text was performed with the python Natural Language Toolkit (NLTK) <ref type="bibr" target="#b24">[25]</ref>, including using the NLTK Tokenizer to tokenize the text. The overarching research model was developed to retrieve specific informaDon from huge published journals using the topic modelling approach of NLP. In this study, we used mulDple journal arDcles related to diseases and genes with sequence of paragraphs. The task is to generate the summary at most predominant sentence level. ExtracDve summarizaDon approach applied in the study produced naturally grammaDcal summaries without much linguisDc connotaDon or analysis. Since extracDve summarizaDon uses a supervised technique, the sentence selecDon process involves scoring each sentence in the original cleaned document. In this case, a label is produced to indicate whether a sentence met the condiDons which are the chosen length of the sentence or the summary threshold indicated in the model. It is only when these predefined condiDons are met before a sentence could be considered to be included in the final summary. The supervised learning method allows for maximizaDon of the likelihood of sentence consideraDon from the input document. This approach could also be generalised on other arDcles such as media, blogs, news and so on and will produce the same outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1)</head><p>Sentence scoring method: : In this study, a scoring funcDon is introduced to generate the sentence score dicDonary which hold the value assigned to each sentence <ref type="bibr" target="#b25">[26]</ref>. This denotes the probability that the sentence will be selected and included in the summary (see Figure <ref type="figure">1</ref>). The summary length is fixed, therefore, the top N sentences with the highest score rankings are chosen for the summary. In this study, the quality of the document summary largely depends on the chosen sentences and this would reveal the relevance of the informaDon retrieved from within the full document. The process of scoring the sentence is represented in equaDons 1 and 2. If the sentence is not in the sentence score dicDonary keys during the processing, the words in the word frequencies dicDonary is added to the sentence scores (see equaDon 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentscores[S] = Wordfreq[W]</head><p>(1)</p><p>During the sentence model processing interval, the length of the sentence is either increased or reduced by certain values within the sentence scores dicDonary. Therefore, new sentences are added into the sentence dicDonary scores. The sentence model would check whether the new sentences are in the sentence dicDonary. If the sentence exists in the sentence dicDonary, then the model will proceed accordingly. But if the process sentence is not in the sentence scores dicDonary keys, then the word in the word frequencies dicDonary is added to the sentence in the sentence scores dicDonary (see equaDon 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentscores[S]+ = Wordfreq[W]</head><p>(2)</p><p>2) Word frequency:: DicDonary of word frequency corpus was generated within the model. The word frequencies were selected automaDcally based on the prevalence or occurrence of the words in the corpus dicDonary created in the model (see Figure <ref type="figure" target="#fig_0">2</ref>). The length of the sentence selected for the word frequencies was less than 30. Sentences with less than Fig. <ref type="figure">1</ref>. Tokenize sentence score for the arJcle summary 30 (&lt; 30) words were selected. These maximum weighted frequency (Freqmax) of each word were calculated by using the product of the word frequencies (Wfreq) and the values (V ). These are then added to final summary (see equaDon 3).</p><formula xml:id="formula_0">Freqmax = Max(Wfreq * V )<label>(3)</label></formula><p>The next equaDon allows us to calculate the maximum word in the word frequencies (see equaDon 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word</head><p>(4) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Research Pipeline</head><p>The pipeline model for the research follows a sequenDal approach of processes that could allow the smooth and efficient informaDon retrieval. The pipeline in Figure <ref type="figure" target="#fig_1">3</ref> was used to answer the research quesDons in this study.</p><p>1) Data Collec-on: The dataset was scraped from the web. About 100 papers were extracted from PubMed Central (PMC) database ("https : //www.ncbi.nlm.nih.gov/pmc/ 00 ) using a search key combinaDon of 'gene' and 'disease'. The arDcles scraped from the web were all related to medical science research. Papers related to diseases and the mutated genes causaDon were extracted for this study <ref type="bibr" target="#b21">[22]</ref>. These papers were extracted with HTML tags that are required to be preprocessed, cleaned and summarized for the topic modelling approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2)</head><p>Pre-processing &amp; Feature Extrac-on: The web-based dataset scraped from PubMed journal was in raw state and unstructured which consist of HTML tags, special characters, symbols and numbers that had to be processed and cleaned. The preprocessing involved converDng the dataset into text documents using NLP packages such as BeauDfulSoup, regular expression, lxml, tokenisaDon and using NLTK library. In the feature extracDon process, we parse the web arDcles source code in order to extract the textual material needed for the final summary. As the arDcles were parsed through the source code, the text for extracDon are between the paragraphs' tags &lt; p &gt; text &lt; /p &gt;. During the process of formaTng the clean arDcles, we performed extra filtering of special characters from the processed text in order to find and replace these symbols automaDcally. Finally, these extracted paragraphs text are combined to form a single string to store the clean web content for further topic model processing (see Figure <ref type="figure" target="#fig_2">4</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3)</head><p>Stopwords: We further removed a list of stop-words from the propocessed arDcles. Words such as pronounce that are not necessary or essenDal for the final summary (see Figure <ref type="figure">5</ref> for the list of stop words).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Topic Modelling &amp; Visualiza-on:</head><p>This study was able to reveal prevalence of terms that emerged within the documents and show their relevance by how the projecDon of the topic modelling circle and the size of a word in the result visualisaDon. The result was visualised using PyLDAvis which is a web-based interacDve visualizaDon package that allows the display of the topics that were idenDfied using the LDA approach <ref type="bibr" target="#b26">[27]</ref>. PyLDAvis was used for extracDng informaDon from the fiVed LDA topic models to design a web-based interacDve visualizaDon. The main method that was applied in Fig. <ref type="figure">5</ref>. List of stop-words removed from the arJcle this study was 'pyLDAvis.gensim.prepare' which takes as an argument topic models from LDA, the vectorized text corpus, and the derived lexicon which contains the dicDonary terms from the study <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Each idenDfied topic is encoded in the circles of the PyLDAvis and the bigger the circle the more projecDon or prevalence is the topic (as seen in Figures <ref type="figure" target="#fig_5">7</ref> and<ref type="figure" target="#fig_6">8</ref>). The higher the number of common words among sentences indicates that the sentences are semanDcally related.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MODEL</head><p>A. Defining seman-c significance we define the semanDc significance of term t to the topic n given the parameter weight of the (λ) where(0 ≤ λ ≤ 1) <ref type="bibr" target="#b23">[24]</ref>. Let pt denotes the minimal probability of the term t in the lexical corpus. Let nt denote the probability of term t element of 1, ..., N for n element of 1,..., K, where N denotes the frequency of terms in the vocabulary (see equaDon <ref type="bibr" target="#b0">1)</ref> where (λ) is the weight given to probability of the terms t in topic n (equaDon 1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Defining Saliency Term</head><p>In this study we define saliency term as given a word 'w 0 , we compute its minimal probability P(TM/w). where TM is the topic model. The possibility that the emerge word w was generated from the LDA topic model (TM).</p><p>We also compute the marginal probability P(TM): -with the possibility that any word w 0 randomly selected was generated by TM. We define the uniqueness of each idenDfied word 0 w 0 as the divergence occurrence between P(TM/w) and P(TM) <ref type="bibr" target="#b27">[28]</ref>: we were able to compute 5 topics (t) and 10 passes which were selected from the Latent Dirichlet AllocaDon (LDA) topic modelling (see equaDon 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>U(6)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=5</head><p>The uniqueness of each term is described as how significance and semanDcally associated they are to the topics. For example, a term could be semanDcally associated to more than one topic. The frequency and populaDon of terms are denoted by the size of the topic circles and also the inter-topic distance denote how closely related the topics are. We noDce a few words that are expressed in several topics, but observing this word w reveals liVle informaDon about the mixture or semanDc associaDon of the topics. In some cases, this word might be scored very low in the computaDon of it's uniqueness. In order to compute the saliency, we used the following model equaDon 7:</p><formula xml:id="formula_1">Sw = P(w) * Uw (7)</formula><p>As illustrated in Figure <ref type="figure" target="#fig_6">8</ref>, adjusDng the lambda metric can aid in the significant classificaDon and reducing the complexity of the topics. This helps to remove ambiguity of the terms associaDon by making term distribuDon clearly. Looking at the figure, we observed that given equal frequency of words, the list of the most common, relevant or disDncDve terms (e.g. gene, disease, expression, associate ) are prevalence in the visualised graph-plot distribuDon. The saliency measures the distribuDon of the speeds and idenDficaDon of topic associaDon and composiDon (e.g. prevalence topic 1 terms such as genes,disease etc. These terms are all semanDcally associated to topics 2 and 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. LATENT SEMANTIC ANALYSIS</head><p>Latent SemanDc Analysis (LSA) is a robust Algebraic and StaDsDcal method which extracts hidden semanDc structures of words and sentences. LSA is used to extract features that cannot be directly menDoned within the dataset <ref type="bibr" target="#b28">[29]</ref>. These features are essenDal to data, but are not original features of the dataset. It is an unsupervised approach along with the usage of Natural Language Processing (NLP). It is an efficient technique in order to abstract out the hidden context of the document <ref type="bibr" target="#b29">[30]</ref>. We performed a mini summary from the original summary from the study using latent semanDc analysis (LSA) for text summarizaDon. The mini summary was done from the summary of the original clean 100 arDcles extracted from PubMed (https : //pubmed.ncbi.nlm.nih.gov/) database. This summary is then fiVed into the ROUGE metric system to measure the efficacy of the model. Results from the LSA present a robust summary of the enDre arDcles with useful informaDon extracted about specific genes that are associated to cancer disease. Below is the summary and visualisaDon of key terms from the summary using a world cloud (Figure <ref type="figure" target="#fig_3">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sample Extracted Summary</head><p>The sentences with the most prevalence sentence score was used for the summary together. We used the heap queue (heapq) library to select the most or very useful sentences. The heapq is used in implemenDng the priority queues for word frequencies in sentences with higher weight is given more priority in processing the summary. The threshold indicates the number of sentences to summarize (see Table <ref type="table" target="#tab_0">III</ref>). Different threshold points were selected for the summary and the result indicate differently even though the word frequency selected is less than 30 maximum (&lt; 30). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Findings</head><p>The summary result has revealed very interesDng findings of genes that are associated to some Cancerous and type 2 diabetes diseases (see Table <ref type="table" target="#tab_0">II</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ROUGE: RELIABILITY &amp; VALIDITY OF MODEL</head><p>ROUGE is a metric evaluaDon model which stands for Recall Oriented Understudy for Gisting Evaluation. It is an intrinsic metric for automaDcally evaluaDng document summaries <ref type="bibr" target="#b30">[31]</ref>. This is originally based on a metric used for machine translaDon called Bilingual EvaluaDon Understudy (BLEU). BLEU metric is a score for comparing a machine or candidate translaDon of text to one or more human annotaDon or reference translaDons. Although developed originally for text translaDon, it can be used to evaluate text generated for a set of natural language processing acDviDes. ROUGE has measures that allow for the evaluaDon of the accuracy of system summary as compared to a human created summary known as the model summary <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. The measures were able to count the number of overlapping units of word such as n -gram, bi -gram and word pairs between the system generated summaries and the model summaries created by humans. This study introduces a few ROUGE measures: ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, ROUGE-S included in the original ROUGE evaluaDon model and used in this research. ROUGE was used to check for the reliability and validity of our model. Aqer the model is fiVed, the external quality of the model is verified according to the fit metric test ROUGE. Common metrics include, but are not limited to, parsimonious fit, valueadded fit, absolute fit and other metrics, and the intrinsic quality of the model is verified through the fit analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE III ROUGE METRICS MEASUREMENT SUMMARIES</head><p>System and Human Annotated Summaries Type Summary SSummary 'Some of the genes in the BCAA metabolic pathway such as MLYCD (rank 164)HADHB (rank 354)IVD (rank 713)MUT (rank 921)and PCCB (rank 684) are also ranked highly by Hridaya. The SVMs are based on 181 features broadly grouped into (1) gene8c( <ref type="formula">2</ref>) epigene8c( <ref type="formula" target="#formula_0">3</ref>) transcriptomic( <ref type="formula">4</ref>) phenotypicand <ref type="bibr" target="#b4">(5)</ref> evolu8onary. The genes are PDGFRBABL1FLT1; and these genes are drug targets of cancer drugs like Dasa8nib (targets -PDGFRBABL1)Pazopanib (targets -PDGFRBFLT1)Pona8nib (target -ABL1)26.' Our result revealed that ROUGE-1 expressed beVer average result for the Recall (R), Precision (P), F1 score respecDvely with a 95% confident interval (see Table <ref type="table" target="#tab_3">IV</ref>). The result revealed beVer evaluaDon metric in the Recall column of the ROUGE evaluaDon metrics. The results expressed beVer in ROUGE-1 with the Recall slightly over 83%, Precision slightly over 85% and F1-Score slightly over 84% as reviewed in Table V.  summary model, produce a more appealing result. This was because the second summary was closely aligned with the original automated system summary (see Table <ref type="table" target="#tab_5">V</ref>). This shows that the closeness of the human model summary to the system or reference summary produces beVer average across all ROUGE measuring dimensions (Recall, Precision and F1 score).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing the system generated summary with a new human</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Procedure: Recall &amp; Precision</head><p>We have mulDple processed arDcles or documents extracted from the web based on key search terms. The documents are stored in a given name CleanHTML.txt file and an automaDc summary was generated and stored in a file called summary.txt. We then produced a set of human annotated reference summaries of the CleanHTML.txt document. The Recall in the context of the ROUGE metric simply means we are calculaDng how much of reference summary (the human summary) is the system summary (automated machine summary) recovering or capturing from our text. In considering the individual words in a sentence we simply represent this with the formula in equaDon 8.</p><p>The metric will produce a perfect result of 1 which usually will be the case if indeed the sentence matches. This metric simply means all the words in the reference summary has been captured by the system summary.</p><p>In the system generated summary, which someDmes might be very large based on the threshold selected, capturing all the words in the reference or model summary. However, most of the worlds in the system summary might be unnecessary verbose. But, this where precision becomes very important. In conducDng precision on the summary, we are essenDally measuring how much of the system or machine summary is required? We can measure precision using the equaDon 9.</p><p>This means we will evaluate and calculate words in the sentence summary of the Recall overlapping with the total words in the system summary. This will predict the words that are relevant which appears in the reference and over the total words in the system summary.The system's summary mostly contains unnecessary words in the summary. Therefore, our precision becomes crucial as we are trying to predict generated summaries that should be concise in nature. In this study, we combined and computerised both the Precision and Recall and further report the F1 -score measure.</p><p>In order to ascertain the validity of the study, we measured ROUGE-N, ROUGE -S and ROUGE -L which are the granularity of texts that was compared between the system summaries and the reference or human annotated model summaries. ROUGE -1 refers to the overlap of unigrams between the system summary and reference summary. ROUGE -2 refers to the overlap of bigrams between the system reference and the model or reference summaries. We computed precision and recall scores of the ROUGE -2. The main reason why ROUGE-1 could be considered over others or in conjuncDon with ROUGE -2 or even other fine granularity measures is because it reveals the fluency of the summaries or if used in a translaDon task. The intuiDon is that following the word ordering of the reference m=summary indicate that the summary is more fluent.</p><p>The precision result tells us about the percentage (%) of the overlap between the system summary bigrams and the reference summary. We noDced in the case of the abstracDve summarizaDon as both the summaries of system and reference summaries get larger. There are few overlapping bigrams outcome as we are not always or directly re-using the whole sentences for the summarizaDon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RESULTS &amp; FINDINGS</head><p>The terms in the topic modelling show text which are mostly frequent in the document these were depicted by the size of the circle (as seen in Figures <ref type="figure" target="#fig_5">7</ref> and<ref type="figure" target="#fig_6">8</ref> ). RepresentaDon of the result using scaVer plot would reveal the distance between topics, the distribuDon and relaDonship between topic levels.</p><p>The distance between two or more topics is an approximaDon of their semanDc relaDonship. Note that close topics such as topics 1, 2 and 3 are semanDcally related which describes the terms in the topics. As observed in Figures <ref type="figure" target="#fig_5">7</ref> and<ref type="figure" target="#fig_6">8</ref> the terms gene and disease are described in the arDcles in relaDon to the topics distribuDon. This reveals that topics 1, 2 and 3 are semanDcally distributed and have relaDonship on topic levels. These reveal five selected topics from the topic model analysed using the LDA model. The LDA model was one of the input argument together with the corpus and dicDonary of the emerging terms used for the topic modelling. The slider (λ) in the web-based interacDve visualizaDon depicts the relevance metric of the rank terms. It is worth knowing that the terms of the topic are ranked in decreasing order by default in accordance with the topic-specific probability (λ = 1). Figure <ref type="figure" target="#fig_5">7</ref> reveals the common terms from the topic model when the slider is at the full probability. Note that the search key gene and disease were used to extract the text data (100 journal papers) from PubMed journal database related to the terms <ref type="bibr" target="#b21">[22]</ref>. Figure <ref type="figure" target="#fig_6">8</ref> shows the most common terms in topic 2 to be 'gene' and 'disease' when the slider (λ = 0.48) is posiDoned at 0.48 probability. VIII. LIMITATION This study's limitaDons are observed in the precise summary predicDon of arDcles of varying wriVen styles. Some of the summarizaDon models in most cases prefer nouns. Themes emerging from arDcles influences the grammaDcal structure of certain arDcle summaries. Another limitaDon of the study was that the model takes longer to evaluate the emerging terms within the topic modelling approach used due to the large text data for analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. DISCUSSION</head><p>In this study, we presented a fully data-driven approach for automaDc text summarizaDon. We proposed and evaluated the model on unstructured datasets which show some results comparable to the current state-of-the-art topic modelling techniques without depending on modificaDons using any linguisDc informaDon models <ref type="bibr" target="#b33">[34]</ref>. Manual summarizaDon is laborious and challenging task to accomplish. Therefore, automaDzaDon of the task is very essenDal. This process is gaining popularity among researchers. SummarizaDon technique has been applied to various natural language processing (NLP) task such as in the areas of text analysis, classificaDon, automated quesDon and answering, financial and legal texts summarizaDon, news summarizaDon and reviewing of news headlines and the generaDon of social media arDcles <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Performing research in these various topics could benefit from the early stages of document summaries which can be integrated into any base model at intermediate stages to help reduce the length of the document for further analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. CONCLUSION</head><p>AutomaDc summarizaDon is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document <ref type="bibr" target="#b35">[36]</ref>. Text summarizaDon is a very laborious problem to work on without accuracy in the summaries extracted from the documents. This study proposed fully automated single and mulDple documents text summarizaDon. MulDple documents were extracted and summarised while preserving the overarching meaning and purpose of the collecDve arDcles. The LDA model was one of the input arguments together with the corpus and dicDonary of the terms that were used to perform the topic modelling in the study. The model designed within the study could conduct a cross-language text summarizaDon where arDcles from other foreign languages could be processed and the summary translated into English and other languages. Our proposed future study will look into performing topic modelling with these documents and observe whether the approach retain the meaning of the original documents. The result from the future research will be compared with a current machine learning gene predicDon applicaDon model designed for a new study on genes and diseases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. DicJonary of word frequency corpus</figDesc><graphic coords="4,323.30,51.75,231.30,128.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Pipeline of text mining processing</figDesc><graphic coords="4,322.85,410.52,231.26,92.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Raw HTML arJcle dataset processed to clean text</figDesc><graphic coords="5,59.55,104.19,231.25,115.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Word cloud visualisaJon from the LSA summary (threshold &gt;= 3)</figDesc><graphic coords="7,59.60,251.54,231.29,231.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>HModel1'Some BCAA genes such as MLYCD, IVD , MUT and PCCB are ranked highly by Hridaya using SVM that is based on 181 features. These genes are drug targets of cancer drugs such as Dasa8nib, Pazopanib and Pona8nib.' HModel2 'A few genes in the BCAA metabolic pathway are also ranked highly by Hridaya and some examples include MUT (rank 921), IVD (rank 713), PCCB (rank 684), HADHE (rank 354) and MLYCD (rank 164). The SVMs are grouped into five categories based on 181 features and the categories are; gene8c, epigene8c, transcriptomic, phenotypicand and evolu8onary. The genes are PDGFRBABL1FLT1 and are drug targets of cancer drugs such as Dasa8nib (targets -PDGFRBABL1), Pazopanib (targets -PDGFRBFLT1) and Pona8nib (target -ABL1)26.'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Topic model visualizaJon of interacJve web-based topics</figDesc><graphic coords="9,49.40,269.63,252.20,155.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Topic model visualizaJon of interacJve model show search terms predominantly projected</figDesc><graphic coords="9,49.40,536.38,252.22,153.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SUMMARY</head><label>I</label><figDesc>OF ARTICLE USING DIFFERENT THRESHOLDS</figDesc><table><row><cell></cell><cell>LSA Extrac8ve Summary</cell></row><row><cell>Threshol</cell><cell>Summary</cell></row><row><cell>d</cell><cell></cell></row><row><cell>&gt;= 3</cell><cell cols="2">'Some of the genes in the BCAA metabolic pathway</cell></row><row><cell></cell><cell>such as MLYCD (rank 164)HADHB</cell></row><row><cell></cell><cell cols="2">(rank 354)IVD (rank 713)MUT (rank 921)and PCCB</cell></row><row><cell></cell><cell cols="2">(rank 684) are also ranked highly by Hridaya. The</cell></row><row><cell></cell><cell cols="2">SVMs are based on 181 features broadly grouped</cell></row><row><cell></cell><cell cols="2">into (1) gene8c(2) epigene8c(3) transcriptomic(4)</cell></row><row><cell></cell><cell cols="2">phenotypicand (5) evolu8onary. The genes are</cell></row><row><cell></cell><cell cols="2">PDGFRBABL1FLT1; and these genes are drug targets</cell></row><row><cell></cell><cell cols="2">of cancer drugs like Dasa8nib (targets -</cell></row><row><cell></cell><cell>PDGFRBABL1)Pazopanib (targets -</cell></row><row><cell></cell><cell>PDGFRBFLT1)Pona8nib (target -ABL1)26'</cell></row><row><cell>&gt;= 5</cell><cell cols="2">'The genes are PDGFRBABL1FLT1; and these genes</cell></row><row><cell></cell><cell cols="2">are drug targets of cancer drugs like Dasa8nib</cell></row><row><cell></cell><cell>(targets -PDGFRBABL1)Pazopanib (targets</cell></row><row><cell></cell><cell cols="2">-PDGFRBFLT1)Pona8nib (target -ABL1)26. For a</cell></row><row><cell></cell><cell cols="2">given genethe product of the two probabili8es</cell></row><row><cell></cell><cell>P'(DCM|All)2009=˘</cell><cell>2009˘</cell></row><row><cell></cell><cell cols="2">P'(Diseasefunc8onal|All)xP'(DCM|Diseasefunc8on</cell></row><row><cell></cell><cell cols="2">al)called Hridaya-poten8alis the final es8mated</cell></row><row><cell></cell><cell cols="2">poten8al of a gene to be a DCM func8onal gene.</cell></row><row><cell></cell><cell cols="2">Encouraginglywe find that the Hridaya-poten8als</cell></row><row><cell></cell><cell cols="2">are much higher for genes having differen8al exon</cell></row><row><cell></cell><cell cols="2">usage (739 genes) than the rest of the genes</cell></row><row><cell></cell><cell cols="2">(Wilcoxon rank-rump-value2009=˘ 20091.31e-73)˘ .'</cell></row><row><cell>&gt;= 7</cell><cell cols="2">'The genes are PDGFRBABL1FLT1; and these genes</cell></row><row><cell></cell><cell cols="2">are drug targets of cancer drugs like Dasa8nib</cell></row><row><cell></cell><cell>(targets -PDGFRBABL1)Pazopanib (targets</cell></row><row><cell></cell><cell cols="2">-PDGFRBFLT1)Pona8nib (target -ABL1)26. For a</cell></row><row><cell></cell><cell cols="2">given genethe product of the two probabili8es</cell></row><row><cell></cell><cell>P'(DCM|All)2009=˘</cell><cell>2009˘</cell></row><row><cell></cell><cell cols="2">P'(Diseasefunc8onal|All)xP'(DCM|Diseasefunc8on</cell></row><row><cell></cell><cell cols="2">al)called Hridaya-poten8alis the final es8mated</cell></row><row><cell></cell><cell cols="2">poten8al of a gene to be a DCM func8onal gene.</cell></row><row><cell></cell><cell cols="2">Furthermoreas the set of DCM func8onal genes is a</cell></row><row><cell></cell><cell>subset of</cell></row><row><cell></cell><cell>disease func8onal genes</cell></row><row><cell></cell><cell>P'(DCM,Diseasefunc8onal|All)2009=˘ 2009˘</cell></row><row><cell></cell><cell>P'(DCM|All).'</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV ROUGE</head><label>IV</label><figDesc>METRICS MEASUREMENT &amp; ANALYSIS [Ssummary&amp;Hmodel1]</figDesc><table><row><cell></cell><cell cols="2">Average ROUGE Metrics</cell><cell></cell></row><row><cell>ROUGE</cell><cell>Recall</cell><cell>Precision F1 Score</cell><cell>Conf.int</cell></row><row><cell>ROUGE-1</cell><cell cols="2">0.83784 0.40260 0.54386</cell><cell>95%</cell></row><row><cell>ROUGE-2</cell><cell cols="2">0.44444 0.21053 0.28572</cell><cell>95%</cell></row><row><cell>ROUGE-3</cell><cell cols="2">0.31429 0.14667 0.20000</cell><cell>95%</cell></row><row><cell>ROUGE-4</cell><cell cols="2">0.20588 0.09459 0.12962</cell><cell>95%</cell></row><row><cell>ROUGE-L</cell><cell cols="2">0.78378 0.37662 0.50877</cell><cell>95%</cell></row><row><cell>ROUGE-W-1.2</cell><cell cols="2">0.34210 0.29676 0.31782</cell><cell>95%</cell></row><row><cell>ROUGE-S*</cell><cell cols="2">0.69069 0.15721 0.25612</cell><cell>95%</cell></row><row><cell>ROUGE-SU*</cell><cell cols="2">0.69943 0.16356 0.26512</cell><cell>95%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table V revealed a beVer and well-expressed precision results within all the ROUGE metrics used for the study evaluaDon. ROUGE-1 shows the best percentage measure.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V ROUGE</head><label>V</label><figDesc>METRICS MEASUREMENT &amp; ANALYSIS [Ssummary&amp;Hmodel2]</figDesc><table><row><cell></cell><cell cols="2">Average ROUGE Metrics</cell><cell></cell></row><row><cell>ROUGE</cell><cell>Recall</cell><cell>Precision F1 Score</cell><cell>Conf.int</cell></row><row><cell>ROUGE-1</cell><cell cols="2">0.83544 0.85714 0.84615</cell><cell>95%</cell></row><row><cell>ROUGE-2</cell><cell cols="2">0.56410 0.57895 0.57143</cell><cell>95%</cell></row><row><cell>ROUGE-3</cell><cell cols="2">0.37662 0.38667 0.38158</cell><cell>95%</cell></row><row><cell>ROUGE-4</cell><cell cols="2">0.22368 0.22973 0.22666</cell><cell>95%</cell></row><row><cell>ROUGE-L</cell><cell cols="2">0.60759 0.62338 0.61538</cell><cell>95%</cell></row><row><cell>ROUGE-W-1.2</cell><cell cols="2">0.17792 0.43741 0.25295</cell><cell>95%</cell></row><row><cell>ROUGE-S*</cell><cell cols="2">0.61960 0.65243 0.63559</cell><cell>95%</cell></row><row><cell>ROUGE-SU*</cell><cell cols="2">0.62488 0.65756 0.64080</cell><cell>95%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lancaster at semeval-2018 task 3: InvesJgaJng ironic features in english tweets</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dearden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Interna4onal Workshop on Seman4c Evalua4on</title>
		<meeting>The 12th Interna4onal Workshop on Seman4c Evalua4on</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="587" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language independent evaluaJon of translaJon style and consistency: Comparing human and machine translaJons of camus&apos; novel &quot;the stranger</title>
		<author>
			<persName><forename type="first">M</forename><surname>El-Haj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interna4onal Conference on Text, Speech, and Dialogue</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="116" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Using mechanical turk to create a corpus of arabic summaries</title>
		<author>
			<persName><forename type="first">M</forename><surname>El-Haj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ExtracJve text summarizaJon using neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gahlot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10137</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Neural summarizaJon by extracJng sentences and words</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07252</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Review of text summarizaJon in indian regional languages</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thapa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 3rd Interna4onal Conference on Compu4ng Informa4cs and Networks</title>
		<meeting>3rd Interna4onal Conference on Compu4ng Informa4cs and Networks</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">AutomaJc text summarizaJon: A comprehensive survey</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>El-Kassas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Salama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rafea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Expert Systems with Applica4ons</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page">113679</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Text summarizaJon using natural language processing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Prudhvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chowdary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S R</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent System Design</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="535" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automated text simplificaJon: A survey</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Al-Thanyyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Azmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Compu4ng Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Arabic topic detecJon using automaJc text summarisaJon</title>
		<author>
			<persName><forename type="first">R</forename><surname>Koulali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Haj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meziane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 ACS Interna4onal Conference on Computer Systems and Applica4ons (AICCSA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single document extracJve text summarizaJon using geneJc algorithms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cha+erjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mi+al</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 Third Interna4onal Conference on Emerging Applica4ons of Informa4on Technology</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="19" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint abstracJve and extracJve method for long financial document summarizaJon</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zmandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Haj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Financial Narra4ve Processing Workshop</title>
		<meeting>the 3rd Financial Narra4ve Processing Workshop</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="99" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using a keyness metric for single and mulJ document summarisaJon</title>
		<author>
			<persName><forename type="first">M</forename><surname>El-Haj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Mul4Ling 2013 Workshop on Mul4lingual Mul4-document Summariza4on</title>
		<meeting>the Mul4Ling 2013 Workshop on Mul4lingual Mul4-document Summariza4on</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="64" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">AbstracJve text summarizaJon using lstm-cnn based deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mul4media Tools and Applica4ons</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="857" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey on abstracJve text summarizaJon</title>
		<author>
			<persName><forename type="first">N</forename><surname>Moratanch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chitrakala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Interna4onal Conference on Circuit, power and compu4ng technologies (ICCPCT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A neural a+enJon model for abstracJve sentence summarizaJon</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dofm: Domain feature miner for robust extracJve summarizaJon</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mohanty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Informa4on Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">102474</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Applied text analysis with python: Enabling language-aware data products with machine learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bengfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bilbro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ojeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mooc design principles: Topic modellingpyldavis visualizaJon summarizaJon of learners&apos; engagement</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F O</forename><surname>Onah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L L</forename><surname>Pang</surname></persName>
		</author>
		<idno type="DOI">10.21125/edulearn.2021.0282</idno>
		<ptr target="h+p://dx.doi.org/10.21125/edulearn.2021.0282" />
	</analytic>
	<monogr>
		<title level="m">EDULEARN21 Proceedings, ser. 13th InternaJonal Conference on EducaJon and New Learning Technologies</title>
		<imprint>
			<date type="published" when="2021-07">July, 2021 2021</date>
			<biblScope unit="page" from="1082" to="1091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocaJon</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocaJon for tag recommendaJon</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krestel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fankhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nejdl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third ACM conference on Recommender systems</title>
		<meeting>the third ACM conference on Recommender systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Enuwajgx: Machine learning gene predicJon sorware applicaJon model -an innovaJve method to precision medicine and predicJve analysis of visualising mutated genes associated to neurological phenotype of diseases</title>
		<author>
			<persName><forename type="first">D</forename><surname>Onah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Interna4onal Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management -KDIR</title>
		<meeting>the 14th Interna4onal Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management -KDIR</meeting>
		<imprint>
			<publisher>INSTICC. SciTePress</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="281" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learners&apos; engagement in adult literacy educaJon. ncsall reports# 28</title>
		<author>
			<persName><forename type="first">H</forename><surname>Beder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Medina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Riccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Na4onal Center for the Study of Adult Learning and Literacy (NCSALL)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ldavis: A method for visualizing and interpreJng topics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sievert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on interac4ve language learning</title>
		<meeting>the workshop on interac4ve language learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Natural language processing with Python: analyzing text with the natural language toolkit</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Assessing sentence scoring techniques for extracJve text summarizaJon</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>De Souza Cabral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Lins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Cavalcanj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Simske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Expert systems with applica4ons</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="5755" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">VisualizaJon techniques for topic model checking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Ar4ficial Intelligence</title>
		<meeting>the AAAI Conference on Ar4ficial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Termite: VisualizaJon techniques for assessing textual topic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the interna4onal working conference on advanced visual interfaces</title>
		<meeting>the interna4onal working conference on advanced visual interfaces</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="74" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Text summarizaJon using latent semanJc analysis model in mobile android platorm</title>
		<author>
			<persName><forename type="first">O.-M</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-P</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-A</forename><surname>Jaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 9th Asia Modelling Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="35" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Text summarizaJon using latent semanJc analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Ozsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">N</forename><surname>Alpaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cicekli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Informa4on Science</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="405" to="417" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rouge: A package for automaJc evaluaJon of summaries</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summariza4on branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Looking for a few good metrics: Rouge and its evaluaJon</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ntcir workshop</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The limits of automaJc summarisaJon according to rouge</title>
		<author>
			<persName><forename type="first">N</forename><surname>Schluter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Associa4on for Computa4onal Linguis4cs</title>
		<meeting>the 15th Conference of the European Chapter of the Associa4on for Computa4onal Linguis4cs</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="41" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">AutomaJc text summarizaJon: A state-of-the-art review</title>
		<author>
			<persName><forename type="first">O</forename><surname>Klymenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ma+hes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICEIS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="648" to="655" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Literature review of automaJc single document text summarizaJon using nlp</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pervin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Begum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interna4onal Journal of Innova4on and Applied Studies</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="857" to="865" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">VariaJons of the similarity funcJon of textrank for automated summarizaJon</title>
		<author>
			<persName><forename type="first">F</forename><surname>Barrios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Argerich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wachenchauzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03606</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
