{
  "id": 4291349901590631578,
  "name": "Onah_et_al._-_2023_-_A_Data-driven_Latent_Semantic_Analysis_for_Automat.grobid.tei.xml",
  "segments": [
    {
      "header": "I. INTRODUCTION",
      "content": "topic modelling has been performed on several types of documents in the past.\nhowever, this study presents a novel approach to topic modelling by performing extracdve summarizadon on over 100 ardcles related to genes and associated diseases and feeding the summary as an input argument a latent dirichlet allocadon (lda) model in order to perform the topic modelling.\nthe idea here is to idendfy the commonalides between ardcles of the same genre describing a specific topic of interest in the research.\nthe study is addressing journal ardcles retrieved from pubmed central 1 h+ps://www.ncbi.nlm.nih.gov/pmc/ (pmc 1 ) database discussing about genes and their associated diseases.\none strategy might be to read through each of the documents, highlighdng the terms or phrases most relevant to each, and then sort them all into piles.\nonce you had gone through all the documents and grouped them, you could examine each pile more closely.\nperhaps you would use the main phrases or words from each pile to write up the summaries and give each a unique name-the topic of the pile.\nat its core, this sordng task relies on our ability to compare two documents and determine their similarity.\ndocuments that are similar to each other are grouped together and the resuldng groups broadly describe the overall themes, topics, and paverns inside the corpus.\nwith so many documents being extracted from social media, review comments from online plaworms and microblogs as twiver, a huge amount of natural language data is being mined and are available to be analysed.\ncertainly, it is reasonable to translate summarized documents accurately.\nan example, ardcles extracted in a different language from english to be translated to make sense similar to the original language of which the ardcle was wriven.automadc text summarizadon is the process of performing specific nlp task by producing a concise summary of documents (single or muldple) without any manual support while preserving the meaning or important points of the original document.\nin this study, we try to answer the following research quesdons:\u2022 how automated text summarizadon techniques were used in an extracdve summary of ardcles?\u2022 how topic modelling models were used in producing emerging terms that are related to muldple and different journal ardcles?\n\u2022 are the search terms used for the text mining of ardcles from the database predominant in the emerging terms that were extracted from the processed text?\nthe experimental results show that the proposed model achieves good performance in terms of the document summary and the topic modelling informadon retrieved from the full document.\nsecdon 2, covers related study on summarizadon.\nsecdon 3, conceptualise the methods and describes the techniques applied in the study.\nsecdon 4 describes topic modelling, secdon 5, presents a descripdon of the model pipeline.\nsecdon 6, presents the results and findings.\nthe limitadon of the study is presented in secdon 7 and finally, the discussion and conclusion of the study are presented in secdons 8 and 9."
    },
    {
      "header": "II. RELATED WORK",
      "content": "the amount of text data being produced worldwide is enormous and growing rapidly.\nunless these text data are extracted and make meaning, then the most important and relevant informadon would be lost.\nsummarizadon is described as the process of presendng huge data informadon in a concise manner while focusing on the most useful secdons of the data whilst preserving the original meaning.\nthe most important element of text summarizadon is to produce a clear and concise summary taken from the large datasets that would make sense to the reader and direct to the main points.\nthere is a need for automadon of these increasingly available web text data for informadon retrieval and sustainability.\nin this modern era of big data, text mining has been retrieved from various sources, website, databases, journals and conference ardcles in related studies.\nthe voluminous text data need to be collected and summarised in order to retrieve useful informadon concerning the main content of the document."
    },
    {
      "header": "A. Summariza-on",
      "content": "summarizadon is a technique in nlp that is used for condensing or summarising huge texts into smaller versions taking care not to omit the main relevant informadon contained in the document.\nthere are so many important models for performing automadc text summarizadon in various nlp tasks such as classificadon, automadc quesdon and answering, computadonal journalism, financial summarizadon, news summarizadon and foreign language summary transladon.\none of the key factors of the document summary is that it can be integrated into these nlp applicadons to reduce the size of the document for processing while possibly retaining the original informadon contained in the document,.\nthere are two different approaches to automadc summarizadon; these are extracdon and abstracdon.1) extrac-ve approach: extracdve summarizadon approach considers the top n sentences based on their score rankings for the summary generadon,.\nextracdve summarizadon approach takes object as input and generate the summary based on the probability vector.\nwe will briefly describe the abstracdve approach of summarizadon and explain why we decided to use extracdve approach in this study.2) abstrac-ve approach:in abstracdve text summarizadon technique, this follows the convendon of unsupervised approach where machine learning paradigms such as deep learning plays a big role in generadng the document summary.\nthis approach considers a bovom-up summary for which some of the sentences might not be part of the original document.\ndesigning an abstracdve model for summarizadon is very problemadc and challenging because it involves a more complex language modelling.in this study, we decided to use extracdve approach for ardcle summarizadon, because we wanted all parts of the sentences that will be summarised to be from the original document."
    },
    {
      "header": "B. Topic Modelling",
      "content": "topic modelling is the process of labelling and describing documents into topics.\nthis is an unsupervised machine learning technique for abstracdng topics from collecdons of documents.\ntopic modelling approach is based on an inducdve modelling used to abstract core themes from a weighted graphical representadon of documents obtained during the processing stages.\nin order to apply topic models in nlp applicadon, there is need for extrapoladon of topics from unstructured datasets.\nin this study, scitkit -learn and gensim were used to extract the topics from the models using 0 gensim.models.ldamodel.ldamodel 0 , which takes in as input argument the text corpus, number of topics to be extracted and id2word that contains the dicdonary terms for the preprocessed document."
    },
    {
      "header": "C. Latent Dirichlet Alloca-on",
      "content": "latent dirichlet allocadon (lda) is a technique applied in topic modelling introduced by.\nthis is a topic discovery technique used to generate topics based on the probability that each given term might occur within the document.\nthe document can be in the form of mixture of topics that might not necessarily be disdnct and words may appear in muldple topics,.\nin this approach, presented with words or token from muldple documents from which a probability topics model is constructed, we observed word distribudons for each mixture of topics in the document."
    },
    {
      "header": "III. METHODS",
      "content": "the model applied some nlp techniques for the inidal preprocessing of the data for extracdve summarizadon.\nthe proposed model in this study is scalable and generalizable for producing arbitrarily size summaries by splitng the documents into resemble content.\nthe study applied sentence scoring on the clean document to extract text that fell within the threshold of high frequency score used in the model.\nduring the summarizadon process, we calculated word frequency and the high sentence scores that was used to summarise the ardcles.\nthe next phase is to perform a cosine similarity scores of the sentences using a matrix dimensions of n * n, where n is the number of sentences in the document.\nwe then extract the top n sentences based on their ranking for the summary generadon that was then fed into the lda topic modelling.the study was designed to apply a generalised concept of lda topic modelling technique to create a dicdonary of terms that was fed from the summarised ardcles.\nthis dicdonary of terms was used to build a vectorised corpus of lexicon lda model.\none of the key approaches that was used in the experiment was the 'pyldavis.gensim.prepare' method which takes as an argument our lda model, the corpus and the derived lexicon which contains the dicdonary terms for the study,,,.\nanother method that was in the study was the 'gensim.models.ldamodel.ldamodel 0 , which takes the summary corpus as an input argument, the number of topics to be extracted and the 'id2word 0 that contains our dicdonary terms for the journal ardcles."
    },
    {
      "header": "A. Model Descrip-on",
      "content": "the overarching research model was developed to retrieve specific informadon from huge published journals using the topic modelling approach of nlp.\nthe task is to generate the summary at most predominant sentence level.\nextracdve summarizadon approach applied in the study produced naturally grammadcal summaries without much linguisdc connotadon or analysis.\nsince extracdve summarizadon uses a supervised technique, the sentence selecdon process involves scoring each sentence in the original cleaned document.\nin this case, a label is produced to indicate whether a sentence met the condidons which are the chosen length of the sentence or the summary threshold indicated in the model.\nit is only when these predefined condidons are met before a sentence could be considered to be included in the final summary.\nthe supervised learning method allows for maximizadon of the likelihood of sentence consideradon from the input document.\nthis approach could also be generalised on other ardcles such as media, blogs, news and so on and will produce the same outcomes."
    },
    {
      "header": "1)",
      "content": "sentence scoring method: : in this study, a scoring funcdon is introduced to generate the sentence score dicdonary which hold the value assigned to each sentence.\nthis denotes the probability that the sentence will be selected and included in the summary (see figure).\nthe summary length is fixed, therefore, the top n sentences with the highest score rankings are chosen for the summary.\nin this study, the quality of the document summary largely depends on the chosen sentences and this would reveal the relevance of the informadon retrieved from within the full document.\nthe process of scoring the sentence is represented in equadons 1 and 2.\nif the sentence is not in the sentence score dicdonary keys during the processing, the words in the word frequencies dicdonary is added to the sentence scores (see equadon 1 )."
    },
    {
      "header": "Sentscores[S] = Wordfreq[W]",
      "content": "(1)during the sentence model processing interval, the length of the sentence is either increased or reduced by certain values within the sentence scores dicdonary.\ntherefore, new sentences are added into the sentence dicdonary scores.\nthe sentence model would check whether the new sentences are in the sentence dicdonary.\nif the sentence exists in the sentence dicdonary, then the model will proceed accordingly.\nbut if the process sentence is not in the sentence scores dicdonary keys, then the word in the word frequencies dicdonary is added to the sentence in the sentence scores dicdonary (see equadon 2)."
    },
    {
      "header": "Sentscores[S]+ = Wordfreq[W]",
      "content": "(2)2) word frequency:: dicdonary of word frequency corpus was generated within the model.\nthe word frequencies were selected automadcally based on the prevalence or occurrence of the words in the corpus dicdonary created in the model (see figure).\nthe length of the sentence selected for the word frequencies was less than 30.\nsentences with less than fig..\ntokenize sentence score for the arjcle summary 30 (< 30) words were selected.\nthese maximum weighted frequency (freqmax) of each word were calculated by using the product of the word frequencies (wfreq) and the values (v ).\nthese are then added to final summary (see equadon 3).the next equadon allows us to calculate the maximum word in the word frequencies (see equadon 4)."
    },
    { "header": "Word", "content": "" },
    {
      "header": "B. Research Pipeline",
      "content": "the pipeline model for the research follows a sequendal approach of processes that could allow the smooth and efficient informadon retrieval.\nthe pipeline in figurewas used to answer the research quesdons in this study.1) data collec-on: the dataset was scraped from the web.\nabout 100 papers were extracted from pubmed central (pmc) database (\"https : //www.ncbi.nlm.nih.gov/pmc/ 00 ) using a search key combinadon of 'gene' and 'disease'.\nthe ardcles scraped from the web were all related to medical science research.\npapers related to diseases and the mutated genes causadon were extracted for this study.\nthese papers were extracted with html tags that are required to be preprocessed, cleaned and summarized for the topic modelling approach."
    },
    {
      "header": "2)",
      "content": "pre-processing & feature extrac-on: the web-based dataset scraped from pubmed journal was in raw state and unstructured which consist of html tags, special characters, symbols and numbers that had to be processed and cleaned.\nthe preprocessing involved converdng the dataset into text documents using nlp packages such as beaudfulsoup, regular expression, lxml, tokenisadon and using nltk library.\nin the feature extracdon process, we parse the web ardcles source code in order to extract the textual material needed for the final summary.\nas the ardcles were parsed through the source code, the text for extracdon are between the paragraphs' tags < p > text < /p >.\nduring the process of formatng the clean ardcles, we performed extra filtering of special characters from the processed text in order to find and replace these symbols automadcally.\nfinally, these extracted paragraphs text are combined to form a single string to store the clean web content for further topic model processing (see figure)."
    },
    {
      "header": "3)",
      "content": "stopwords: we further removed a list of stop-words from the propocessed ardcles.\nwords such as pronounce that are not necessary or essendal for the final summary (see figurefor the list of stop words)."
    },
    {
      "header": "4) Topic Modelling & Visualiza-on:",
      "content": "this study was able to reveal prevalence of terms that emerged within the documents and show their relevance by how the projecdon of the topic modelling circle and the size of a word in the result visualisadon.\nthe result was visualised using pyldavis which is a web-based interacdve visualizadon package that allows the display of the topics that were idendfied using the lda approach.\npyldavis was used for extracdng informadon from the fived lda topic models to design a web-based interacdve visualizadon.\nthe main method that was applied in fig..\nlist of stop-words removed from the arjcle this study was 'pyldavis.gensim.prepare' which takes as an argument topic models from lda, the vectorized text corpus, and the derived lexicon which contains the dicdonary terms from the study,.\neach idendfied topic is encoded in the circles of the pyldavis and the bigger the circle the more projecdon or prevalence is the topic (as seen in figuresand).\nthe higher the number of common words among sentences indicates that the sentences are semandcally related."
    },
    {
      "header": "IV. MODEL",
      "content": "a. defining seman-c significance we define the semandc significance of term t to the topic n given the parameter weight of the (\u03bb) where(0 \u2264 \u03bb \u2264 1).\nlet pt denotes the minimal probability of the term t in the lexical corpus.\nlet nt denote the probability of term t element of 1, ..., n for n element of 1,..., k, where n denotes the frequency of terms in the vocabulary (see equadonwhere (\u03bb) is the weight given to probability of the terms t in topic n (equadon 1)"
    },
    {
      "header": "B. Defining Saliency Term",
      "content": "in this study we define saliency term as given a word 'w 0 , we compute its minimal probability p(tm/w).\nwhere tm is the topic model.\nthe possibility that the emerge word w was generated from the lda topic model (tm).we also compute the marginal probability p(tm): -with the possibility that any word w 0 randomly selected was generated by tm.\nwe define the uniqueness of each idendfied word 0 w 0 as the divergence occurrence between p(tm/w) and p(tm): we were able to compute 5 topics (t) and 10 passes which were selected from the latent dirichlet allocadon (lda) topic modelling (see equadon 6)."
    },
    { "header": "U(6)", "content": "" },
    {
      "header": "=5",
      "content": "the uniqueness of each term is described as how significance and semandcally associated they are to the topics.\nfor example, a term could be semandcally associated to more than one topic.\nthe frequency and populadon of terms are denoted by the size of the topic circles and also the inter-topic distance denote how closely related the topics are.\nwe nodce a few words that are expressed in several topics, but observing this word w reveals livle informadon about the mixture or semandc associadon of the topics.\nthis helps to remove ambiguity of the terms associadon by making term distribudon clearly.\nlooking at the figure, we observed that given equal frequency of words, the list of the most common, relevant or disdncdve terms (e.g. gene, disease, expression, associate ) are prevalence in the visualised graph-plot distribudon.\nthe saliency measures the distribudon of the speeds and idendficadon of topic associadon and composidon (e.g. prevalence topic 1 terms such as genes,disease etc.\nthese terms are all semandcally associated to topics 2 and 3)."
    },
    {
      "header": "V. LATENT SEMANTIC ANALYSIS",
      "content": "latent semandc analysis (lsa) is a robust algebraic and stadsdcal method which extracts hidden semandc structures of words and sentences.\nlsa is used to extract features that cannot be directly mendoned within the dataset.\nthese features are essendal to data, but are not original features of the dataset.\nwe performed a mini summary from the original summary from the study using latent semandc analysis (lsa) for text summarizadon.\nthe mini summary was done from the summary of the original clean 100 ardcles extracted from pubmed (https : //pubmed.ncbi.nlm.nih.gov/) database.\nthis summary is then fived into the rouge metric system to measure the efficacy of the model.\nresults from the lsa present a robust summary of the endre ardcles with useful informadon extracted about specific genes that are associated to cancer disease."
    },
    {
      "header": "A. Sample Extracted Summary",
      "content": "the sentences with the most prevalence sentence score was used for the summary together.\nwe used the heap queue (heapq) library to select the most or very useful sentences.\nthe heapq is used in implemendng the priority queues for word frequencies in sentences with higher weight is given more priority in processing the summary.\nthe threshold indicates the number of sentences to summarize (see table).\ndifferent threshold points were selected for the summary and the result indicate differently even though the word frequency selected is less than 30 maximum (< 30)."
    },
    { "header": "B. Findings", "content": "" },
    {
      "header": "VI. ROUGE: RELIABILITY & VALIDITY OF MODEL",
      "content": "rouge is a metric evaluadon model which stands for recall oriented understudy for gisting evaluation.\nthis is originally based on a metric used for machine transladon called bilingual evaluadon understudy (bleu).\nbleu metric is a score for comparing a machine or candidate transladon of text to one or more human annotadon or reference transladons.\nrouge has measures that allow for the evaluadon of the accuracy of system summary as compared to a human created summary known as the model summary,.\nthis study introduces a few rouge measures: rouge-1, rouge-2, rouge-3, rouge-l, rouge-s included in the original rouge evaluadon model and used in this research.\naqer the model is fived, the external quality of the model is verified according to the fit metric test rouge.\ncommon metrics include, but are not limited to, parsimonious fit, valueadded fit, absolute fit and other metrics, and the intrinsic quality of the model is verified through the fit analysis."
    },
    {
      "header": "TABLE III ROUGE METRICS MEASUREMENT SUMMARIES",
      "content": "system and human annotated summaries type summary ssummary 'some of the genes in the bcaa metabolic pathway such as mlycd (rank 164)hadhb (rank 354)ivd (rank 713)mut (rank 921)and pccb (rank 684) are also ranked highly by hridaya.\nthe svms are based on 181 features broadly grouped into (1) gene8c() epigene8c() transcriptomic() phenotypicandevolu8onary.\nthe genes are pdgfrbabl1flt1; and these genes are drug targets of cancer drugs like dasa8nib (targets -pdgfrbabl1)pazopanib (targets -pdgfrbflt1)pona8nib (target -abl1)26.' our result revealed that rouge-1 expressed bever average result for the recall (r), precision (p), f1 score respecdvely with a 95% confident interval (see table).\nthe result revealed bever evaluadon metric in the recall column of the rouge evaluadon metrics.\nthe results expressed bever in rouge-1 with the recall slightly over 83%, precision slightly over 85% and f1-score slightly over 84% as reviewed in table v.\nsummary model, produce a more appealing result.\nthis was because the second summary was closely aligned with the original automated system summary (see table).\nthis shows that the closeness of the human model summary to the system or reference summary produces bever average across all rouge measuring dimensions (recall, precision and f1 score)."
    },
    {
      "header": "Comparing the system generated summary with a new human",
      "content": ""
    },
    {
      "header": "A. Procedure: Recall & Precision",
      "content": "we then produced a set of human annotated reference summaries of the cleanhtml.txt document.\nthe recall in the context of the rouge metric simply means we are calculadng how much of reference summary (the human summary) is the system summary (automated machine summary) recovering or capturing from our text.\nthis metric simply means all the words in the reference summary has been captured by the system summary.in the system generated summary, which somedmes might be very large based on the threshold selected, capturing all the words in the reference or model summary.\nwe can measure precision using the equadon 9.this means we will evaluate and calculate words in the sentence summary of the recall overlapping with the total words in the system summary.\nthis will predict the words that are relevant which appears in the reference and over the total words in the system summary.the system's summary mostly contains unnecessary words in the summary.\nin this study, we combined and computerised both the precision and recall and further report the f1 -score measure.in order to ascertain the validity of the study, we measured rouge-n, rouge -s and rouge -l which are the granularity of texts that was compared between the system summaries and the reference or human annotated model summaries.\nrouge -1 refers to the overlap of unigrams between the system summary and reference summary.\nrouge -2 refers to the overlap of bigrams between the system reference and the model or reference summaries.\nthe intuidon is that following the word ordering of the reference m=summary indicate that the summary is more fluent.the precision result tells us about the percentage (%) of the overlap between the system summary bigrams and the reference summary."
    },
    {
      "header": "VII. RESULTS & FINDINGS",
      "content": "the terms in the topic modelling show text which are mostly frequent in the document these were depicted by the size of the circle (as seen in figuresand).\nnote that close topics such as topics 1, 2 and 3 are semandcally related which describes the terms in the topics.\nas observed in figuresandthe terms gene and disease are described in the ardcles in reladon to the topics distribudon.\nthe lda model was one of the input argument together with the corpus and dicdonary of the emerging terms used for the topic modelling.\nfigurereveals the common terms from the topic model when the slider is at the full probability.\nfigureshows the most common terms in topic 2 to be 'gene' and 'disease' when the slider (\u03bb = 0.48) is posidoned at 0.48 probability.\nanother limitadon of the study was that the model takes longer to evaluate the emerging terms within the topic modelling approach used due to the large text data for analysis."
    },
    {
      "header": "IX. DISCUSSION",
      "content": "in this study, we presented a fully data-driven approach for automadc text summarizadon.\nwe proposed and evaluated the model on unstructured datasets which show some results comparable to the current state-of-the-art topic modelling techniques without depending on modificadons using any linguisdc informadon models.\nmanual summarizadon is laborious and challenging task to accomplish.\ntherefore, automadzadon of the task is very essendal.\nthis process is gaining popularity among researchers.\nsummarizadon technique has been applied to various natural language processing (nlp) task such as in the areas of text analysis, classificadon, automated quesdon and answering, financial and legal texts summarizadon, news summarizadon and reviewing of news headlines and the generadon of social media ardcles,.\nperforming research in these various topics could benefit from the early stages of document summaries which can be integrated into any base model at intermediate stages to help reduce the length of the document for further analysis."
    },
    {
      "header": "X. CONCLUSION",
      "content": "automadc summarizadon is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document.\ntext summarizadon is a very laborious problem to work on without accuracy in the summaries extracted from the documents.\nthis study proposed fully automated single and muldple documents text summarizadon.\nthe lda model was one of the input arguments together with the corpus and dicdonary of the terms that were used to perform the topic modelling in the study.\nthe model designed within the study could conduct a cross-language text summarizadon where ardcles from other foreign languages could be processed and the summary translated into english and other languages.\nour proposed future study will look into performing topic modelling with these documents and observe whether the approach retain the meaning of the original documents."
    }
  ]
}
