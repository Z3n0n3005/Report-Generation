{"id": 7950148402276318678, "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "The multi-armed bandit (mab) problem considers the sequential strategy one must devise when playing a row of slot machines. The 'context' is the physiology of the patient, the type of resources available for each project, or the features of the website user."}, {"header": "Background and preliminaries", "content": "Mab policies choose the next arm to play, with the goal of maximizing attained rewards, based upon the history observed so far. Bayesian algorithms can be used to extend mab algorithms to more realistic scenarios. flexible reward functions and bayesian inference have been proposed."}, {"header": "SMC for multi-armed bandits", "content": "We model non-stationary, stochastic mabs in a state-space framework. We use smc to compute per-arm parameter posteriors at each bandit round. We approximate per- arm filtering densities with smc-based random measures p m (\u03b8 t,a |h 1:t ), for which there are strong theoretical convergence guarantees."}, {"header": "Evaluation", "content": "Smc-based policies achieve the right exploration-exploitation tradeoff, we show. The main evaluation metric is the cumulative regret of the bandit agent, with results averaged over 500 realizations. We present results for smcbased policies with m = 2000 samples, and provide an evaluation of the impact of m in appendix b."}, {"header": "Conclusion and discussion", "content": "Smc-based bayesian mab framework allows for interpretable modeling of nonlinear and time-evolving reward functions. It sequentially learns the sufficient statistics and dynamics of the bandit to find the right exploration-exploitation balance."}]}