{
  "id": "Urteaga_",
  "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content":  "The multi-armed bandit (MAB) problem considers the sequential strategy one must devise when playing a row of slot machines. The MAB crystallizes the fundamental trade-off between exploration and exploitation in sequential decision making."
    },
    {
      "header": "Background and preliminaries",
      "content":  "MAB policies choose the next arm to play, with the goal of maximizing attained rewards, based upon the history observed so far. We overcome constraints on both the bandit's assumed reward function and its time-evolving model, by leveraging sequential Monte Carlo (SMC)"
    },
    {
      "header": "SMC for multi-armed bandits",
      "content":  "We use the SMC random measure p M (\u03b8 t |H 1:t ) for both Thompson sampling and Bayes-UCB policies. The dynamic linear model is a flexible and widely used framework to characterize timeevolving systems. The SMC-based MAB policies can be applied to non-stationary bandit problems with minimal assumptions."
    },
    {
      "header": "Evaluation",
      "content":  "SMC-based bandit policies can capture non-stationary trends in personalized news article recommendations. The main evaluation metric is the cumulative regret of the bandit agent, as defined in Equations."
    },
    {
      "header": "Conclusion and discussion",
      "content":  "The proposed SMC-based Bayesian MAB framework allows for interpretable modeling of nonlinear and time-evolving reward functions. The time-varying uncertainty of the sequentially updated parameter posterior encourages exploration of arms that have not been played recently, but may have reached new exploitable rewards."
    }
  ]
}
