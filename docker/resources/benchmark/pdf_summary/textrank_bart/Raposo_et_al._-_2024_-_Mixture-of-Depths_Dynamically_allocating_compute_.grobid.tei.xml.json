{"id": "Raposo_e", "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml", "segments": [{"header": "Introduction", "content": [{"summary_text": "Ideally, transformers would use smaller total compute budgets by not spending compute unnecessarily. We leverage an approach akin to Mixture of Experts (MoE) transformers, in which dynamic token-level routing decisions are made across the network depth."}]}, {"header": "Background", "content": [{"summary_text": "The transformer architecture has become the workhorse of a revolution in practical artificial intelligence. This has spurred tremendous interest in making transformer architectures more efficient. We propose a predictive router to enable efficient inference for conditional computation in transformers."}]}, {"header": "Implementing Mixture-of-Depths Transformers", "content": [{"summary_text": "To enforce a total compute budget per forward pass we leverage the notion of capacity. Capacity defines the total number of tokens that comprise the input to a given computation. The former path is computationally expensive."}]}, {"header": "Results", "content": [{"summary_text": "We tested routing every block or every other block, using capacities from 12.5% to 95% of the total sequence. MoD transformers that use stochastic routing (implemented using a top-\ud835\udc58 operation on router weights sampled from a Gaussian distribution) perform drastically worse than both the baseline and normal MoD transformer."}]}, {"header": "Discussion", "content": [{"summary_text": "Learned routing mechanisms are sometimes non-causal; that is, information about the future is used to determine a given token's routing decision. Top-k routing mechanisms present difficulties in post-training autoregressive sampling. It is impossible to use information about future token identities to determine routing decisions."}]}]}