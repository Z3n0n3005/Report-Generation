{
  "id": 4334641094224494844,
  "name": "Onah_et_al._-_2023_-_A_Data-driven_Latent_Semantic_Analysis_for_Automat.grobid.tei.xml",
  "segments": [
    {
      "header": "I. INTRODUCTION",
      "content": "The study is addressing journal ardcles retrieved from pubmed central 1 h+ps://www.ncbi.nlm.gov/pmc/ (pmc 1 ) database discussing about genes and their associated diseases. The idea is to idendfy the commonalides between ardcles of the same genre describing a specific topic of interest in the research."
    },
    {
      "header": "II. RELATED WORK",
      "content": "Summarizadon is the process of summarising huge data informadon in a concise manner. The most important element of text summarization is to produce a clear and concise summary taken from the large datasets."
    },
    {
      "header": "A. Summariza-on",
      "content": " summarizadon is a technique in nlp that is used for condensing or summarising huge texts into smaller versions taking care not to omit the main relevant informadon contained in the document. There are two different approaches to automadc summarizdon; these are extracdon and abstracdon. In this study, we decided to use extracdve approach for ardcle summarizer."
    },
    {
      "header": "B. Topic Modelling",
      "content": " topic modelling is the process of labelling and describing documents into topics. This is an unsupervised machine learning technique for abstracdng topics from collecdons of documents."
    },
    {
      "header": "C. Latent Dirichlet Alloca-on",
      "content": " latent dirichlet allocadon (lda) is a technique applied in topic modelling. This is a topic discovery technique used to generate topics based on the probability that each given term might occur within the document."
    },
    {
      "header": "III. METHODS",
      "content": "The study was designed to apply a generalised concept of lda topic modelling technique to create a dicdonary of terms that was fed from the summarised ardcles. The proposed model is scalable and generalizable for producing arbitrarily size summaries by splitng the documents into resemble content."
    },
    {
      "header": "A. Model Descrip-on",
      "content": "extracdve summarizadon approach applied in the study produced naturally grammadcal summaries without much linguisdc connotadon or analysis. The overarching research model was developed to retrieve specific informadon from huge published journals."
    },
    {
      "header": "1)",
      "content": "In this study, a scoring funcdon is introduced to generate the sentence score dicdonary which hold the value assigned to each sentence. The top n sentences with the highest score rankings are chosen for the summary."
    },
    {
      "header": "Sentscores[S] = Wordfreq[W]",
      "content": "The length of the sentence is either increased or reduced by certain values within the sentence scores dicdonary. New sentences are added into the sentence dicDonary scores. The sentence model would check whether the new sentences are in the sentence."
    },
    {
      "header": "Sentscores[S]+ = Wordfreq[W]",
      "content": "The word frequencies were selected automadcally based on the prevalence or occurrence of the words in the corpus dicdonary created in the model. The maximum weighted frequency (freqmax) of each word was calculated by using the product of the word frequencies (wfreq) and the values (v ). These are then added to final summary."
    },
    { "header": "Word", "content": "" },
    {
      "header": "B. Research Pipeline",
      "content": "About 100 papers were extracted from pubmed central (pmc) database using a search key combinadon of 'gene' and 'disease' The papers were all related to medical science research."
    },
    {
      "header": "2)",
      "content": "The dataset was in raw state and unstructured which consist of html tags, special characters, symbols and numbers that had to be processed and cleaned. The preprocessing involved converdng the dataset into text documents using nlp packages such as beaudfulsoup, regular expression, lxml, tokenisadon and using nltk library. The feature extracdon process, we parse the web ardcles source code in order to extract the textual material needed for the final summary."
    },
    {
      "header": "3)",
      "content": "Stopwords are words that are not necessary or essendal for the final summary. We removed a list of stop-words from the propocessed ardcles."
    },
    {
      "header": "4) Topic Modelling & Visualiza-on:",
      "content": "Pyldavis is a web-based interacdve visualizadon package that allows the display of the topics that were idendfied using the lda approach. The bigger the circle the more projecdon or prevalence is the topic. The higher the number of common words among sentences indicates that the sentences are semandcally related."
    },
    {
      "header": "IV. MODEL",
      "content": "Let nt denote the probability of term t element of 1, k denote the frequency of terms in the vocabulary. (\u03bb) is the weight given to probability of the terms t in topic n (equadon 1)"
    },
    {
      "header": "B. Defining Saliency Term",
      "content": "We define saliency term as given a word 'w 0, we compute its minimal probability p(tm/w) where tm is the topic model. We were able to compute 5 topics (t) and 10 passes which were selected from the latent dirichlet allocadon."
    },
    { "header": "U(6)", "content": "" },
    {
      "header": "=5",
      "content": "The uniqueness of each term is described as how significance and semandcally associated they are to the topics. The frequency and populadon of terms are denoted by the size of the topic circles. The inter-topic distance denote how closely related the topics are."
    },
    {
      "header": "V. LATENT SEMANTIC ANALYSIS",
      "content": " latent semandc analysis (lsa) is a robust algebraic and stadsdcal method. Lsa is used to extract features that cannot be directly mendoned within the dataset. The mini summary was done from the summary of the original clean 100 ardcles."
    },
    {
      "header": "A. Sample Extracted Summary",
      "content": "We used the heap queue (heapq) library to select the most or very useful sentences. The sentences with the most prevalence sentence score were used for the summary together."
    },
    { "header": "B. Findings", "content": "" },
    {
      "header": "VI. ROUGE: RELIABILITY & VALIDITY OF MODEL",
      "content": "Rouge is a metric evaluadon model which stands for recall oriented understudy for gisting evaluation. The external quality of the model is verified according to the fit metric test rouge."
    },
    {
      "header": "TABLE III ROUGE METRICS MEASUREMENT SUMMARIES",
      "content": "The genes in the bcaa metabolic pathway such as mlycd (rank 164)hadhb (rank 354)ivd (rank 713)mut (rank 921)and pccb ( rank 684) are also ranked highly by hridaya. The genes are pdgfrbabl1flt1; and these genes are drug targets of cancer drugs like dasa8nib."
    },
    {
      "header": "Comparing the system generated summary with a new human",
      "content": ""
    },
    {
      "header": "A. Procedure: Recall & Precision",
      "content": "Rouge-n, rouge -s and rouge-l are the granularity of texts that was compared between the system summaries and the reference or human annotated model summaries."
    },
    {
      "header": "VII. RESULTS & FINDINGS",
      "content": "The terms in the topic modelling show text which are mostly frequent in the document. These were depicted by the size of the circle (as seen in figuresand) Note that close topics such as topics 1, 2 and 3 are semandcally related."
    },
    {
      "header": "IX. DISCUSSION",
      "content": "The automadzadon of the task is gaining popularity among researchers. It can be integrated into any base model at intermediate stages to help reduce the length of the document for further analysis."
    },
    {
      "header": "X. CONCLUSION",
      "content": "Automated text summarizadon is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document. Future study will look into performing topic modelling with these documents and observe whether the approach retain the meaning."
    }
  ]
}
