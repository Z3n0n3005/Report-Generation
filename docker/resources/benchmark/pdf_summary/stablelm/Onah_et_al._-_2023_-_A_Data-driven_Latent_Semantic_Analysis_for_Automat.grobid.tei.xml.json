{"id": "Onah_et_", "name": "Onah_et_al._-_2023_-_A_Data-driven_Latent_Semantic_Analysis_for_Automat.grobid.tei.xml", "segments": [{"header": "I. INTRODUCTION", "content": "This study presents a novel approach to topic modeling by performing extracDE summaries on over 100 articles related to genes and associated diseases, feeding the summary as an input argument to a Latent Dirichlet Allocation (LDA) model to perform topic modeling, aiming to identify common themes across articles of the same genre describing a specific topic of interest in the research, while addressing journal articles retrieved from PubMed Central database discussing genes and their associated diseases."}, {"header": "II. RELATED WORK", "content": "The rapid increase in text data worldwide necessitates the effective extraction and meaningful presentation of crucial information, which is achieved through the process of text summarization, focusing on the most relevant portions while preserving the original meaning, as described in natural language understanding and processing, making it a well-known task; thus, ensuring the preservation of valuable data information."}, {"header": "A. Summariza-on", "content": "The segment describes two types of text summarization techniques in NLP, the extracursive and abstractive approaches, and their respective methods for condensing large texts while ensuring the main relevant information remains intact, with the extracursive approach prioritizing the top N sentences based on their score rankings for the summary generation, and the abstractive approach utilizing machine learning paradigms like deep learning for document summarization, focusing on the original document's content."}, {"header": "B. Topic Modelling", "content": "Topic modeling is an unsupervised machine learning technique for automatically extracting meaningful topics from collections of documents, allowing them to be labeled and describ"}, {"header": "C. Latent Dirichlet Alloca-on", "content": "LDA, a powerful topic modeling technique, was pioneered by Latent Dir"}, {"header": "III. METHODS", "content": "The summarizaDon model, employing LDA topic modeling technique and leveraging genes and diseases keywords search, was designed to scrape text data from PubMed journal database, resulting in the creation of a lexicon LDA model using a preprocessed dictionary of terms derived from summarised articles, which was then used to build the vectorised corpus for the LDA model, utilizing the 'pyLDAvis.gensim.prepare' method as a key approach."}, {"header": "A. Model Descrip-on", "content": "The segment describes that a significant portion of text processing tasks were accomplished using the Python Natural Languag"}, {"header": "1)", "content": "A scoring function is introduced to create a sentence score dictionary in the study, which holds the assigned values for each sentence."}, {"header": "Sentscores[S] = Wordfreq[W]", "content": "During the sentence model processing interval, the length of sentences is adjusted within the sentence scores dicDonary, potentially increasing or decreasing it, while new sentences are added if they exist in the dicDonary, or existing words are appended to the existing sentences if they are not present in the dicDonary, as per the equaDon 2 logic."}, {"header": "Sentscores[S]+ = Wordfreq[W]", "content": "The model automatically generates a word frequency corpus, which is then used to calculate the maximum word based on its prevalence or occurrence within the corpus, creating a dicDonary of word frequency, as depicted in Figure 4, and enabling the calculation of word frequencies using equaDon 4."}, {"header": "Word", "content": "The segment discusses"}, {"header": "B. Research Pipeline", "content": "The pipeline model for the research employs a sequential approach to efficiently gather and process medical science research data, extracting approximately 100 papers from the PubMed Central database using a combination of 'gene' and 'disease' search terms, focusing on papers related to diseases and mutated genes, ultimately providing a smooth and comprehensive dataset for analysis."}, {"header": "2)", "content": "The segment describes the process of preprocessing and feature extraction from a web-based dataset scraped from PubMed journal, which involved converting the raw, unstructured data into structured text documents, using NLP libraries like Beautiful Soup, regular expressions, lxml, tokenization, and NLTK, followed by parsing the source code of web articles to extract relevant textual material, filtering out special characters, and combining the extracted paragraphs into a single clean string for further topic modeling."}, {"header": "3)", "content": "We removed unnecessary stopwords from the processed articles, focusing on essential words for generating a concise summary as depicted in Figure X; where X represents the specific figure number"}, {"header": "4) Topic Modelling & Visualiza-on:", "content": "The study effectively identified and showcased the prevalence and relevance of terms within documents through the application of topic modeling with LDA, which was visualized using PyLDAvis, enabling interactive display of identified topics and their sizes, showcasing their significance.<|im_end|>\nTo come up with this summary, I read the given text and identified th"}, {"header": "IV. MODEL", "content": "The segment discusses the process of defining the significance of the semanDc term t for a specific topic n, taking into account the parameter weight (\u03bb) within th"}, {"header": "B. Defining Saliency Term", "content": "The study establishes a saliency term, denoted by 'w0', which represents the minimal probability derived from a topic model (LDA) for a given word 'w0', while also calculating the marginal probability 'P(TM)' and assessing the uniqueness of identified words 'w0' by comparing their occurrences in the topic model with their probability distribution in the original text data."}, {"header": "U(6)", "content": ""}, {"header": "=5", "content": "The uniqueness of terms in a topic network is characterized by their significance and semantic association to various subjects, with larger topic circles and lower inter-topic distances indicating a more cohesive network, while words expressed in multiple topics can provide insights into the mixture of semantic associations; specifically, a word's rarity in certain topics might be reflected in its low uniqueness score in the computation."}, {"header": "V. LATENT SEMANTIC ANALYSIS", "content": "Latent Semantic Analysis (LSA) is a powerful computational technique that identifies and represents hidden semantic structures within words and sentences, enabling the extraction of features that cannot be directly represented within the datas"}, {"header": "A. Sample Extracted Summary", "content": "The segment describes the process of selecting the most useful sentences for a summary using a heap queue (heapq) library, where sentences with higher word frequencies are given higher processing priority, and a threshold is set to determine the number of sentences to summarize, as illustrated in Table X, where X represents the specific table number.<|im_end|>\nTo create a summary using"}, {"header": "B. Findings", "content": "The study's intriguing findings on genes linked to Cancer and Type 2 diabetes, as presented in Table form, have emerg"}, {"header": "VI. ROUGE: RELIABILITY & VALIDITY OF MODEL", "content": "ROUGE is a metric for evaluating document summaries as it focuses on Recall, being oriented towards Gisting Evaluation, and is an intrinsic"}, {"header": "TABLE III ROUGE METRICS MEASUREMENT SUMMARIES", "content": "The BCAA metabolic genes MLYCD, HADHB, IVD, MUT, and PCCB, which are highly ranked by Hridaya, are also prominently featured in System and Human Annotated Summaries, utilizing 181 features primarily categorized as gene8c."}, {"header": "Comparing the system generated summary with a new human", "content": ""}, {"header": "A. Procedure: Recall & Precision", "content": "In this study, precision and recall are assessed using the ROUGE metrics to evaluate the overlap between human-annotated reference summaries and automated machine summaries, focusing on measuring the system's ability to recover relevant content while minimizing unnecessary verbosity, with F1-score and precision-recall curves providing insights into the trade-off between precision and recall, and ROUGE metrics revealing the fluency of summaries at different granularity levels."}, {"header": "VII. RESULTS & FINDINGS", "content": "The topic modeling reveals text segments with significant frequency, represented by the size of the circle, illustrating semantic relationships among closely related topics, as exemplified by Figures 1 and 2, where the distance between topics approximates their semantic connection, particularly for topics 1, 2, and 3.<|im_end|>\nTo come up with this summary, I"}, {"header": "IX. DISCUSSION", "content": "The study presents a data-driven automated text summarization model that achieves comparable results to current state-of-the-art topic modeling techniques without relying on linguistic models, using unstructured datasets for evaluation."}, {"header": "X. CONCLUSION", "content": "AutomaDc summarizaDon efficiently condenses a text document into a summary, preserving the most crucial information from the original document. \n\nStep "}]}