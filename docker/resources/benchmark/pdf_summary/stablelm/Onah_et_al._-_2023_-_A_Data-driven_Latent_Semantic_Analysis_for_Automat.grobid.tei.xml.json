{"id": 3831095148674258050, "name": "Onah_et_al._-_2023_-_A_Data-driven_Latent_Semantic_Analysis_for_Automat.grobid.tei.xml", "segments": [{"header": "I. INTRODUCTION", "content": "This study presents a novel approach to topic modeling by performing extracuded summarization on journal articles related to genes and associated diseases, using a Latent Dirichlet Allocation (LDA) model to identify common themes among documents of the same genre, and demonstrating its effectiveness in summarizing and modeling the content of the articles, achieving good performance in both tasks."}, {"header": "II. RELATED WORK", "content": "The rapid growth of text data worldwide necessitates the extraction and meaningful interpretation of valuable information, which is effectively summarized using natural language understanding and processing techniques, ensuring clarity and conciseness while preserving the original meaning, and addressing the need for automated collection and summarization of increasingly available web text data for efficient information retrieval and sustainability in the era of big data."}, {"header": "A. Summariza-on", "content": "Automated summarization, a technique in NLP, condenses large texts into smaller summaries while preserving key elements and meaning, integrating it into various NLP applications to reduce document size while retaining original information, employing two approaches: extracive and abstractive, with the extracive approach focusing on direct object summarization and utilizing unsupervised machine learning in deep learning models for document summarization, while abstractive models consider a bottom-up summary approach for documents with the same vocabulary as the original document."}, {"header": "B. Topic Modelling", "content": "Topic modeling is the unsupervised machine learning technique for abstracting topics from collections of documents, achieved through an advanced modeling process that describes documents as weighted graphical representations, and then extrapolating these topics from unstructured datasets using libraries such as SciKit-Learn and Gensim, with the 0 gensim.models.ldamodel.ldamodel 0 input argument including the text corpus, the desired number of topics, and the id2word dictionary for preprocessed documents."}, {"header": "C. Latent Dirichlet Alloca-on", "content": "Latent Dirichlet Allocation (LDA) is a topic modeling technique introduced by Dirichlet Allocation (DA) that generates topics based on the probability distribution of terms within documents, considering the possibility of discontiguous topics and the co-occurrence of words across multiple documents, leading to the construction of a probability topic model for each mixture of topics in a document."}, {"header": "III. METHODS", "content": "The summarizadon model was developed to scrape text data from the Pubmed journal database using genes and diseases keywords search, extracting and processing 100 papers from the Pubmed Central repository, applying Natural Language Processing techniques for inidal preprocessing, generating word frequency and high sentence scores, creating sentence vectors, performing cosine similarity scores, and utilizing an LDA topic modeling approach to create a dictionary of terms for lexicon-based corpus construction, which was then used to build a vectorized lexicon LDA model for topic projection."}, {"header": "A. Model Descrip-on", "content": "The study utilized the Python Natural Language Toolkit (NLTK) to process text, including tokenization and topic modeling, to extract information from numerous journal articles related to diseases and genes, generating natural language summaries without significant linguistic connotation or analysis, employing a supervised summarization technique that scores sentences based on predefined conditions and maximizes the likelihood of sentence consideration from the input document, which can be generalized to various types of articles, such as media, blogs, and news."}, {"header": "1)", "content": "The study introduces a scoring function to generate a sentence score dictionary, determining the probability of inclusion in the summary based on sentence relevance and selecting the top n sentences with the highest scores for the final document summary, revealing the importance of specific sentences in the document retrieval process."}, {"header": "Sentscores[S] = Wordfreq[W]", "content": "During the sentence model processing interval, the length of sentences is adjusted within the sentence scores dictionary, potentially adding new sentences if they exist, and if not, adding the corresponding word from the word frequencies dictionary to the sentence in the sentence scores dictionary, adhering to the rules specified in Equadon 2."}, {"header": "Sentscores[S]+ = Wordfreq[W]", "content": "The word frequency in the model's corpus is automatically determined based on word prevalence, with a limited sentence selection of less than 30 words, where the maximum weighted frequency (freqmax) is calculated by multiplying word frequencies (wfreq) with values (v), which are then added to the final summary, allowing for the calculation of the maximum word in the word frequencies (see Equations 3 and 4) in cases where less than 30 words are selected."}, {"header": "Word", "content": "In the prov"}, {"header": "B. Research Pipeline", "content": "The pipeline model for this research employs a sequential series of processes to efficiently and smoothly retrieve information, as exemplified by the use of the sequential approach within the figure to collect, preprocess, clean, and summarize data from approximately 100 papers related to medical science research extracted from the PubMed Central (PMCID) database, focusing on papers related to diseases and mutated genes, which were then subjected to topic modeling for the study."}, {"header": "2)", "content": "The segment describes the process of preprocessing and feature extraction from a web-based dataset scraped from Pubmed journal, which involved converting the raw, unstructured data into structured text documents, utilizing natural language processing (NLP) libraries like BeautifulSoup, regular expressions, and NLTK, parsing the source code of web articles to extract textual material within paragraph tags, filtering out special characters, and combining the extracted paragraphs to form a clean web content for further topic modeling processing."}, {"header": "3)", "content": "The process involved filtering out a list of non-essential stopwords from the processed articles, leaving only essential words for the final summary, as exemplified by the provided figure showcasing the stop-word lis"}, {"header": "4) Topic Modelling & Visualiza-on:", "content": "The study identified and visualized terms' prevalence and relevance within the documents using LDA topic modeling and PyLDA, which displays topic circles based on the size of word circles and the number of common words among sentences, revealing semantic relationships among them."}, {"header": "IV. MODEL", "content": "We quantify the semantic significance of term T for topic N, considering the weight parameter \u03bb within the range [0, 1], where P_t denotes the minimal probability of term T in the lexical corpus, and nt is the probability of term T occurring among 1 to n elements in the vocabulary (including n), with \u03bb reflecting the relative importance assigned to the probability of terms T within topic N, adhering to the Equations 1 and 2 provided."}, {"header": "B. Defining Saliency Term", "content": "The study defines a saliency term as determining the minimal probability of a word 'w' given a topic model, while also computing the marginal probability and calculating the uniqueness of each identified word 'w' within the context of latent Dirichlet allocation (LDA) topic modeling, resulting in the computation of 5 topics (t) and 10 passes, which were selected from the LDA topic modeling process, as described in Equation 6."}, {"header": "U(6)", "content": ""}, {"header": "=5", "content": "The uniqueness of each term in a topic network is assessed by its significance and semantically associated significance to the topics, with adjustments to the lambda metric aiding in significant classification and simplifying topic distributions, revealing patterns in terms expressed in multiple topics, and highlighting saliency measures that reflect topic association and composition, such as prevalence of common terms like genes, diseases, and expressions."}, {"header": "V. LATENT SEMANTIC ANALYSIS", "content": "Latent Semantic Analysis (LSA) is a powerful unsupervised method that identifies hidden semantic structures within words and sentences, enabling extraction of essential but non-original features for data abstraction, while also utilizing natural language processing (NLP), resulting in an efficient technique for abstracting the contextual content of documents, and was applied to summarize the original 100 articles from the Pubmed database, yielding a robust summary with valuable insights on specific genes associated with cancer disease, which were further visualized using a word cloud."}, {"header": "A. Sample Extracted Summary", "content": "The most prevalent sentences, determined by utilizing a heap queue (heapq) library for prioritizing word frequencies, are selected for summarization, with the threshold indicating the number of sentences to summarize, and different threshold points are employed to produce varying summaries based on the selected word frequency, even when the frequency is below the maximum of 30 occurrences (< 30)."}, {"header": "B. Findings", "content": "The segment discusses intriguing findings from a summary that associate specific genes with cancer and type 2 diabetes, as displayed in a table ("}, {"header": "VI. ROUGE: RELIABILITY & VALIDITY OF MODEL", "content": "Rouge, a metric evaluation model focused on recall-oriented assessment for information retrieval, is based on the Bilingual Evaluation Understudy (BLEU) metric and assesses the accuracy of document summaries by comparing system-generated summaries to human-created summaries, utilizing measures such as overlapping units of word n-grams, bi-grams, and word pairs, and is utilized to verify the reliability and validity of a model in the context of natural language processing applications."}, {"header": "TABLE III ROUGE METRICS MEASUREMENT SUMMARIES", "content": "The human-annotated summaries of genes in the BCAA metabolic pathway, such as MLYCD, HBAB, IVD, MUT, and PCCB, are highly ranked by Hridaya, based on 181 features grouped into various dimensions, and the human summaries closely align with the automated system summary, resulting in an average recall, precision, and F1 score over 83%, 85%, and 84%, respectively, as reviewed in Table V."}, {"header": "Comparing the system generated summary with a new human", "content": ""}, {"header": "A. Procedure: Recall & Precision", "content": "In this study, the precision and recall metrics were combined to evaluate the system-generated summaries, focusing on measuring the accuracy and relevance of the generated summaries while considering the granularity of texts compared with human-annotated reference summaries, aiming to optimize the F1-score measure and assess the fluency of summaries."}, {"header": "VII. RESULTS & FINDINGS", "content": "The topic modeling results, represented by the size of the circle in scatter plots, show the frequency of terms in the document, with larger circles indicating more frequent terms, and a scatter plot using the scaver plot reveals the distance, distribution, and relationship between topic levels, highlighting semantically related topics and the ranking of terms based on relevance metric \u03bb, while also noting the influence of themes on the summary predictions and the longer evaluation time for large text data in the topic modeling approach."}, {"header": "IX. DISCUSSION", "content": "The study presents a data-driven approach for automated text summarization, achieving comparable results to current state-of-the-art topic modeling techniques without relying on linguistic information models, making manual summarization a challenging yet essential task, and its application in various NLP tasks such as text analysis, classification, question answering, financial and legal texts summarization, news summarization, and reviewing of news headlines, with potential integration into any base model at intermediate stages for document reduction and further analysis."}, {"header": "X. CONCLUSION", "content": "The study presents a fully automated, single and multiple document text summarization approach that accurately extracts key points from texts while preserving overarching meanings and purposes, and aims to translate foreign language documents into English while retaining their original content, with a focus on cross-language text summarization and comparing results with existing machine learning gene prediction models."}]}