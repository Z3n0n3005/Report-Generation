{"id": "Urteaga_", "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "The multi-armed bandit problem in sequential decision making, where arms are chosen based on maximizing cumulative rewards in various real-world scenarios, is extended to incorporate contextual information using sequential Monte Carlo methods for time-varying and nonlinear reward functions, leading to a flexible SMC-based framework for solving non-stationary and nonlinear MABs, which accommodates complex reward models and leverages SMC for both posterior sampling and estimation of sufficient statistics utilized by Bayesian MAB policies."}, {"header": "Background and preliminaries", "content": "The MAB (Maximizing Average Reward) framework extends the exploration-exploitation tradeoff in sequential decision-making, specifically focusing on arms in the bandit literature. It models the stochastic reward process with uncertainty captured by \u03b8 \u2208 \u0398, where \u03b8 * t denotes the union of all, per-arm, parameters at time t. The MAB formulation covers stationary and non-contextual bandits, allowing for time-varying context and parameter updates via the Bayesian approach. The optimal action is determined by maximizing the expected reward expectation, considering the uncertainty induced by the true model parameters. Various MAB policies, such as Thompson sampling and Bayes-UCB, have been proposed to address the exploration-exploitation tradeoff, but these are limited to specific reward functions and time-evolving models. The SMC-based MAB framework extends these solutions to accommodate more flexible reward functions, non-stationary bandit environments, and a modular approach to Bayesian MAB algorithms."}, {"header": "SMC for multi-armed bandits", "content": "The SMC-based Bayesian MAB framework combines Sequential Monte Carlo for computing posterior and sufficient statistics, a state-space model for non-stationary bandits with complex nonlinear reward functions, and a sequential updating approach that avoids assumptions on model parameter knowledge, while combining Thompson sampling and Bayes-UCB policies for non-stationary bandits, specifically for non-stationary bandits with linear dynamical systems and non-Gaussian stochastic innovations."}, {"header": "Evaluation", "content": "The empirical evaluation of a SMC-based Bayesian MAB framework in non-stationary bandit scenarios with continuous, binary, and discrete-categorical reward distributions demonstrates satisfactory performance, capturing non-stationary trends in personalized news article recommendations, while also illustrating the flexibility of SMC-based Thompson sampling in handling unknown model parameters and achieving sublinear cumulative regret in challenging cases."}, {"header": "Conclusion and discussion", "content": "The authors propose a SMC-based Bayesian MAB framework that extends Bayesian MAB policies to accommodate nonlinear and time-varying models, allowing for interpretable modeling of time-evolving reward functions and sequential learning of sufficient statistics and dynamics to find the optimal exploration-exploitation balance, resulting in improved cumulative regret performance in simulated and practical scenarios."}]}