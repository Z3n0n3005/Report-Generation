{
  "id": 1433723314581187940,
  "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content": "The multi-armed bandit (MAB) problem in sequential decision making, where arms adapt to changing reward distributions, is extended to contextual MABs, encompassing time-varying reward functions and nonlinear reward models, and addressed through the Sequential Monte Carlo (SMC) method, enabling flexible and accurate computation of statistics for Bayesian MAB policies, addressing challenges in non-stationary and nonlinear MABs, and offering a SMC-based MAB framework for restless and nonlinear reward models."
    },
    {
      "header": "Background and preliminaries",
      "content": "The MAB (Maximizing Average Reward) framework encapsulates the trade-off between exploration and exploitation in sequential decision-making, specifically for multi-armed bandits with stochastic reward and time-varying context. It addresses the uncertainty in the true reward-generating distribution by formulating the problem as maximizing rewards observed from sequentially chosen actions, while accommodating time-varying context and parameters via the subscript t. Various MAB policies, such as \u03f5-greedy, Bayes-UCB, and Thompson Sampling, have been proposed to overcome the exploration-exploitation tradeoff, with different approaches to incorporating uncertainty and adapting to changing environments. These methods aim to maximize the attainable rewards while minimizing the cumulative regret, considering the stochastic nature of the problem and the agent's uncertainty about the true optimal action."
    },
    {
      "header": "SMC for multi-armed bandits",
      "content": "Bayesian posterior distributions for non-stationary, stochastic multi-armed bandits with complex nonlinear reward functions, as we model them using Sequential Monte Carlo (SMC) in a state-space framework, enabling the computation of sufficient statistics and posterior filtering for a rich class of models, including non-stationary bandits with stateless and context-dependent nonlinear reward functions, subject to non-gaussian stochastic innovations, while adhering to the standard MAB formulation and using Thompson Sampling and Bayesian UCB policies. The SMC-based posterior random measure p m (\u03b8 t,a |h 1:t ) is crucial for sequential updates and propagation of parameter posteriors per-arm, enabling accurate approximations to the true posterior density with high probability, even for time-evolving bandits, and extends the applicability of Bayesian MAB policies from stationary to dynamic systems."
    },
    {
      "header": "Evaluation",
      "content": "Empirical evaluation of the SMC-based Bayesian MAB framework in non-stationary bandit scenarios with continuous, binary, and discrete-categorical reward distributions reveals satisfactory performance across a wide range of stationary bandit parameterizations and sizes, as SMC-based policies achieve the right exploration-exploitation tradeoff, demonstrating adaptability to changes in the identity of the optimal arm and achieving sublinear regret curves even in scenarios with unknown reward variances."
    },
    {
      "header": "Conclusion and discussion",
      "content": "The authors propose a sequential Monte Carlo (SMC)-based Bayesian Multi-Armed Bandits (MAB) framework that extends Bayesian MAB policies, including Thompson Sampling and Bayesian UCB, to handle elusive bandit environments with nonlinear and time-varying models, while accurately estimating the sufficient statistics and dynamics of the bandit from online data to find the optimal exploration-exploitation balance, leading to improved cumulative regret performance in practical scenarios."
    }
  ]
}
