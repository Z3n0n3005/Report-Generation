{"id": 6949827695286197266, "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "Transformer models can achieve smaller compute budgets by intelligently allocating compute resources through techniques like mixture-of-depths (mod) that prioritize static computation graphs and known tensor sizes, allowing for efficient language modeling without sacrificing overall performance and potentially achieving faster training with a smaller flops per forward pass."}, {"header": "Background", "content": "Transformer architectures, pioneered by Bengio (2013), have revolutionized practical AI with high capabilities at a cost of expensive training and serving procedures, leading to increased interest in optimizing efficiency through conditional computation methods like early-exit and adaptive iteration, as well as the development of methods like the \"mixture-of-experts\" layer for fine-tuning and conditional routing, with the potential to reduce total compute expenditure while maintaining computational efficiency."}, {"header": "Implementing Mixture-of-Depths Transformers", "content": "The high-level strategy for using a smaller compute budget in a transformer includes setting a static compute budget less than that of a vanilla transformer, limiting the number of tokens in a sequence that can participate in a block's computations, employing a per-block router to emit scalar weights for each token expressing the router's preference for participation, identifying the top-\ud835\udc58 scalar weights to select tokens that will participate in block computations, enforcing a total compute budget per forward pass using the notion of capacity, and leveraging learned routing schemes to optimize the model's performance, with expert-choice routing being preferred due to its load balancing properties and the ability to route tokens based on their preferences."}, {"header": "Results", "content": "Optimal mod transformer hyperparameters, determined through training with a small budget, result in models with lower loss and fewer parameters, while also offering faster training and better performance in autoregressive sampling. The best mod variant has the option to route every other block and uses a top-k of 256, while smaller mod variants can perform as well or better than the isoflop-optimal baseline and are faster to step. Empirical studies suggest that adding depth rather than width to mod models can lead to memory savings and improved performance, and the optimal capacity appears to be 12.5% for interleaved routing blocks. Mod transformers can be integrated with mode models and offer significant compute savings, particularly during autoregressive sampling. Switching from non-causal top-\ud835\udc58 routing to a causal predictor-based approach during auto-regressive evaluation leads to minimal performance degradation."}, {"header": "Discussion", "content": "Mixture-of-depths Transformers demonstrate the potential for improving both speed and performance with models that utilize fewer flops per forward pass, allowing for faster and better-performing models within a given training flop budget, while also considering learned routing decisions to optimize flops usage and decoupling routing for queries, keys, and values in mod variants."}]}