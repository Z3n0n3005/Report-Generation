{"id": "Raposo_e", "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "MoD transformers, akin to Mixture of Experts, dynamically allocate compute budget by making token-level routing decisions across the network depth, allowing for efficient resource utilization and trade-offs between performance and speed, achieving equal or better log probabilities per sequence with a smaller FLOP footprint per forward pass."}, {"header": "Background", "content": "The transformer architecture, through its efficiency improvements and widespread adoption in AI, has spurred research to enhance its capabilities while minimizing resource-intensive training and inference processes, leading to advancements in conditional computation methods and the development of predictive routers for efficient inference in transformer-based models like CoLT5 MoD, specifically focusing on the decoder-only setting, as exemplified by the \"mixture-of-experts\" layer introduced by researchers."}, {"header": "Implementing Mixture-of-Depths Transformers", "content": "The high-level strategy for using a smaller compute budget in a transformer-based model involves setting a static compute budget less than that of a vanilla transformer, limiting the number of tokens in a sequence that can participate in a block's computations, and using a per-block router to emit a scalar weight for each token expressing the router's preference for that token to participate in a block's computations or to route around it. The top-\ud835\udc58 scalar weights are then used to identify tokens that will participate in a block's computations, while downstream performance is affected by how aggressively the blocks' capacities are shrunk and the routing algorithm implemented. The authors hypothesize that learned routing is preferable, as the network can learn which tokens require more or less processing than others, and the network can preserve its performance by routing tokens to one of two computational paths: (1) self-attention and MLP blocks, and (2) a residual connection."}, {"header": "Results", "content": "The development of MoD (Modified Dot Product Transformers) models, which are faster to step and achieve comparable performance to isoFLOP optimal baselines, is achieved by employing routing strategies, adjusting capacity, and optimizing hyperparameters, while also revealing patterns in token engagement and correlation with output predictions' entropy."}, {"header": "Discussion", "content": "Mixture-of-Depths transformers demonstrate the potential for improving model performance with fewer FLOPs per forward pass, allowing faster and better-performing models without overtraining smaller models, while leveraging learned routing decisions to efficiently allocate FLOPs for self-attention and subsequent MLPs, enabling the decoupling of routing for queries, keys, and values, and offering insights for long-term memory systems that optimize computational efficiency while considering the influence of tokens on both current and future predictions."}]}