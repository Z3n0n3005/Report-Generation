{"id": "Raposo_e", "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "We leverage an approach akin to Mixture of Experts (MoE) transformers, in which dynamic token-level routing decisions are made across the network depth. In our implementation total compute is user defined and unchanging prior to training, rather than being a function of the network's on-the-fly decisions."}, {"header": "Background", "content": "The transformer architecture has become the workhorse of a revolution in practical artificial intelligence, bringing unprecedented capabilities at the cost of expensive training runs and serving procedures. A wide variety of recent work has developed conditional computation methods for transformers."}, {"header": "Implementing Mixture-of-Depths Transformers", "content": "Set a static compute budget that is less than that of an equivalent vanilla transformer. Limit the number of tokens in a sequence that can participate in a block's computations (i.e., selfattention and subsequent MLP) The computation graph and tensor sizes remain static throughout training."}, {"header": "Results", "content": "We first trained models with a relatively small FLOP budget (6e18) to determine optimal hyperparameters (see figure the isoFLOP optimal baseline (also 220M, figure 3 model #1), but is upwards of 60% faster to step during training. When run on equivalent hardware these two model variants take take approximately the same amount of wall-clock time to train."}, {"header": "Discussion", "content": "Learned routing mechanisms are sometimes non-causal; that is, information about the future is used to determine a given token's routing decision. This is generally true for top-k routing mechanisms, which forego the need for auxiliary balancing losses."}]}