{"id": 5097872631358280427, "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "In language modeling not all tokens and sequences require the same time or effort to accurately make a prediction. transformer models expend the same amount of compute per token in a forward pass. General formulations of this challenging problem may not work well with existing hardware constraints since they tend to introduce dynamic computation graphs."}, {"header": "Background", "content": "Mixture-of-depths (mod) transformers have a smaller total flop footprint compared to vanilla or moe transformers. Instead of having multiple experts, mod deploys a single expert which can be dynamically skipped."}, {"header": "Implementing Mixture-of-Depths Transformers", "content": "token-choice routing can have load balancing problems since there isn't a guarantee that tokens divide themselves appropriately between the possible paths. The top-ùëò operation is non-causal. We experimented with versions where the router weights are also included along the computational path for those tokens that bypass the block's computations."}, {"header": "Results", "content": "variants of the mod transformer were trained for 6e18 flops to determine the optimal hyperparameters for further isoflop analyses. When run on equivalent hardware these two model variants take take approximately the same amount of wall-clock time to train. We tested routing every block or every other block, using capacities from 12.5% to 95%."}, {"header": "Discussion", "content": "Mixture-of-depths transformers demonstrate that one can improve on isoflop-optimal baseline performance with models that use fewer flops per forward pass. For a given training flop budget-we can train models that are both faster and better performing than their baseline counterparts."}]}