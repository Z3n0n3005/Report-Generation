{"id": 7919445899046386220, "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "The multi-armed bandit (mab) problem considers the sequential strategy one must devise when playing a row of slot machines. The 'context' is the physiology of the patient, the type of resources available for each project, or the features of the website user."}, {"header": "Background and preliminaries", "content": "The mab crystallizes the fundamental trade-off between exploration and exploitation in sequential decision making. It formulates the problem of maximizing rewards observed from sequentially chosen actions when interacting with an uncertain environment. It can be viewed as different approaches to a bayesian formulation of the mab problem."}, {"header": "SMC for multi-armed bandits", "content": "We use sequential monte carlo to compute posteriors and sufficient statistics of interest for a rich-class of mabs. We model non-stationary, stochastic mabs in a state-space framework. We allow for such parameters to evolve independently per-arm in time. We extend bayes-ucb to reward models where the quantile functions are not analytically tractable, by leveraging the smc-based parameter predictive posterior random measure."}, {"header": "Evaluation", "content": "Policies that compute and use smc random measure posteriors incur in minimal regret loss in comparison to the optimal kalman filterbased agent.notice how in figuresregret increases when the optimal arms swap (as shown in figures)"}, {"header": "Conclusion and discussion", "content": "Smc-based bayesian mab framework allows for interpretable modeling of nonlinear and time-evolving reward functions. It learns the sufficient statistics and dynamics of the bandit from online data, to find the right exploration-exploitation balance."}]}