{"id": "Urteaga_", "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "The multi-armed bandit (MAB) problem considers the sequential strategy one must devise when playing a row of slot machines. The contextual MAB, where at each interaction with the world side information (known as 'context') is available, is a natural extension of this."}, {"header": "Background and preliminaries", "content": "The MAB crystallizes the fundamental trade-off between exploration and exploitation in sequential decision making. It formulates the problem of maximizing rewards observed from sequentially chosen actions. The SMC-based MAB framework we present generalizes existing Bayesian MAB policies."}, {"header": "SMC for multi-armed bandits", "content": ""}, {"header": "Evaluation", "content": ""}, {"header": "Conclusion and discussion", "content": "The proposed SMC-based Bayesian MAB framework allows for interpretable modeling of nonlinear and time-evolving reward functions. It sequentially learns the sufficient statistics and dynamics of the bandit from online data, to find the right exploration-exploitation balance."}]}