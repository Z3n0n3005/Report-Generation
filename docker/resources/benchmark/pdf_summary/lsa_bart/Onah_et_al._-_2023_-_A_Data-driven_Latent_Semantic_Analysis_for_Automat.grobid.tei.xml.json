{"id": "Onah_et_", "name": "Onah_et_al._-_2023_-_A_Data-driven_Latent_Semantic_Analysis_for_Automat.grobid.tei.xml", "segments": [{"header": "I. INTRODUCTION", "content": "This study presents a novel approach to topic modelling by performing extracDve summarizaDon on over 100 arDcles related to genes and associated diseases. It uses a Latent Dirichlet AllocaDon (LDA) model in order to perform the topic modelling."}, {"header": "II. RELATED WORK", "content": "Text summarizaDon is a well-known task in natural language understanding and processing. Summarize the following segment into 1 sentence: The amount of text data being produced worldwide is enormous and growing rapidly."}, {"header": "A. Summariza-on", "content": "SummarizaDon is a technique in NLP that is used for condensing or summarising huge texts into smaller versions. Extrac-ve approach considers the top N sentences based on their score rankings for the summary generaDon. Abstrac-ve technique follows the convenDon of unsupervised approach where machine learning paradigms such as deep learning plays a big role."}, {"header": "B. Topic Modelling", "content": " Topic modelling is the process of labelling and describing documents into topics. This is an unsupervised machine learning technique for abstracDng topics."}, {"header": "C. Latent Dirichlet Alloca-on", "content": "Latent Dirichlet AllocaDon (LDA) is a technique applied in topic modelling. LDA was introduced by the group of researchers at the University of California, Berkeley."}, {"header": "III. METHODS", "content": "The study was designed to apply a generalised concept of LDA topic modelling technique to create a dicDonary of terms that was fed from the summarised arDcles. This dic donary was used to build a vectorised corpus of lexicon LDA model."}, {"header": "A. Model Descrip-on", "content": "Most of the processing of the text was performed with the python Natural Language Toolkit (NLTK) Summarize the following segment into 1 sentence:"}, {"header": "1)", "content": "A scoring funcDon is introduced to generate the sentence score dicDonary which hold the value assigned to each sentence. Summarize the following segment into 1 sentence: Sentence scoring method."}, {"header": "Sentscores[S] = Wordfreq[W]", "content": "During the sentence model processing interval, the length of the sentence is either increased or reduced by certain values within the sentence scores dicDonary. New sentences are added into the sentence dic donary scores."}, {"header": "Sentscores[S]+ = Wordfreq[W]", "content": "The word frequencies were selected automaDcally based on the prevalence or occurrence of the words in the corpus dicDonary created in the model. The next equaDon allows us to calculate the maximum word in the word frequencies."}, {"header": "Word", "content": ""}, {"header": "B. Research Pipeline", "content": "Papers related to diseases and the mutated genes causaDon were extracted for this study. The pipeline model for the research follows a sequenDal approach of processes that could allow the smooth and efficient information retrieval."}, {"header": "2)", "content": "The preprocessing involved converDng the dataset into text documents using NLP packages. In the feature extracDon process, we parse the web arDcles source code in order to extract the textual material needed for the final summary. These extracted paragraphs text are combined to form a single string to store the clean web content for further topic model processing."}, {"header": "3)", "content": "Words such as pronounce that are not necessary or essenDal for the final summary. Stopwords: We further removed a list of stop-words from the propocessed arDcles."}, {"header": "4) Topic Modelling & Visualiza-on:", "content": "The result was visualised using PyLDAvis which is a web-based interacDve visualizaDon package that allows the display of the topics that were idenDfied using the LDA approach."}, {"header": "IV. MODEL", "content": "Defining seman-c significance we define the semanDc significance of term t to the topic n given the parameter weight. Summarize the following segment into 1 sentence:"}, {"header": "B. Defining Saliency Term", "content": "In this study we define saliency term as given a word 'w 0 , we compute its minimal probability P(TM/w).where TM is the topic model. The possibility that the emerge word w was generated from the LDA topic model (TM)"}, {"header": "U(6)", "content": ""}, {"header": "=5", "content": "The uniqueness of each term is described as how significance and semanDcally associated they are to the topics. The frequency and populaDon of terms are denoted by the size of the topic circles and also the inter-topic distance."}, {"header": "V. LATENT SEMANTIC ANALYSIS", "content": "LSA is a robust Algebraic and StaDsDcal method which extracts hidden semanDc structures of words and sentences. It is used to extract features that cannot be directly menDoned within the dataset."}, {"header": "A. Sample Extracted Summary", "content": "We used the heap queue (heapq) library to select the most or very useful sentences. The sentences with the most prevalence sentence score was used for the summary together."}, {"header": "B. Findings", "content": " genes that are associated to some Cancerous and type 2 diabetes diseases (see Table) The summary result has revealed very interesDng findings."}, {"header": "VI. ROUGE: RELIABILITY & VALIDITY OF MODEL", "content": "ROUGE is a metric evaluaDon model which stands for Recall Oriented Understudy for Gisting Evaluation. It is an intrinsic metric for automaDcally evaluaDng document summaries."}, {"header": "TABLE III ROUGE METRICS MEASUREMENT SUMMARIES", "content": "Some of the genes in the BCAA metabolic pathway such as MLYCD (rank 164)HADHB (rank 354)IVD (rank 713)MUT (rank 921)and PCCB (rank 684) are also ranked highly."}, {"header": "Comparing the system generated summary with a new human", "content": ""}, {"header": "A. Procedure: Recall & Precision", "content": "In considering the individual words in a sentence we simply represent this with the formula in equaDon 8. The metric will produce a perfect result of 1 which usually will be the case if indeed the sentence matches."}, {"header": "VII. RESULTS & FINDINGS", "content": "The terms in the topic modelling show text which are mostly frequent in the document. These were depicted by the size of the circle (as seen in Figures) Distance between two or more topics is an approximaDon of their semanDc relaDonship."}, {"header": "IX. DISCUSSION", "content": "We presented a fully data-driven approach for automaDc text summarizaDon. We proposed and evaluated the model on unstructured datasets which show some results comparable to the current state-of-the-art topic modelling techniques."}, {"header": "X. CONCLUSION", "content": "AutomaDc summarizaDon is the process of reducing a text document with a computer program. The goal is to create a summary that retains the most important points of the original document."}]}