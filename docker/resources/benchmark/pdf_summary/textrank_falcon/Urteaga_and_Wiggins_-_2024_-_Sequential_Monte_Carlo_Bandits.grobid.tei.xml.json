{
  "id": 5749778053033361322,
  "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content": "-armed bandit (mab) problem considers the sequential strategy one must devise when playing a row of slot machines: i.e., which arm to play next to maximize cumulative returns . this analogy extends to a wide range of real-world challenges that require online learning, while simultaneously maximizing some notion of reward . the contextual mab, where at each interaction with the world side information is available, is a natural extension of this abstraction ."
    },
    {
      "header": "Background and preliminaries",
      "content": "b policy (a|h 1:t ) aims at maximizing its cumulative rewards, or equivalently, minimizing its cumulative regret (the loss incurred due to not knowing the best arm a * t at each time t), i.e., r t = t t=1 y t,a* t ( t) , at , where a t denotes the realization"
    },
    {
      "header": "SMC for multi-armed bandits",
      "content": "puting the posteriors in equations (), () and () for a variety of mab models, for which smc enables us to accommodate (i) any likelihood function that is computable up to a proportionality constant, and (ii) a time-varying model described by a transition density from which we can draw samples ."
    },
    {
      "header": "Evaluation",
      "content": "-based bayesian policies achieve the right exploration-exploitation tradeoff . we present results for a variety of mab environments with reward functions detailed in sections 4.1, 4.2 and 4.3 . the main evaluation metric is the cumulative regret of the bandit agent, as defined in equation ."
    },
    {
      "header": "Conclusion and discussion",
      "content": "proposed smc-based bayesian mab framework allows for interpretable modeling of nonlinear and time-evolving reward functions . we show that sc-basic random measures are accurate enough for Bayesian bandit policies to find satisfactory exploration-exploitation tradeoffs . this is a theoretical analysis of the dependency between the dynamic bandit model's characteristics, the resulting rate of optimal arm changes, and in turn, update the explorable balance."
    }
  ]
}
