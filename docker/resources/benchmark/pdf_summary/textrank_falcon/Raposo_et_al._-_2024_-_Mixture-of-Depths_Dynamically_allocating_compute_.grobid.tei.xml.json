{
  "id": "Raposo_e",
  "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content":  "Ideally, transformers would not spend compute unnecessarily .Conditional computation is a technique that tries to reduce total compute by expending it only when needed . The network must learn how to dynamically allocate the available compute by making decisions per-token, in each layer, about where to spend compute ."
    },
    {
      "header": "Background",
      "content":  "The transformer architecture has become the workhorse of a revolution in artificial intelligence . This has spurred tremendous interest in making transformer architectures more efficient A wide variety of recent work has developed conditional computation methods for transformers . Some of this work focuses on learning to decide when to end computation on a given token ."
    },
    {
      "header": "Implementing Mixture-of-Depths Transformers",
      "content":  "Set a static compute budget that is less than that of an equivalent vanilla transformer . For example, the self-attention and MLP in each vanilla transformer block have a capacity of T-the total number of tokens across the sequence and batch . However, since they use multiple experts per block, their total capacity is approximately equal to that of a vanilla transformer."
    },
    {
      "header": "Results",
      "content":  "We first trained models with a relatively small FLOP budget (6e18) to determine optimal hyperparameters . When run on equivalent hardware these two model variants take approximately the same amount of wall-clock time to train . It seems the network is robust to significant capacity reductions as long as there is frequent opportunity for full capacity self-attention ."
    },
    {
      "header": "Discussion",
      "content":  "Learned routing could be a powerful mechanism for deciding which tokens these might be, perhaps funnelling them into a long-term memory buffer that is available during future self-attention . Our results show that indeed FLOPs may be inefficiently used in vanilla transformer models ."
    }
  ]
}
