{
  "id": 8862267286679124736,
  "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content": "dels expend the same amount of compute per token in a forward pass . the network must learn how to dynamically allocate the available compute by making decisions per-token, in each layer, about where to spend compute from the available budget . this strategy is akin to mixture of experts (moe) transformers ."
    },
    {
      "header": "Background",
      "content": "de variety of recent work has developed conditional computation methods for transformers . some of this work focuses on \"early exiting,\" that is, learning to decide when to end computation on a given token . in mod, unlike in early-exit methods, a token can skip middle layers ."
    },
    {
      "header": "Implementing Mixture-of-Depths Transformers",
      "content": "ken-choice routing, a router produces per-token probability distributions across computational paths . the top-k operation depends on the magnitude of the router weights . if k is less than the sequence length, we leverage the implicit knowledge that tokens will be dropped ."
    },
    {
      "header": "Results",
      "content": "del #3 achieves equal performance to the isoflop optimal baseline . but steps 66% faster, due to relatively fewer flops needed per forward pass . mod transformers that use stochastic routing perform drastically worse than baseline and normal mod transformer (figure)"
    },
    {
      "header": "Discussion",
      "content": "ple auxiliary classifier, or auxiliary loss on the router, is sufficient to learn the top-k routing decisions . learned routing could be a powerful mechanism for deciding which tokens these might be, perhaps funnelling them into a long-term memory buffer that is available during future self-attention ."
    }
  ]
}
