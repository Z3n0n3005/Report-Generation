{
  "id": "Raposo_e",
  "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content":  "In language modeling not all tokens and sequences require the same amount of time or effort to accurately make a prediction . Ideally, transformers would use smaller total compute budgets by not spending compute unnecessarily . The network must learn how to dynamically allocate the available compute by making decisions per-token, in each layer, about where to spend compute from the available budget ."
    },
    {
      "header": "Background",
      "content":  "The transformer architecture has become the workhorse of a revolution in artificial intelligence . This has spurred tremendous interest in making transformer architectures more efficient A wide variety of recent work has developed conditional computation methods for transformers . Some of this work focuses on learning to decide when to end computation on a given token ."
    },
    {
      "header": "Implementing Mixture-of-Depths Transformers",
      "content":  "We use a per-block router to emit a scalar weight for each token . This is merely the tokens' participation that is dynamic and context-sensitive, as determined by the router . We hypothesize that certain tokens might not require as much processing as others ."
    },
    {
      "header": "Results",
      "content":  "We first trained models with a relatively small FLOP budget (6e18) to determine optimal hyperparameters . When run on equivalent hardware these two model variants take approximately the same amount of wall-clock time to train . We tested routing every block or every other block, using capacities from 12.5% to 95% of the total sequence . However, aggressive capacity reduction was best ."
    },
    {
      "header": "Discussion",
      "content":  "Mixture-of-Depths transformers demonstrate that one can improve on isoFLOP-optimal baseline performance with models that use fewer FLOPs per forward pass . This means that-for a given training FLOP budget-we can train models that are both faster and better-performing than their baseline counterparts . We can then use any saved FLOP by, for example, making the model bigger or training it for longer ."
    }
  ]
}
