{"id": 4626595104389360967, "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "deling not all tokens and sequences require the same amount of time or effort to accurately make a prediction . transformers would use smaller total compute budgets by not spending compute unnecessarily . the most promising conditional computation methods may instead be those that are harmonious with our current hardware stack ."}, {"header": "Background", "content": "become the workhorse of a revolution in practical artificial intelligence . a wide variety of recent work has developed conditional computation methods for transformers . some of this work focuses on \"early exiting,\" that is, learning to decide when to end computation on a given token, allowing the token to skip any remaining transformer layers after the exit decision is made ."}, {"header": "Implementing Mixture-of-Depths Transformers", "content": "vanilla transformer may permit all tokens in a sequence to participate in self-attention . we use a per-block router to emit a scalar weight for each token . this is merely the tokens' participation that is dynamic and context-sensitive, as determined by the router ."}, {"header": "Results", "content": "d models with a relatively small flop budget (6e18) to determine optimal hyperparameters . in general, the optimal mod transformer achieves a lower loss than the optimal baseline . we found the best mod variant to be that which has the option to route every other block ."}, {"header": "Discussion", "content": "xture-of-depths transformers demonstrate that one can improve on isoflop-optimal baseline performance . this means that-for a given training flop budget-we can train models that are both faster and better-performing than baseline counterparts . it is crucial to use learned routing decisions to determine whether a token should participate in self-attention ."}]}