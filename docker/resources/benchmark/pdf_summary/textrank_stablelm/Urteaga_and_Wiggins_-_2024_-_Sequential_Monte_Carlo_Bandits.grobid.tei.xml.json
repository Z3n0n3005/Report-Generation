{
  "id": "Urteaga_",
  "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content": "The multi-armed bandit problem in sequential decision making, which extends to real-world challenges in online learning, involves maximizing reward while balancing exploration and exploitation, and utilizes techniques grounded in statistical advances on sequential decision processes and multi-armed bandits, with a focus on accommodating complex reward models and time-varying reward functions, leading to a SMC-based framework for solving non-stationary and nonlinear MABs, enabling accurate posterior approximations and flexible solutions for restless or non-stationary multiarmed bandits."
    },
    {
      "header": "Background and preliminaries",
      "content": "The study of maximizing rewards in sequential bandits, where arms are chosen based on uncertain environments and true models, is addressed through stochastic MAB (MAB with uncertainty) formulations, which incorporate Bayesian inference, explore-exploit tradeoffs, and sequential Monte Carlo methods to handle non-stationary bandit environments and accommodate complex reward functions and uncertainty over models."
    },
    {
      "header": "SMC for multi-armed bandits",
      "content": "The SMC-based posterior random measure p M (\u03b8 t+1,a |H 1:t ) is used to update the parameter posteriors for non-stationary bandits with complex nonlinear reward functions, while combining Thompson sampling and Bayes-UCB policies for sequential inference and estimation of sufficient statistics, avoiding assumptions on model parameter knowledge and leveraging their Bayesian marginalization, and addressing pathdegeneracy issues through quick forgetting properties."
    },
    {
      "header": "Evaluation",
      "content": "The empirical evaluation of SMC-based Bayesian MAB frameworks in non-stationary bandit scenarios with continuous, binary, and discrete-categorical reward distributions demonstrates satisfactory performance in capturing non-stationary trends and achieving sublinear cumulative regret, while SMC-based Thompson sampling and Bayes-UCB are able to adapt their choice of arm and attain sublinear regret in challenging, unknown non-stationary bandit models."
    },
    {
      "header": "Conclusion and discussion",
      "content": "The authors present a SMC-based Bayesian MAB framework that extends the applicability of Bayesian MAB policies to address previously elusive bandit environments, accommodating nonlinear and time-varying models of the world, and allows for interpretable modeling of nonlinear and time-evolving reward functions, while providing accurate exploration-exploitation tradeoffs and theoretical analysis for Thompson sampling and Bayes-UCB policies."
    }
  ]
}
