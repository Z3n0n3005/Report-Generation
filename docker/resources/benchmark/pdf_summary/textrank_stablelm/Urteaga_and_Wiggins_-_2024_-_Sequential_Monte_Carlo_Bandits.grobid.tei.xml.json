{"id": 3384170743928021142, "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "The multi-armed bandit (MAB) problem in sequential decision making, which extends to real-world challenges like online learning, is addressed through the development of a flexible Sequential Monte Carlo (SMC) based MAB framework that accommodates nonlinear and non-gaussian bandit rewards, time-varying reward models, and accommodates unknown parameters via Rao-Blackwellization, while leveraging SMC for both posterior sampling and estimation of sufficient statistics utilized by Bayesian MAB policies, addressing the challenges of classic MAB algorithms in domains like e-commerce and digital services."}, {"header": "Background and preliminaries", "content": "The study of non-stationary and flexible reward bandits, where the reward function varies over time, is addressed through Sequential Monte Carlo (SMC) methods, which are used to compute posterior distributions that lack an explicit closed-form, allowing for accommodating nonlinear and non-gaussian rewards, addressing non-stationary bandit environments, and being applicable to state-of-the-art Bayesian Multi-Armed Bandits (MAB) algorithms like Thompson Sampling and Bayes-UCB policies in a modular fashion."}, {"header": "SMC for multi-armed bandits", "content": "Sequential Monte Carlo (SMC) is used to compute posteriors and sufficient statistics for non-stationary bandits with complex nonlinear reward functions, in a state-space framework, allowing for independent parameter evolution per-arm and accommodating any computable likelihood function and time-varying models with strong theoretical convergence guarantees, while combining SMC with Thompson Sampling and Bayes-UCB policies for accurate estimation of Bayesian MAB policies in both stationary and time-evolving bandits scenarios."}, {"header": "Evaluation", "content": "Satisfactory performance across a wide range of stationary and non-stationary bandit parameterizations and sizes is demonstrated by Smc-based policies, which achieve the right exploration-exploitation tradeoff and capture non-stationary trends in personalized news article recommendations, with minimal regret loss for unknown reward variances and achieving sublinear regret curves for challenging non-stationary contextual gaussian bandit cases."}, {"header": "Conclusion and discussion", "content": "The authors propose a sequential Monte Carlo (SMC)-based Bayesian Multi-Armed Bandits (MAB) framework that extends the applicability of Bayesian MAB policies to previously elusive bandit environments, accommodating nonlinear and time-varying models of the world, and provides accurate exploration-exploitation tradeoffs by sequentially learning sufficient statistics and dynamics of the bandit from online data, allowing for interpretable modeling of nonlinear and time-evolving reward functions."}]}