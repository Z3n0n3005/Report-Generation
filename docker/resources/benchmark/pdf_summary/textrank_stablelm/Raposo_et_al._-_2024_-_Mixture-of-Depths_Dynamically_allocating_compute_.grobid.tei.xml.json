{"id": 8039001063538809915, "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "Mod Transformers can achieve equal or better log probabilities per sequence while using a smaller flops per forward pass by intelligently routing computations across different layers, allowing for faster execution while maintaining performance and potentially reducing compute expenditure per token in the forward pass."}, {"header": "Background", "content": "Efforts to improve transformer architectures through more efficient conditional computation methods, such as early-exit and mixture-of-depths, have spurred significant interest, allowing for adaptive decision-making during token processing and reducing total compute expenditure, with successful implementations of the \"mixture-of-experts\" layer in both LSTM and transformer models."}, {"header": "Implementing Mixture-of-Depths Transformers", "content": "Section 3.5 discusses the concept of capacity in Transformer models, which determines the total flops for conditional computations and the trade-off between compute expense and prediction accuracy, with token capacity being the primary factor in determining the total flops for transformers that use conditional computation, and considering two learned routing schemes: token-choice and expert-choice, with token-choice routing allowing for relative routing weights to prioritize critical tokens, while expert-choice routing has non-causal top-\ud835\udc58 operation but may be simplified by including router weights along the computational path for bypassing tokens."}, {"header": "Results", "content": "Mod transformer variants can achieve optimal performance and faster execution speed by utilizing fewer flops per forward pass and fewer parameters, while also being faster to step, as demonstrated by the IsoFlop analysis and experiments, with the optimal mod transformer requiring as many flops per forward pass as the IsoFlop optimal baseline, allowing for direct prediction of the model size for a given training budget."}, {"header": "Discussion", "content": "Using learned routing mechanisms, such as top-k routing, can optimize the use of flops in transformer models by determining which tokens participate in self-attention and MLPs, potentially allowing for more efficient computation and longer training times, while also decoupling routing decisions for queries, keys, and values, potentially increasing context-length available for making predictions, and adjusting the types of computations and their cost for fine-tuning model performance."}]}