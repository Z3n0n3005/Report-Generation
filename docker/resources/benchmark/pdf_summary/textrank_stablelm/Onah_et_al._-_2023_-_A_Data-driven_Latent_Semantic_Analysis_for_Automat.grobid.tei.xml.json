{"id": 3579104570643191549, "name": "Onah_et_al._-_2023_-_A_Data-driven_Latent_Semantic_Analysis_for_Automat.grobid.tei.xml", "segments": [{"header": "I. INTRODUCTION", "content": "This study presents a novel approach to topic modeling by performing extracuded summarization on over 100 articles related to genes and associated diseases, feeding the summarized content as an input argument to a Latent Dirichlet Allocation (LDA) model to perform topic modeling, aiming to identify common themes between articles of the same genre describing a specific topic of interest in the research, while addressing journal articles retrieved from the PubMed Central (PMC) database discussing genes and their associated diseases."}, {"header": "II. RELATED WORK", "content": "The rapid growth of text data worldwide demands effective text summarization techniques to preserve valuable information and ensure meaningful understanding for readers, while also addressing the need for automated extraction and preservation of web text data for efficient information retrieval and sustainability in the era of big data."}, {"header": "A. Summariza-on", "content": "The process of summarizing large texts using Natural Language Processing (NLP) techniques, such as summarizadon, involves condensing content while preserving the main relevant information, and can be integrated into various NLP applications to reduce document size while retaining original content, with two main approaches: extrac-ve and abstrac-ve, with the latter relying on deep learning for complex language modeling in unsupervised approaches like the one used in this study for article summarization."}, {"header": "B. Topic Modelling", "content": "Topic modeling is the unsupervised machine learning technique used to abstract core themes from a weighted graphical representation of documents, requiring extrapolation of topics from unstructured datasets and utilizing scikit-learn and gensim for extracting topics from models, with 0 gensim.models.ldamodel.ldamodel being used as input argument, which takes in the text corpus, number of topics, and id2word for preprocessed documents."}, {"header": "C. Latent Dirichlet Alloca-on", "content": "Latent Dirichlet Allocation (LDA) is a topic modeling technique introduced by applying it to the process of generating topics based on the probability of each term occurring within documents, considering the possibility of documents being mixtures of topics and words appearing in multiple topics, where a probability topics model is constructed using word distributions for each mixture of topics from multiple documents."}, {"header": "III. METHODS", "content": "The model utilized Natural Language Processing (NLP) techniques for preprocessing the data for extracuded summarization, applying sentence scoring, calculating word frequency and high sentence scores, performing cosine similarity scores, extracting top sentences based on their ranking, and utilizing LDA topic modeling to create a dictionary of terms for vectorized corpus construction, which was then used to build a lexicon LDA model, utilizing the 'pyldavis.gensim.prepare' method and 'gensim.models.ldamodel.ldamodel' with input arguments derived from the summarization corpus, number of topics, and the journal articles' dictionary terms."}, {"header": "A. Model Descrip-on", "content": "The study developed an overarching research model using topic modeling in natural language processing to retrieve specific information from large volumes of published journals, employing a supervised summarization approach that scores sentences based on their relevance and length, ultimately producing naturally grammatical and contextually preserved summaries, which can be generalized to various forms of textual data, such as media, blogs, news, and more, maximizing the likelihood of sentence consideration from the input document."}, {"header": "1)", "content": "The study introduces a scoring function to generate a sentence score dictionary, determining the probability of inclusion in the summary based on sentence relevance and selecting the top n sentences with the highest scores for the final document summary, revealing the importance of specific sentences in the document retrieval process."}, {"header": "Sentscores[S] = Wordfreq[W]", "content": "During the sentence model processing interval, the length of sentences is adjusted within the sentence scores dictionary, potentially adding new sentences if they exist, and if not, adding the corresponding word from the word frequencies dictionary to the sentence in the sentence scores dictionary, as per the equadon 2; otherwise, the model checks for existence in the sentence dicdonary."}, {"header": "Sentscores[S]+ = Wordfreq[W]", "content": "The model automatically selects word frequencies based on their prevalence in the corpus, with a limit of 30 words, calculating maximum weighted frequencies, and adding them to the final summary, using a process involving equadon 3 and 4."}, {"header": "Word", "content": ""}, {"header": "B. Research Pipeline", "content": "The pipeline model for this research employs a sequential series of processes to facilitate smooth and efficient information retrieval, as exemplified by the sequential handling of data collection, preprocessing, cleaning, and summarization, utilizing a combination of web scraping, extracting relevant medical science research papers from the PubMed Central (PMC) database, and applying topic modeling techniques to analyze the extracted data."}, {"header": "2)", "content": "The process of preprocessing and feature extraction involved converting raw, unstructured data from a web-based Pubmed journal into structured text documents, utilizing NLP libraries, parsing HTML source code to extract relevant textual material, filtering out special characters, and combining the extracted paragraphs into a single clean web content for further topic modeling."}, {"header": "3)", "content": "The process involved filtering out a list of non-essential stopwords from the processed articles, leaving only essential words for the final summary, as exemplified by the provided list of stop words."}, {"header": "4) Topic Modelling & Visualiza-on:", "content": "This study utilized LDA topic modeling and PyLDAVIS to visualize the prevalence and relevance of terms within documents, demonstrating their significance through the size of word circles and the number of common words among sentences, ultimately revealing semantic relationships among the texts."}, {"header": "IV. MODEL", "content": "We quantify the seman-c significance of term t for topic n within the context of a given parameter \u03bb, considering the minimal probability of term t in the lexical corpus and its frequency in the vocabulary, all weighted by \u03bb to reflect its relevance to the topic, using the equation (1 \u2264 \u03bb \u2264 1) and the term's probability in the corpus (pt), as well as its occurrence count (nt) within the specified topic range (1 \u2264 n \u2264 k) and its overall frequency in the corpus (n)."}, {"header": "B. Defining Saliency Term", "content": "The study defines saliency term as determining the minimal probability of a word 'w' given a topic model 'tm', while also computing the marginal probability and calculating the uniqueness of each identified word 'w' within the context of latent Dirichlet allocation (LDA) topic modeling, resulting in the computation of 5 topics and 10 passes, with each pass selected randomly from the LDA topic modeling process, and the divergence between p(tm/w) and p(tm) being used to quantify the uniqueness of each word."}, {"header": "U(6)", "content": ""}, {"header": "=5", "content": "The uniqueness of each term in a topic distribution is characterized by its significance and semantically associated significance to the topics, with larger topic circles and lower inter-topic distances indicating stronger relationships between topics, and the prevalence of certain terms revealing insights into the mixture and association of topics, particularly when observed among words expressed in multiple topics, leading to clearer term distribution and saliency measures that highlight the prevalence of terms related to specific topics."}, {"header": "V. LATENT SEMANTIC ANALYSIS", "content": "Latent Semantic Analysis (LSA) is a powerful method that identifies hidden semantic structures within words and sentences, enabling the extraction of essential but non-original features from datasets, which are crucial for data but not original features, and subsequently aids in text summarization by providing useful information about specific genes associated with cancer disease, assessed through Rouge metric system."}, {"header": "A. Sample Extracted Summary", "content": "We employed the heapq library to prioritize the most or very useful sentences based on word frequencies, ensuring that sentences with higher weight, such as those with a weight greater than the threshold, are given more processing priority in constructing the summary, with the selected threshold influencing the resulting summary, even when the word frequency is below the maximum limit of 30."}, {"header": "B. Findings", "content": ""}, {"header": "VI. ROUGE: RELIABILITY & VALIDITY OF MODEL", "content": "Rouge is a metric-oriented evaluation model specifically designed for text summarization, originally based on the Bilingual Evaluation Understudy (BLEU) metric, which compares machine or candidate translations to human annotated or reference translations, and includes various Rouge measures such as Rouge-1, Rouge-2, Rouge-3, Rouge-l, and Rouge-s in this study, which is then verified through fit metric tests using various fit metrics and fit analysis to assess the model's external quality."}, {"header": "TABLE III ROUGE METRICS MEASUREMENT SUMMARIES", "content": "The human-annotated summaries of genes in the BCAA metabolic pathway, such as MLYCD, HBAB, IVD, MUT, and PCCB, are highly ranked by Hridaya, based on 181 features grouped into various dimensions, and the human summaries closely align with the automated system summaries, resulting in an average recall, precision, and F1 score over 84% for the Rouge-1 evaluation metric."}, {"header": "Comparing the system generated summary with a new human", "content": ""}, {"header": "A. Procedure: Recall & Precision", "content": "The study combines precision and recall measures, calculating the F1-score, while also measuring Rouge metrics to evaluate the overlap of unigrams, bigrams, and overall fluency in the system-generated summaries, while annotating human-recommended reference summaries to assess relevant content recovery and accuracy, resulting in a more comprehensive analysis of the study's findings."}, {"header": "VII. RESULTS & FINDINGS", "content": "The topic modeling highlights text terms with significant frequency, represented by the size of the circle, and these terms are semantically related, with gene and disease described in relation to topic distribution; the LDA model was an input argument, along with the corpus and dictionary of emerging terms, and the visualization reveals common terms at varying probabilities, with 'gene' and 'disease' being most prominent in topic 2 at a probability of 0.48; however, the model's evaluation of emerging terms within the topic modeling approach is slower due to large text data for analysis."}, {"header": "IX. DISCUSSION", "content": "A fully data-driven approach for automated text summarization is proposed and evaluated on unstructured datasets, achieving comparable results to current state-of-the-art topic modeling techniques without relying on linguistic information models, making manual summarization a challenging task and automating it a significant advancement, gaining popularity among researchers and being applied to various NLP tasks such as text analysis, classification, questioning and answering, financial and legal texts summarization, news summarization, and generating social media articles, with early stages of document summaries integrated into any base model at intermediate stages to help reduce document length for further analysis."}, {"header": "X. CONCLUSION", "content": "The process of automatically reducing a text document to create a summary, while ensuring accuracy, is a challenging task, and this study proposes fully automated methods for single and multiple document text summarization using the LDA model, which can handle cross-language text summarization and retain the original document's meaning."}]}