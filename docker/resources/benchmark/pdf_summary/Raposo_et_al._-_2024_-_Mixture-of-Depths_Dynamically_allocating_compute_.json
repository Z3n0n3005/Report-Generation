{"id": 8062976220141542182, "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_", "abstract_seg": "Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens (\ud835\udc58) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-\ud835\udc58 routing mechanism. Since \ud835\udc58 is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the \ud835\udc58 tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50% faster to step during post-training sampling.", "segments": [{"header": "Introduction", "content": "And yet, transformer models expend the same amount of compute per token in a forward pass.\nTogether, these results imply that MoD transformers learn to route intelligently (i.e., skipping computations that are unnecessary) since they can achieve equal or better log probabilities per sequence despite a smaller FLOP footprint per forward pass."}, {"header": "Background", "content": "This has spurred tremendous interest in making transformer architectures more efficient A wide variety of recent work has developed conditional computation methods for transformers.\nSome of this work focuses on \"early exiting\", that is, learning to decide when to end computation on a given token, allowing the token to skip any remaining transformer layers after the exit decision is made Other work has developed methods for iterating transformer layers with shared weights for an adaptive number of steps CoLT5 MoD focuses on the decoder-only setting, and so we propose a predictive router to enable efficient inference for conditional computation in transformers.One successful formulation of conditional computation is the the \"mixture-of-experts\" layer (MoE) as introduced by "}, {"header": "Implementing Mixture-of-Depths Transformers", "content": "For example, the self-attention and MLP in each vanilla transformer block have a capacity of \ud835\udc47-the total number of tokens across the sequence and batch.\nThird, because we only route through two paths, a single top-\ud835\udc58 operation can efficiently split the tokens into two mutually exclusive sets, one for each computational path, preventing the over-or under-processing problem mentioned above.Figure As a reminder of the high-level intuition, each token is processed by a router to produce a scalar weight, and the top-\ud835\udc58 weights are then used to choose the token identities that will route through a transformer's block, which comprises self-attention and the subsequent MLP."}, {"header": "Results", "content": "Altogether, then, there exist MoD transformers that perform as well as isoFLOP-optimal baselines and are faster to step, both because they use fewer FLOPs per parameter and because they use fewer parameters.Figure We noticed that MoD transformers had memory savings relative to equivalently sized baseline models at larger sizes, with some variants requiring fewer total devices (i.e., a smaller TPU topology)."}, {"header": "Discussion", "content": "Rather, it is crucial to use learned routing decisions-much like in Mixture-of-Experts transformers-to determine whether a token should participate in self-attention and the subsequent MLP (requiring FLOPs), or not (saving FLOPs).We can then use any saved FLOPs by, for example, making the model bigger or training it for longer.\nOur results show that indeed FLOPs may be inefficiently used in vanilla transformer models, and that there may be more efficient ways for them to be expended.Learned routing mechanisms are sometimes non-causal; that is, information about the future is used to determine a given token's routing decision."}]}