{"id": 4158128593827648727, "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "-armed bandit (mab) problem considers the sequential strategy one must devise when playing a row of slot machines . the analogy extends to a wide range of real-world challenges that require online learning, while simultaneously maximizing some notion of reward .the contextual mab, where at each interaction with the world side information (known as 'context' is available, is a natural extension of this abstraction ."}, {"header": "Background and preliminaries", "content": "b crystallizes the fundamental trade-off between exploration and exploitation in sequential decision making . it formulates the problem of maximizing rewards observed from sequentially chosen actions a -named arms in the bandit literature-when interacting with an uncertain environment . however, its application is limited to reward models where the quantile functions are analytically tractable."}, {"header": "SMC for multi-armed bandits", "content": "quential monte carlo to compute posteriors and sufficient statistics of interest for a rich-class of mabs . we model non-stationary, stochastic bandits in a state-space framework .within this bandit framework, a bayesian policy must characterize the posterior of the unknown parameters p( t |h 1:t-1 )"}, {"header": "Evaluation", "content": "pare their performance to solutions based on analytically attainable posteriors with bernoulli and contextual linear gaussian reward functions .we observe noticeable (almost linear) regret increases when the dynamics of the parameters swap the identity of the optimal arm ."}, {"header": "Conclusion and discussion", "content": "-based posterior random measures are accurate enough for bayesian bandit policies to find satisfactory exploration-exploitation tradeoffs . the proposed framework allows for interpretable modeling of nonlinear and time-evolving reward functions . empirical results show good cumulative regret performance of the proposed policies in simulated mab environments ."}]}