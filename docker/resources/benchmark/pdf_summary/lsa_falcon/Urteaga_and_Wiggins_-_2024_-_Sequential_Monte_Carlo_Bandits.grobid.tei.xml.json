{"id": "Urteaga_", "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "The multi-armed bandit problem considers the sequential strategy one must devise when playing a row of slot machines: i.e., which arm to play next to maximize cumulative returns .The contextual MAB, where at each interaction with the world side information (known as 'context' is available, is a natural extension of this abstraction . The MAB crystallizes the fundamental trade-off between exploration and exploitation in sequential decision making ."}, {"header": "Background and preliminaries", "content": "The MAB crystallizes the fundamental trade-off between exploration and exploitation in sequential decision making . It formulates the problem of maximizing rewards observed from sequentially chosen actions a  A -named arms in the bandit literature-when interacting with an uncertain environment .The reward generating process is stochastic, often parameterized with   to capture the intrinsic properties of each arm .In the Bayesian setting, the uncertainty over the true"}, {"header": "SMC for multi-armed bandits", "content": "Bayesian policy must characterize the posterior of the unknown parameters p( t |H 1:t ) as in Equation (4), in which the time-varying dynamics of the underlying transition distribution are incorporated .We use sequential Monte Carlo to compute posteriors and sufficient statistics of interest for a rich-class of MABs .Algorithm 1 is described in terms of a generic reward likelihood function p a (Y |x, ) that must be comp"}, {"header": "Evaluation", "content": "Results in Appendix A.2 validate the performance of SMC-based Bayesian MAB framework in non-stationary bandits .We compare their performance to solutions based on analytically attainable posteriors with Bernoulli and contextual linear Gaussian reward functions For results we present below, we simulate different parameterizations of dynamic linear models described in Section 3.2 .The main evaluation metric is the cumulative regret of the bandit agent, as defined in Equation ."}, {"header": "Conclusion and discussion", "content": "MC-based posterior random measures are accurate enough for Bayesian bandit policies to find satisfactory exploration-exploitation tradeoffs .We present below cumulative regret results for different parameterizations of 5-armed Bernoulli bandits ."}]}