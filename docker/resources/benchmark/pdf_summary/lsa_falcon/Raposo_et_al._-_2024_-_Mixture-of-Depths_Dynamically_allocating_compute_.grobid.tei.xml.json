{"id": 9221027833464145059, "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "dels expend the same amount of compute per token in a forward pass . however, general formulations of this challenging problem may not work well with existing hardware constraints since they tend to introduce dynamic computation graphs ."}, {"header": "Background", "content": "become the workhorse of a revolution in practical artificial intelligence, bringing unprecedented capabilities at the cost of expensive training runs and serving procedures .a wide variety of recent work has developed conditional computation methods for transformers.we speculate that this might be a useful property ."}, {"header": "Implementing Mixture-of-Depths Transformers", "content": "-level strategy is as follows:\u2022 set a static compute budget that is less than that of an equivalent vanilla transformer . the self-attention and mlp in each vanilla transformer block have a capacity of T-the total number of tokens across the sequence and batch .token-choice routing can have load balancing problems since there isn't a guarantee that tokens divide themselves appropriately between the possible paths ."}, {"header": "Results", "content": "d transformers drag baseline isoflop curve \"down and to the right\"variants of the mod transformer were trained for 6e18 flops to determine optimal hyperparameters . crucially, when run on equivalent hardware these two model variants take approximately the same amount of wall-clock time to train ."}, {"header": "Discussion", "content": "xture-of-depths transformers demonstrate that one can improve on isoflop-optimal baseline performance with models that use fewer flops per forward pass . results show indeed flop-s may be inefficiently used in vanilla transformer models .learned routing mechanisms are sometimes non-causal; that is, information about the future is used to determine a given token's routing decision ."}]}