{"id": "Raposo_e", "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "In language modeling not all tokens and sequences require the same amount of time or effort to accurately make a prediction .In our implementation total compute is user defined and unchanging prior to training, rather than being a function of the network's on-the-fly decisions .We leverage an approach akin to Mixture of Experts (MoE) transformers in which dynamic token-level routing decisions are made across the network depth ."}, {"header": "Background", "content": "The transformer architecture has become the workhorse of a revolution in practical artificial intelligence . This has spurred tremendous interest in making transformer architectures more efficient .Some of this work focuses on \"early exiting\", that is, learning to decide when to end computation on a given token, allowing the token to skip any remaining transformer layers after the exit decision is made ."}, {"header": "Implementing Mixture-of-Depths Transformers", "content": "pute budget that is less than that of an equivalent vanilla transformer by limiting the number of tokens in a sequence that can participate in the block's computations . The computation graph and tensor sizes remain static throughout training; it is merely the tokens' participation that is dynamic and context-sensitive, as determined by the router ."}, {"header": "Results", "content": "We first trained models with a relatively small FLOP budget (6e18) to determine optimal hyperparameters . When run on equivalent hardware these two model variants take approximately the same amount of wall-clock time to train .Altogether, there exist MoD variants that perform as well as isoFLOP-optimal baseline models at larger sizes, with some variants requiring fewer total devices ."}, {"header": "Discussion", "content": "Mixture-of-Depths transformers demonstrate that one can improve on isoFLOP-optimal baseline performance with models that use fewer FLOPs per forward pass .This means that indeed FLOP models may be inefficiently used in vanilla transformer models, and there may be more efficient ways for them to be expended .Learned routing mechanisms are sometimes non-causal; that is, information about the future is used to determine a given token's routing decision ."}]}