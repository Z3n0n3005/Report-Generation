{"id": "Onah_et_", "name": "Onah_et_al._-_2023_-_A_Data-driven_Latent_Semantic_Analysis_for_Automat.grobid.tei.xml", "segments": [{"header": "I. INTRODUCTION", "content": "This study presents a novel approach to topic modeling by performing extracDEve summarizaDon on over 100 articles related to genes and associated diseases, feeding the summary as an input argument to a Latent Dirichlet Allocation (LDA) model to perform topic modeling, aiming to identify common themes across articles of the same genre describing a specific topic of interest in the research, while addressing journal articles retrieved from PubMed Central using automated text summarization techniques and topic modeling models to analyze a large amount of natural language data and achieve good performance in terms of document summary and topic modeling information."}, {"header": "II. RELATED WORK", "content": "The rapid increase and growing volume of text data worldwide necessitate the effective extraction and meaningful presentation of essential information, which is achieved through the process of text summarization, focusing on the most crucial portions while preserving the original meaning, as exemplified by natural language understanding and processing techniques."}, {"header": "A. Summariza-on", "content": "The segment describes two types of text summarization techniques in NLP, the extracursive and abstractive approaches, and their respective methods for condensing large texts while ensuring the main relevant information remains intact, with the extracursive approach prioritizing the top N sentences based on their score rankings for the summary generation, and the abstractive approach utilizing machine learning paradigms like deep learning for document summarization, focusing on the original document's content."}, {"header": "B. Topic Modelling", "content": "Topic modeling is an unsupervised machine learning technique for automatically extracting meaningful topics from collections of documents, allowing them to be label"}, {"header": "C. Latent Dirichlet Alloca-on", "content": "LDA, a method devised by Latent Dirichlet Allocation, is"}, {"header": "III. METHODS", "content": "The summarizaDon model, employing LDA topic modeling and leveraging genes and diseases keywords search in the PubMed journal database, created a lexicon-based corpus of terms to serve as input for the construction of a vectorized LDA model, utilizing the 'pyLDAvis.gensim.prepare' method for efficient preparation of the model, resulting in a dictionary of terms and a vectorized corpus for further analysis."}, {"header": "A. Model Descrip-on", "content": "The segment describes that a significant portion of text processing tasks were accomplished using the Python Natural Languag"}, {"header": "1)", "content": "A scoring function is introduced to create a sentence score dictionary, assigning values to each sentence based on the assigned scoring method in the study."}, {"header": "Sentscores[S] = Wordfreq[W]", "content": "During the sentence scoring interval, the length of sentences is adjusted within the sentence scores dictionary, potentially adding new sentences if they exist, and if not, appending existing words from the word frequencies dictionary to the sentence in the scores dictionary, following the rules specified in the equation (2)."}, {"header": "Sentscores[S]+ = Wordfreq[W]", "content": "The model automatically generates a word frequency corpus and selects the most prevalent words for calculating the maximum word in the word frequencies, as depicted in Figure 2, creating a DicDonary of word frequency, and equaDon 4 allows for this calculation."}, {"header": "Word", "content": "In the prov"}, {"header": "B. Research Pipeline", "content": "The pipeline model for the research employs a sequential approach to efficiently gather and process medical science research data, extracting approximately 100 papers from the PubMed Central database using a combination of 'gene' and 'disease' search terms, focusing on papers related to diseases and mutated genes, ultimately providing relevant information for the study."}, {"header": "2)", "content": "The segment describes the process of preprocessing and feature extraction from a web-based dataset scraped from PubMed journal, which involved converting the raw, unstructured data into structured text documents, parsing HTML source code to extract relevant textual material within paragraph tags, filtering out special characters, and combining the extracted paragraphs to form a clean web content for further topic modeling."}, {"header": "3)", "content": "We removed unnecessary stopwords from the processed articles, focusing on essential words for generating meaningful summaries, as exemplified by the figure illustrating the effect of including"}, {"header": "4) Topic Modelling & Visualiza-on:", "content": "The study effectively identified and showcased the prevalence and relevance of terms within documents through projective topic modeling and LDAvis visualization, demonstrating the significance of word size and project circle in the results."}, {"header": "IV. MODEL", "content": "The definition of seman-c significance in the context of term t for topic n, considering the parameter weight (\u03bb) ranging from 0 to 1,"}, {"header": "B. Defining Saliency Term", "content": "The study establishes a saliency term, denoted by 'w0', which quantifies the minimal probability derived from a topic model (LDA) for a given word 'w0', while also calculating the marginal probability 'P(TM)' and assessing the uniqueness of each identified word by comparing the divergences between 'P(TM/w)' and 'P(TM)' when any randomly selected word is generated from the topic model."}, {"header": "U(6)", "content": ""}, {"header": "=5", "content": "The uniqueness and semantic association of terms within topics are assessed by considering their significance, frequency, and inter-topic distance, revealing insights into the mixture of associated words across multiple topics and the potential rarity of certain words in these contexts, as exemplified by the model equation representing saliency; illustrated in Figure."}, {"header": "V. LATENT SEMANTIC ANALYSIS", "content": "Latent Semantic Analysis (LSA) is a powerful computational method that identifies and represents hidden semantic structures within words and sentences, enabling the extraction of features that cannot be directly represented with"}, {"header": "A. Sample Extracted Summary", "content": "We employed a heap queue (heapq) to prioritize and select the most impactful sentences for the summary, with the priority given to sentences with higher word frequencies, and determined the threshold number of sentences to include in the summary based on the provided table, ensuring a balanced and informative summary using the heapq library in implementing the priority queues"}, {"header": "B. Findings", "content": "The study's intriguing findings on genes linked to Cancer and type 2 diabetes, as presented in a table, have uncover"}, {"header": "VI. ROUGE: RELIABILITY & VALIDITY OF MODEL", "content": "ROUGE is a metric for evaluating document summarization tasks, specifically designed as a \"Recall-Oriented Understudy"}, {"header": "TABLE III ROUGE METRICS MEASUREMENT SUMMARIES", "content": "The BCAA metabolic genes MLYCD, HADHB, IVD, MUT, and PCCB, which are highly ranked by Hridaya, are also considered significant by Support Vector Machine (SVM) models, with their 181 features primarily falling under categories such as gene expression and regulation."}, {"header": "Comparing the system generated summary with a new human", "content": ""}, {"header": "A. Procedure: Recall & Precision", "content": "The process involves extracting web documents based on key search terms, generating human-annotated reference summaries of CleanHTML.txt, utilizing a metric to capture individual words in a sentence while aiming for concise summaries, combining precision and recall to report F1-score, and evaluating using ROUGE-N, ROUGE-S, and ROUGE-L measures, with ROUGE-1 being preferred due to its fluency assessment, considering overlapping bigrams in the generation process."}, {"header": "VII. RESULTS & FINDINGS", "content": "The topic modeling reveals text segments predominantly found in the document, represented by the size of the circle in visual representations (e.g., Figures), where the distance between topics approximates their semantic relationship, particularly highlighting close topics like 1, 2, and 3, which collectively describe the terms within them, as evidenced in Figures."}, {"header": "IX. DISCUSSION", "content": "The study presents a data-driven automated text summarization model that achieves comparable results with current state-of-the-art topic modeling techniques, without relying on linguistic models or language-dependent modifications, using unstructured datasets."}, {"header": "X. CONCLUSION", "content": "The process of AutomadC summarizaDon involves a computer program's reduction of a text document to create a summary that preserves the most crucial information fr"}]}