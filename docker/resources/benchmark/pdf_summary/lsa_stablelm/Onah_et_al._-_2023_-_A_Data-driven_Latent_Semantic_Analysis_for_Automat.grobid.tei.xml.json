{"id": 4033423048679993577, "name": "Onah_et_al._-_2023_-_A_Data-driven_Latent_Semantic_Analysis_for_Automat.grobid.tei.xml", "segments": [{"header": "I. INTRODUCTION", "content": "This study employs automated text summarization and latent Dirichlet allocation (LDA) models to identify common themes among articles related to genes and associated diseases from the PubMed Central database, aiming to uncover similarities and uncover emerging terms across multiple and diverse journal articles."}, {"header": "II. RELATED WORK", "content": "The rapid growth of text data worldwide necessitates the extraction and meaningful interpretation of this information, with text summarization serving as a crucial tool for preserving valuable information while focusing on essential content, ultimately enhancing information retrieval and sustainability in the era of big data."}, {"header": "A. Summariza-on", "content": "Summarizadon, a technique in NLP, condenses large texts into smaller summaries while preserving key elements and meaning, utilizing either the extracve or abstractve approach, with the extracve approach considering direct object scores and selecting the best sentence scores based on word count and sentence count thresholds, while the abstractve model is more complex and challenging to design, but was chosen for its ability to include all parts of the original sentences in the summary."}, {"header": "B. Topic Modelling", "content": "Topic modeling is the unsupervised machine learning technique used to abstract core themes from a weighted graphical representation of documents, enabling the extraction of topics from unstructured datasets in NLP applications, with Scikit-learn and gensim being utilized to extrapolate topics from models, utilizing 0 gensim.models.ldamodel.ldamodel as the input argument, which takes in the text corpus, the number of topics to be extracted, and id2word for preprocessed document dictionary terms."}, {"header": "C. Latent Dirichlet Alloca-on", "content": "Latent Dirichlet Allocation (LDA) is a topic modeling technique that identifies and represents topics in a document as a mixture of words or tokens from multiple documents, constructing a probability topic model based on the likelihood of each term occurring within the document, allowing for the generation of topics from the probability distribution of words across various documents."}, {"header": "III. METHODS", "content": "The summarizadon model, designed for keyword-based text data extraction from the Pubmed journal database, effectively retrieves and preprocesses papers from the Pubmed Central repository, utilizing Natural Language Processing techniques, and generates scalable, generalizable summaries by splitting documents into relevant content, while employing sentence scoring, word frequency analysis, and LDA topic modeling to create a dictionary of terms for the summarization process, ultimately producing high-quality summaries for a set of 100 relevant words in each sentence, followed by extracting the top N sentences for the summary generation process, which is then fed into the LDA topic modeling technique to create a vectorized corpus of lexicon, resulting in a scalable and generalized concept of LDA topic modeling for the journal articles."}, {"header": "A. Model Descrip-on", "content": "The study utilized the Python Natural Language Toolkit (NLTK) for processing text, including tokenization and topic modeling, to extract specific information from numerous journal articles related to diseases and genes, resulting in naturally generated, grammatically correct, and contextually relevant summaries without extensive linguistic analysis or annotation, achieved through a supervised summarization approach that scores sentences based on predefined conditions and maximizes the likelihood of sentence consideration from the input document."}, {"header": "1)", "content": "The study introduces a scoring function to generate a sentence score dictionary, determining the probability of inclusion in the summary based on sentence relevance and selecting the top n sentences with the highest scores for the document summary, with the quality largely dependent on the chosen sentences and revealing the importance of the information retrieved from the full document, represented by equads 1 and 2, where if a sentence is not in the score dictionary, the words from the word frequencies dictionary are added to the sentence scores."}, {"header": "Sentscores[S] = Wordfreq[W]", "content": "During the sentence model processing interval, the length of sentences is adjusted within the sentence scores dictionary, potentially adding or reducing them, while also considering existing sentences in the sentence dicdonary. If a new sentence is not already present, it is added to the sentence scores dicdonary, while if a sentence is not in the dicdonary, the corresponding word from the word frequencies dicdonary is incorporated into the sentence in the dicdonary."}, {"header": "Sentscores[S]+ = Wordfreq[W]", "content": "A model generates a word frequency dictionary based on the prevalence of words in a corpus, selects sentences with less than 30 words, calculates maximum weighted frequencies (Freqmax) using the product of word frequencies and values, and adds these to a final summary, with the next step allowing for the calculation of the maximum word in the word frequencies (Equation 3) and the subsequent step (Equation 4) in the process."}, {"header": "Word", "content": "In the prov"}, {"header": "B. Research Pipeline", "content": "The pipeline model in this research employs a sequential approach to efficiently gather, preprocess, and summarize medical science research papers related to diseases and mutated genes, utilizing a combination of web scraping and data preprocessing techniques, resulting in a smooth and effective information retrieval system."}, {"header": "2)", "content": "The segment describes the process of preprocessing and feature extraction from a web-based dataset scraped from Pubmed journal, which involved converting the raw, unstructured data into structured text documents, utilizing natural language processing (NLP) libraries like BeautifulSoup, regular expressions, and NLTK, parsing the web articles' source code to extract relevant textual material within paragraph tags, filtering out special characters, and combining the extracted paragraphs to form a clean web content for further topic modeling processing."}, {"header": "3)", "content": "The process of filtering out non-essential stopwords from the processed \"ardcles.words\" dataset involves removing a predefined list of words, such as \"pronounce\" and \"ess"}, {"header": "4) Topic Modelling & Visualiza-on:", "content": "The study utilized PyLDAvis, an interactive visualization tool, to identify and display terms with high relevance in the documents, which were derived from five LDA topic models, and then used these topics to create a web-based interactive visualization, demonstrating the prevalence of terms and their significance based on the size of their circles in the resulting visualizations, with semantically related sentences having higher numbers of common words."}, {"header": "IV. MODEL", "content": "We quantify the semantic significance of term T for topic N, considering the weight parameter \u03bb, where pt is the minimal probability of term T in the lexical corpus, and nt is the probability of term T occurring among N elements in the vocabulary, taking into account the frequency of terms (equation 1), with \u03bb representing the relative importance assigned to the probability of terms T within topic N."}, {"header": "B. Defining Saliency Term", "content": "The study defines a saliency term as determining the minimal probability of a word 'w' given a topic model (represented by tm), while also computing the marginal probability and calculating the uniqueness of each identified word 'w' within the context of latent Dirichlet allocation (LDA) topic modeling, resulting in the computation of 5 topics (t) and 10 passes, which were selected from the LDA model."}, {"header": "U(6)", "content": ""}, {"header": "=5", "content": "The uniqueness of each term in a topic network is characterized by its significance and semantically associated significance to the topics, with the size of the topic circles and inter-topic distance indicating the closeness of related topics; the saliency score is computed using a model equation that adjusts the lambda metric to remove ambiguity and clarify term distribution, while also measuring the distribution of topic associations and composition, revealing insights into the mixture of terms across multiple topics."}, {"header": "V. LATENT SEMANTIC ANALYSIS", "content": "Latent Semantic Analysis (LSA) is a powerful unsupervised method that identifies hidden semantic structures within words and sentences, enabling extraction of essential features not present in the original dataset, and is utilized in conjunction with Natural Language Processing (NLP) for efficient abstraction of document context, resulting in robust summaries and valuable insights on specific genes associated with cancer disease, assessed using the Rouge metric system, and visualized using a World Cloud."}, {"header": "A. Sample Extracted Summary", "content": "The process of selecting the most useful sentences for summarization, utilizing a heap queue (heapq) library to prioritize sentences based on word frequencies, involves assigning higher weight to sentences with higher frequency, with a threshold indicating the number of sentences to summarize, resulting in different summary lengths depending on the chosen threshold point, even when selecting sentences with less than 30 word frequency (< 30)."}, {"header": "B. Findings", "content": "The segment discusses intriguing findings from a summary that associate specific genes with cancer and type 2 diabetes, as displayed in a table ("}, {"header": "VI. ROUGE: RELIABILITY & VALIDITY OF MODEL", "content": "Rouge, a metric evaluation model specifically designed for assessing the accuracy of document summaries, is based on the Bilingual Evaluation Understudy (BLEU) metric and can be applied to evaluate text generated for natural language processing supports, incorporating measures to compare system-generated summaries to human-created summaries, such as Rouge-1, Rouge-2, Rouge-3, Rouge-l, and Rouge-s, which are included in the original Rouge evaluation model used in this study, and it is utilized to assess the reliability and validity of the model, with the final evaluation conducted using the fit metric test Rouge."}, {"header": "TABLE III ROUGE METRICS MEASUREMENT SUMMARIES", "content": "The human-annotated summaries of genes in the BCAA metabolic pathway, such as MLYCD, HBAB, IVD, MUT, and PCCB, are highly ranked by Hridaya, based on 181 features grouped into various dimensions, including gene, epigenetic, transcriptomic, and evolutionary, and are drug targets for cancer drugs like Dasatinib, Pazopanib, and NAPSAib, with average recall, precision, and F1-score values exceeding 83%, 85%, and 84%, respectively, as evidenced by the comparison with an automated system summary in the Rouge evaluation metrics."}, {"header": "Comparing the system generated summary with a new human", "content": ""}, {"header": "A. Procedure: Recall & Precision", "content": "The process involves extracting multiple web documents based on key search terms, generating human-annotated reference summaries of clean HTML text, utilizing Equadon 8 to represent individual words in a sentence as a metric for capturing the essence, combining precision and recall scores using Equadon 9 to evaluate generated summaries' fluency, and measuring Rouge-N, Rouge-S, and Rouge-L to assess the granularity of texts compared between system summaries and human annotated model summaries, ultimately resulting in an F1-score measure to assess the validity of the study."}, {"header": "VII. RESULTS & FINDINGS", "content": "The topic modeling results, visualized through term circles and LDA models, reveal the distribution of topics 1, 2, and 3, their relationships to each other, and the relevance metric of the rank terms, with terms ranked in decreasing order by topic-specific probability, and the common terms from the topic model influencing the grammatical structure of various text summaries, while the model's evaluation of emerging terms within the topic modeling approach is slower due to large text data analysis."}, {"header": "IX. DISCUSSION", "content": "The study presents a data-driven approach for automated text summarization, achieving comparable results to current state-of-the-art techniques without relying on linguistic information models, making manual summarization a challenging yet essential task, and its application in various NLP tasks such as text analysis, classification, questioning, financial and legal texts summarization, news summarization, and reviewing of news headlines, with early stages of document summaries potentially integrated into any base model at intermediate stages to reduce document length for further analysis, benefiting from the process."}, {"header": "X. CONCLUSION", "content": "The study presents a fully automated, multilingual text summarization approach that extracts and summarizes important points from multiple documents while preserving their overarching meaning and purpose, and aims to compare the results with a current machine learning gene prediction model for diseases in a future research study."}]}