{"id": "Urteaga_", "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "The multi-armed bandit problem in sequential decision making, which extends to real-world challenges involving online learning, involves balancing exploration and exploitation to maximize reward, utilizing techniques grounded in statistical advances on sequential decision processes and multi-armed bandits, with a focus on accommodating complex reward models and time-varying reward functions, leading to the development of the SMC-based MAB framework for efficient computation of any required statistic in the context of Bayesian MAB policies."}, {"header": "Background and preliminaries", "content": "The MAB framework crystallizes the exploration-exploitation tradeoff in sequential decision making, encompasses the problem of maximizing rewards observed from chosen actions in an uncertain environment, and proposes a SMC-based approach that extends Bayesian MAB policies to accommodate nonlinear and non-Gaussian rewards, address non-stationary environments, and seamlessly integrates with existing Thompson sampling and Bayes-UCB policies in a modular manner."}, {"header": "SMC for multi-armed bandits", "content": "The SMC-based Bayesian MAB framework is developed for non-stationary bandits with complex nonlinear reward functions, allowing for independent parameter dynamics per-arm and avoiding assumptions on model parameter knowledge, while combining SMC with Thompson sampling and Bayes-UCB policies to compute posterior and sufficient statistics for a rich-class of MABs, enabling accurate estimation of their sufficient statistics for any Bayesian bandit policy."}, {"header": "Evaluation", "content": "The SMC-based Bayesian MAB framework is empirically validated for non-stationary bandit scenarios with continuous, binary, and discrete-categorical reward distributions, demonstrating satisfactory cumulative regret performance in various MAB environments, including personalized news article recommendations, where the dynamic model's parameters are unknown, and SMC-based Thompson sampling and Bayes-UCB achieve sublinear cumulative regret, demonstrating successful exploitation-exploration balance."}, {"header": "Conclusion and discussion", "content": "The authors present a SMC-based Bayesian framework for multi-armed bandits that extends the applicability of Bayesian MAB policies to accommodate nonlinear and time-varying models, allowing for interpretable modeling of reward functions and sequential learning of sufficient statistics and dynamics to find the optimal exploration-exploitation balance, resulting in improved cumulative regret performance in simulated and practical scenarios."}]}