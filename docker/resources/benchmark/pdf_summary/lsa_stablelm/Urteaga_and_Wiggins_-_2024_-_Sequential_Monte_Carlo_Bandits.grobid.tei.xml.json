{"id": 745544385370734124, "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "The multi-armed bandit (MAB) problem in sequential decision making, where arms are chosen based on maximizing cumulative returns, extends to real-world challenges like online learning, where contextual information is available, and the trade-off between exploration and exploitation is crucial, leading to the development of algorithms like Sequential Monte Carlo (SMC) for non-stationary and nonlinear MABs, which generalize existing MAB policies, accommodate complex reward models, and provide flexible frameworks for addressing time-varying and nonlinear reward functions, addressing challenges in restless and non-stationary MABs, and enabling computation of any statistic a Bayesian MAB policy might require."}, {"header": "Background and preliminaries", "content": "The MAB (Maximizing Average Reward) framework extends the exploration-exploitation trade-off in sequential decision-making, specifically for uncertain environments with non-gaussian and nonlinear rewards, and proposes the SMC-based bandit policies that are modular, non-restricted to specific reward functions, and applicable to existing Bayesian MAB algorithms, while addressing challenges in non-stationary and uncertain bandit environments."}, {"header": "SMC for multi-armed bandits", "content": "The sequential Monte Carlo-based Bayesian Multi-Armed Bandit framework is developed to compute posteriors and sufficient statistics for non-stationary, non-Gaussian bandits with complex nonlinear reward functions, allowing for independent parameter evolution per-arm and avoiding particle degeneracy, while addressing the challenges of non-markovian transition distributions and path degeneracy in the estimation of posterior densities for dynamic bandits."}, {"header": "Evaluation", "content": "SMC-based Bayesian policies effectively learn and adapt to non-stationary bandits with contextual rewards, demonstrating the ability to identify and exploit dynamic parameter shifts, resulting in minimized regret and improved performance over time, as evidenced by their successful identification of the optimal arm in a challenging bandit setting, with no dynamic parameter knowledge."}, {"header": "Conclusion and discussion", "content": "The authors propose a sequential Monte Carlo (SMC)-based Bayesian Multi-Armed Bandits (MAB) framework that extends Bayesian MAB policies, accommodating nonlinear and time-varying models of the world, and demonstrates superior performance in addressing elusive bandit environments, achieving interpretable modeling of reward functions, and providing accurate exploration-exploitation tradeoffs, while handling non-stationary bandit environments and mitigating estimation challenges for constant parameters."}]}