{"id": 3019718325499396729, "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "The multi-armed bandit (MAB) problem in sequential decision making, where arms are chosen based on maximizing cumulative returns, extends to real-world challenges such as online learning, where contextual information, such as patient physiology, project resources, or website user features, is available, and the trade-off between exploration and exploitation is crucial, leading to the development of algorithms like Sequential Monte Carlo (SMC) for non-stationary and nonlinear MABs, which generalize existing MAB policies, accommodate complex reward models, and provide a flexible framework for solving these challenging problems."}, {"header": "Background and preliminaries", "content": "The MAB (Maximizing Average Reward) framework extends the exploration-exploitation trade-off in sequential decision-making, specifically for uncertain environments with non-gaussian and nonlinear rewards, and offers alternative policies like Thompson Sampling (TS) and Bayesian UCB (Upper Confidence Bound), while providing a novel SMC-based approach for accommodating stationarity and flexibility in Bayesian MAB algorithms, particularly for Monte Carlo methods and importance sampling techniques."}, {"header": "SMC for multi-armed bandits", "content": "The sequential Monte Carlo-based Bayesian Multi-Armed Bandit framework is developed to compute posteriors and sufficient statistics for non-stationary, non-Gaussian bandits with complex nonlinear reward functions, allowing for independent parameter evolution per-arm, and presents algorithms for drawing samples from transition densities, handling non-stationarity via the General Linear Model, and approximating the posterior density accurately with high probability, even for non-Gaussian and nonlinear reward functions in practice."}, {"header": "Evaluation", "content": "SMC-based Bayesian policies effectively learn and adapt to non-stationary bandits with contextual rewards, demonstrating the ability to identify and exploit dynamic parameter shifts, resulting in minimized regret and improved performance over time, as evidenced by their successful identification of the optimal arm in a challenging bandit setting, with no dynamic parameter knowledge."}, {"header": "Conclusion and discussion", "content": "The authors propose a sequential Monte Carlo (SMC)-based Bayesian Multi-Armed Bandits (MAB) framework that extends Bayesian MAB policies, accommodating nonlinear and time-varying models of the world, and demonstrates superior performance in addressing elusive bandit environments, achieving interpretable modeling of reward functions, and providing accurate exploration-exploitation tradeoffs, while handling non-stationary bandit environments and mitigating estimation challenges for constant parameters."}]}