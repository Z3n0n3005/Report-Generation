{"id": "Raposo_e", "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "MoD transformers, which employ dynamic token-level routing decisions across the network depth, can achieve equal or better log probabilities per sequence while maintaining a smaller FLOP footprint per forward pass, demonstrating an approach that optimizes compute efficiency without sacrificing overall performance."}, {"header": "Background", "content": "The transformer architecture, through its efficiency improvements and widespread adoption in AI, has revolutionized practical applications, while also sparking research to enhance its capabilities at lower costs, with conditional computation methods like early exiting and adaptive iteration, leading to the development of the predictive router for efficient inference in the decoder-only setting, exemplified by the \"mixture-of-experts\" layer introduced by CoLT5 MoD."}, {"header": "Implementing Mixture-of-Depths Transformers", "content": "Our high-level strategy involves setting a static compute budget by limiting the number of tokens in a sequence that can participate in a block's computations, ensuring the computation graph and tensor sizes remain static throughout training while dynamically adjusting token participation based on the router weights, resulting in fewer FLOPs per forward pass compared to a vanilla transformer, with the router weights subjecting to gradient descent and allowing for a simple and accurate auxiliary loss function that achieves 99% accuracy."}, {"header": "Results", "content": "The MoD transformer, which utilizes stochastic routing and fewer FLOPs per parameter, can achieve faster training and better performance than the isoFLOP optimal baseline, while maintaining robustness to capacity reductions, and can potentially offer memory savings and faster autoregressive sampling with smaller TPU topology, as observed during preliminary analyses."}, {"header": "Discussion", "content": "Mixture-of-Depths transformers, through empirical analysis, demonstrate the potential for improving performance with models that utilize fewer FLOPs per forward pass, offering faster and more efficient training while maintaining or surpassing the baseline performance, while also revealing inefficiencies in vanilla transformer models and suggesting the utilization of learned routing mechanisms to optimize future tokens' attention and potentially enhance long-term memory capabilities."}]}