{"id": 3413338353318646169, "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "conditional computation is a technique that tries to reduce total compute by expending it only when needed. We leverage an approach akin to mixture of experts (moe) transformers, in which dynamic token-level routing decisions are made across the network depth. We refer to this strategy as mixture-of-depths (mod) to emphasize how individual tokens pass through different numbers of layers, or blocks."}, {"header": "Background", "content": "The transformer architecture has become the workhorse of a revolution in practical artificial intelligence, bringing unprecedented capabilities at the cost of expensive training runs and serving procedures. One of the promising approaches is conditional computation, whereby learned mechanisms determine when and how to expend computation."}, {"header": "Implementing Mixture-of-Depths Transformers", "content": "We use a per-block router to emit a scalar weight for each token, which expresses the router's preference for that token to participate in a block's computations or to route around it. To enforce a total compute budget per forward pass we leverage the notion of capacity, which defines the total number of tokens that comprise the input to a given computation."}, {"header": "Results", "content": "We first trained models with a relatively small flop budget (6e18) to determine optimal hyperparameters. The best mod variant to be that which has the option to route every other block, and which uses a top-k of 256. aggressive capacity reduction was best (gradual improvements were observed when reducing the capacity down to 12.5% of the total sequence)"}, {"header": "Discussion", "content": "Mixture-of-depths transformers demonstrate that one can improve on isoflop-optimal baseline performance with models that use fewer flops per forward pass."}]}