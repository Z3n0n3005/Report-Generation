{"id": "Raposo_e", "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "Conditional computation is a technique that tries to reduce total compute by expending it only when needed. Here we consider the problem of language modeling using a static compute budget that can be made less than that used by a vanilla transformer. We leverage an approach akin to Mixture of Experts (MoE) transformers."}, {"header": "Background", "content": "The transformer architecture has become the workhorse of a revolution in practical artificial intelligence. This has spurred tremendous interest in making transformer architectures more efficient. We propose a predictive router to enable efficient inference for conditional computation in transformers."}, {"header": "Implementing Mixture-of-Depths Transformers", "content": ""}, {"header": "Results", "content": "We tested routing every block or every other block, using capacities from 12.5% to 95% of the total sequence. We found that aggressive capacity reduction was best. MoD transformers had memory savings relative to equivalently sized baseline models at larger sizes."}, {"header": "Discussion", "content": "Mixture-of-Depths transformers can improve on isoFLOP-optimal baseline performance with models that use fewer FLOPs per forward pass. MoD transformers demonstrate value of routing among different types of computations."}]}