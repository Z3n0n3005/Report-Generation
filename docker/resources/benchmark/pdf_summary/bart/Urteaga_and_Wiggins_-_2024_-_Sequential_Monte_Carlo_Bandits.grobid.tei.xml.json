{"id": "Urteaga_", "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "The multi-armed bandit (MAB) problem considers the sequential strategy one must devise when playing a row of slot machines. The MAB crystallizes the fundamental trade-off between exploration and exploitation in sequential decision making."}, {"header": "Background and preliminaries", "content": ""}, {"header": "SMC for multi-armed bandits", "content": ""}, {"header": "Evaluation", "content": ""}, {"header": "Conclusion and discussion", "content": "The proposed SMC-based Bayesian MAB framework allows for interpretable modeling of nonlinear and time-evolving reward functions. It sequentially learns the sufficient statistics and dynamics of the bandit from online data, to find the right exploration-exploitation balance."}]}