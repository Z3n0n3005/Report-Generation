{
  "id": 9109274723410202958,
  "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content": "consequently, we devise a flexible smc-based framework for solving non-stationary and nonlinear mabs.our contribution is a smc-based mab framework that:(i) computes smc-based random measure posterior mab densities utilized by bayesian mab policies;(ii) requires knowledge of the reward function only up to a proportionality constant, i.e., it accommodates nonlinear and non-gaussian bandit rewards; and, (iii) is applicable to time-varying reward models, i.e., to restless or non-stationary multiarmed bandits.the proposed smc-based mab framework (i) leverages smc for both posterior sampling and estimation of sufficient statistics utilized by bayesian mab policies, i.e.,addresses restless bandits via the general linear dynamical system, and accommodates unknown parameters via rao-blackwellization; and (iii) targets nonlinear and non-gaussian reward models, accommodating stateless and context-dependent, discrete and continuous reward distributions.we introduce in section 2 the preliminaries for our work, which combines sequential monte carlo techniques described in section 2.2, with multi-armed bandit algorithms detailed in section 2.1."
    },
    {
      "header": "Background and preliminaries",
      "content": "because a bandit agent must take into account the uncertainty on the unknown parameters, prior knowledge on the reward model and its parameters can be incorporated into bayesian policies, capturing the full state of knowledge via the parameter posteriorwhere p at (y t |x t , \u03b8 t ) is the likelihood of the observed reward y t after playing arm a t at time t.computation of this posterior is critical for bayesian mab algorithms.in thompson sampling, one uses p(\u03b8 t |h 1:t ) to compute the probability of an arm being optimal, i.e., \u03c0(a|x t+1 , h 1:t ) = p a = a * t+1 |x t+1 , \u03b8 t , h 1:t , where the uncertainty over the parameters must be accounted for.namely, one marginalizes the posterior parameter uncertainty after observing history h 1:t up to time instant t, i.e.,(5)in bayes-ucb, p(\u03b8 t |h 1:t ) is critical to determine the distribution of the expected rewards, i.e.,which is required for computation of the expected reward quantile q t+1,a (\u03b1 t ), formally defined aswhere the quantile value \u03b1 t may depend on time, as proposed by."
    },
    {
      "header": "SMC for multi-armed bandits",
      "content": "throughout, we avoid assumptions on model parameter knowledge and resort to their bayesian marginalization.we combine smc with both thompson sampling and bayes-ucb policies, by sequentially updating, at each bandit interaction t, a smc-based random measure to approximate the time-varying posterior of interest,knowledge of p m (\u03b8 t,a |h 1:t ) enables computation of any per-arm reward statistic bayesian mab policies require.we present algorithm 1 with the sequential importance resampling (sir) methodas introduced by, where:\u2022 the smc proposal distribution q(\u2022) at each bandit interaction t obeys the assumed parameter dynamics: \u03b8\u2022 smc weights are updated based on the likelihood of the observed rewards: wt,a ) -step (9.c) in algorithm 1; and \u2022 the smc random measure is resampled at every time instant -step (9.a).independently of which smc technique is used to compute the posterior random measure p m (\u03b8 t,a |h 1:t ), the fundamental operation in the proposed smc-based mab algorithm 1 is to sequentially update the random measure p m (\u03b8 t,a |h 1:t ) to approximate the true per-arm posterior p(\u03b8 t,a |h 1:t ) over bandit interactions.this smc-based random measure is key, along with transition density p(\u03b8 t,a |\u03b8 t-1,a ), to sequentially propagate parameter posteriors per-arm, and to estimate their sufficient statistics for any bayesian bandit policy."
    },
    {
      "header": "Evaluation",
      "content": "for instance, for a given realization of scenario a shown in figure, there is an optimal arm swap between time-instants t = (300, 550), with arm 1 becoming the optimal for all t \u2265 600; for a realization of scenario b illustrated in figure, there is an optimal arm change around t = 100, a swap around t = 600, with arm 1 becoming optimal again after t \u2265 1600.empirical results for smc-based bayesian policies in scenarios described by equations () and () are shown in figuresand.we study linear dynamics with gaussian reward distributions with known parameters in figure, of interest as it allows us to validate the smc-based random measure in comparison to the optimal, closed-form posterior -the kalman filter-under the assumption of known dynamic parameters.we observe satisfactory cumulative regret performance in figure: i.e., smc-based bayesian agents' cumulative regret is sublinear.\nthe regret loss associated with the uncertainty about \u03c3 2 a is minimal for smc-based bayesian agents, and does not hinder the ability of the proposed smc-based policies to find the right exploration-exploitation balance: i.e., regret is sublinear, and the agents adapt to switches in the identity of the optimal arm.we illustrate in figures 2e-2f the most realistic, yet challenging, non-stationary contextual gaussian bandit case: one where none of the parameters of the model are known."
    },
    {
      "header": "Conclusion and discussion",
      "content": "however, as long as the bandit's latent dynamics incur in a controlled number of optimal arm changes, smc can provide accurate enough posteriors to find the right exploration-exploitation tradeoff, as we show empirically here.a theoretical analysis of the dependency between the dynamic bandit model's characteristics, the resulting rate of optimal arm changes, and smc posterior convergence guarantees, leading to formal regret bounds for the proposed smc-based bayesian policies, is an open research direction.we here apply the proposed smc-based bayesian policies as in algorithm 1 to the original settings where thompson sampling and bayes-ucb were derived, i.e., for stationary bandits with bernoulli and contextual, linear gaussian reward functions;;;.empirical results for these bandits is provided in section a.2, while the stationary logistic bandit case is evaluated in section a.3, where we also evaluate the impact of sample size m in the smc-based bandit algorithms.in stationary bandits, there are no time-varying parameters, i.e., \u03b8 t = \u03b8, \u2200t."
    }
  ]
}
