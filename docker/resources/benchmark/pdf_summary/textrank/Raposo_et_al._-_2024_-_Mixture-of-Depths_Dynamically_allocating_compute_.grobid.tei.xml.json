{
  "id": "Raposo_e",
  "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content": "And yet, transformer models expend the same amount of compute per token in a forward pass.\nTogether, these results imply that MoD transformers learn to route intelligently (i.e., skipping computations that are unnecessary) since they can achieve equal or better log probabilities per sequence despite a smaller FLOP footprint per forward pass."
    },
    {
      "header": "Background",
      "content": "This has spurred tremendous interest in making transformer architectures more efficient A wide variety of recent work has developed conditional computation methods for transformers.\nSome of this work focuses on \"early exiting\", that is, learning to decide when to end computation on a given token, allowing the token to skip any remaining transformer layers after the exit decision is made Other work has developed methods for iterating transformer layers with shared weights for an adaptive number of steps CoLT5 MoD focuses on the decoder-only setting, and so we propose a predictive router to enable efficient inference for conditional computation in transformers.One successful formulation of conditional computation is the the \"mixture-of-experts\" layer (MoE) as introduced by "
    },
    {
      "header": "Implementing Mixture-of-Depths Transformers",
      "content": "For example, the self-attention and MLP in each vanilla transformer block have a capacity of \ud835\udc47-the total number of tokens across the sequence and batch.\nThird, because we only route through two paths, a single top-\ud835\udc58 operation can efficiently split the tokens into two mutually exclusive sets, one for each computational path, preventing the over-or under-processing problem mentioned above.Figure As a reminder of the high-level intuition, each token is processed by a router to produce a scalar weight, and the top-\ud835\udc58 weights are then used to choose the token identities that will route through a transformer's block, which comprises self-attention and the subsequent MLP."
    },
    {
      "header": "Results",
      "content": "Altogether, then, there exist MoD transformers that perform as well as isoFLOP-optimal baselines and are faster to step, both because they use fewer FLOPs per parameter and because they use fewer parameters.Figure We noticed that MoD transformers had memory savings relative to equivalently sized baseline models at larger sizes, with some variants requiring fewer total devices (i.e., a smaller TPU topology)."
    },
    {
      "header": "Discussion",
      "content": "Rather, it is crucial to use learned routing decisions-much like in Mixture-of-Experts transformers-to determine whether a token should participate in self-attention and the subsequent MLP (requiring FLOPs), or not (saving FLOPs).We can then use any saved FLOPs by, for example, making the model bigger or training it for longer.\nOur results show that indeed FLOPs may be inefficiently used in vanilla transformer models, and that there may be more efficient ways for them to be expended.Learned routing mechanisms are sometimes non-causal; that is, information about the future is used to determine a given token's routing decision."
    }
  ]
}
