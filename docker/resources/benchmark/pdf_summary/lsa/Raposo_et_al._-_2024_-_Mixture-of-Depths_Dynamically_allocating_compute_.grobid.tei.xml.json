{
  "id": "Raposo_e",
  "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content": "In our implementation total compute is user defined and unchanging prior to training, rather than being a function of the network's on-the-fly decisions."
    },
    {
      "header": "Background",
      "content": "The transformer architecture has become the workhorse of a revolution in practical artificial intelligence, bringing unprecedented capabilities at the cost of expensive training runs and serving procedures."
    },
    {
      "header": "Implementing Mixture-of-Depths Transformers",
      "content": "Our high-level strategy is as follows:\u2022 Set a static compute budget that is less than that of an equivalent vanilla transformer by limiting the number of tokens in a sequence that can participate in a block's computations (i.e., selfattention and subsequent MLP)."
    },
    {
      "header": "Results",
      "content": "First, the FLOP-per-parameter ratio in MoD Figure There exist MoD variants that are both faster to step (by virtue of requiring fewer FLOPs per forward pass) and better performing than the isoFLOP optimal baseline."
    },
    {
      "header": "Discussion",
      "content": "One can imagine extending this idea even further into the domain of \"long-term memory\": perhaps there are tokens that would be extremely valuable as keys, regardless of whether it is useful for them to also be among the queries at the step of their occurrence."
    }
  ]
}
