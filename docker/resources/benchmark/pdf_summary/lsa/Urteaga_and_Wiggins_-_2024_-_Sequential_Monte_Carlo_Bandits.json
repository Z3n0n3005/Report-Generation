{
  "id": -68579898934697178,
  "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits",
  "abstract_seg": "We extend Bayesian multi-armed bandit (MAB) algorithms beyond their original setting by making use of sequential Monte Carlo (SMC) methods. A MAB is a sequential decision making problem where the goal is to learn a policy that maximizes long term payoff, where only the reward of the executed action is observed. In the stochastic MAB, the reward for each action is generated from an unknown distribution, often assumed to be stationary. To decide which action to take next, a MAB agent must learn the characteristics of the unknown reward distribution, e.g., compute its sufficient statistics. However, closed-form expressions for these statistics are analytically intractable except for simple, stationary cases. We here utilize SMC for estimation of the statistics Bayesian MAB agents compute, and devise flexible policies that can address a rich class of bandit problems: i.e., MABs with nonlinear, stateless-and context-dependent reward distributions that evolve over time. We showcase how non-stationary bandits, where time dynamics are modeled via linear dynamical systems, can be successfully addressed by SMC-based Bayesian bandit agents. We empirically demonstrate good regret performance of the proposed SMC-based bandit policies in several MAB scenarios that have remained elusive, i.e., in non-stationary bandits with nonlinear rewards.",
  "segments": [
    {
      "header": "Introduction",
      "content": "Consequently, we devise a flexible SMC-based framework for solving non-stationary and nonlinear MABs.Our contribution is a SMC-based MAB framework that:(i) computes SMC-based random measure posterior MAB densities utilized by Bayesian MAB policies;(ii) requires knowledge of the reward function only up to a proportionality constant, i.e., it accommodates nonlinear and non-Gaussian bandit rewards; and, (iii) is applicable to time-varying reward models, i.e., to restless or non-stationary multiarmed bandits.The proposed SMC-based MAB framework (i) leverages SMC for both posterior sampling and estimation of sufficient statistics utilized by Bayesian MAB policies, i.e., We introduce in Section 2 the preliminaries for our work, which combines sequential Monte Carlo techniques described in Section 2.2, with multi-armed bandit algorithms detailed in Section 2.1."
    },
    {
      "header": "Background and preliminaries",
      "content": "Because a bandit agent must take into account the uncertainty on the unknown parameters, prior knowledge on the reward model and its parameters can be incorporated into Bayesian policies, capturing the full state of knowledge via the parameter posteriorwhere p at (y t |x t , \u03b8 t ) is the likelihood of the observed reward y t after playing arm a t at time t.Computation of this posterior is critical for Bayesian MAB algorithms.In Thompson sampling, one uses p(\u03b8 t |H 1:t ) to compute the probability of an arm being optimal, i.e., \u03c0(A|x t+1 , H 1:t ) = P A = a * t+1 |x t+1 , \u03b8 t , H 1:t , where the uncertainty over the parameters must be accounted for Namely, one marginalizes the posterior parameter uncertainty after observing history H 1:t up to time instant t, i.e.,(5)In Bayes-UCB, p(\u03b8 t |H 1:t ) is critical to determine the distribution of the expected rewards, i.e.,which is required for computation of the expected reward quantile q t+1,a (\u03b1 t ), formally defined aswhere the quantile value \u03b1 t may depend on time, as proposed by To extend MAB algorithms to more realistic scenarios, many have considered flexible reward functions and Bayesian inference."
    },
    {
      "header": "SMC for multi-armed bandits",
      "content": "We now describe in detail how to use the SMC-based posterior random measure p M (\u03b8 t+1,a |H 1:t ) for both Thompson sampling and Bayes-UCB policies: i.e., which are the specific instructions to execute in steps 5 and 7 of Algorithm 1.\u2022 SMC-based Thompson Sampling: TS operates by drawing a sample parameter \u03b8 (s)t+1 from its updated posterior p(\u03b8 t+1 |H 1:t ), and picking the optimal arm for such sample, i.e.,We use the SMC random measure p M (\u03b8 t |H 1:t ), and propagate it using the transition density p(\u03b8 t+1,a |\u03b8 t,a ), to draw samples from the parameter posterior predictive distribution: i.e., \u03b8t+1 \u223c p M (\u03b8 t+1 |H 1:t ) in Equation ( \u2022 SMC-based Bayes-UCB: We extend Bayes-UCB to reward models where the quantile functions are not analytically tractable, by leveraging the SMC-based parameter predictive posterior random measure p M (\u03b8 t+1 |H 1:t ).We compute the quantile function of interest by first evaluating the expected reward at each round t based on the available posterior samples, i.e., \u00b5The convergence of quantile estimators generated by SMC methods has been explicitly proved in Random measure p M (\u03b8 t+1,a |H 1:t ) in Equation ( The dynamic linear model is a flexible and widely used framework to characterize timeevolving systems with parameters L a \u2208 R d\u0398 a \u00d7d\u0398 a and \u03a3 a \u2208 R d\u0398 a \u00d7d\u0398 a -recall that we specify distinct transition densities per-arm.With known parameters L a and \u03a3 a , the transition distribution p(\u03b8 t,a |\u03b8 t-1,a ) is Gaussian with closed-form updates, i.e., \u03b8 t,a \u223c N (\u03b8 t,a |L a \u03b8 t-1,a , \u03a3 a ).For the more interesting case of unknown parameters, we marginalize parameters L a and \u03a3 a of the transition distributions utilized by the proposed SMC-based Bayesian policies, i.e., we Rao-BlackwellizeRequire: A, p(\u03b8 a ), p(\u03b8 t,a |\u03b8 t-1,a ), p a (Y |x, \u03b8), \u2200a \u2208 A."
    },
    {
      "header": "Evaluation",
      "content": "Section 4.4 illustrates the ability of SMC-based bandit policies to capture non-stationary trends in personalized news article recommendations.The main evaluation metric is the cumulative regret of the bandit agent, as defined in Equation ( We simulate the following two-armed, contextual (x t \u2208 R 2 , \u2200t), linear Gaussian bandit:\u03b8 t,a=0,0 \u03b8 t,a=0,1 = 0.9 -0.1 -0.1 0.9where \u03f5 a=0 \u223c N (\u03f5|0, 0.01 \u2022 I) , p(\u03b8 t,a=1 |\u03b8 t-1,a=1 ) :\u03b8 t,a=1,0 \u03b8 t,a=1,1 = 0.9 0.1 0.1 0.9where \u03f5 a=1 \u223c N (\u03f5|0, 0.01\u03b8 t,a=0,0 \u03b8 t,a=0,1 = 0.5 0.0 0.0 0.5where \u03f5 a=0 \u223c N (\u03f5|0, 0.01 \u2022 I) , p(\u03b8 t,a=1 |\u03b8 t-1,a=1 ) :\u03b8 t,a=1,0 \u03b8 t,a=1,1 = 0.9 0.1 0.1 0.9where \u03f5 a=1 \u223c N (\u03f5|0, 0.01(43) The expected rewards driven by the dynamics of Equations ( Empirical results for SMC-based Bayesian policies in scenarios described by Equations ( We study linear dynamics with Gaussian reward distributions with known parameters in Figure We observe satisfactory cumulative regret performance in Figure We observe in Figures We illustrate in Figures 2e-2f the most realistic, yet challenging, non-stationary contextual Gaussian bandit case: one where none of the parameters of the model are known."
    },
    {
      "header": "Conclusion and discussion",
      "content": "Given that SMC posteriors converge to the true posterior under suitable conditions On the one hand, A theoretical analysis of the dependency between the dynamic bandit model's characteristics, the resulting rate of optimal arm changes, and SMC posterior convergence guarantees, leading to formal regret bounds for the proposed SMC-based Bayesian policies, is an open research direction.We here apply the proposed SMC-based Bayesian policies as in Algorithm 1 to the original settings where Thompson sampling and Bayes-UCB were derived, i.e., for stationary bandits with Bernoulli and contextual, linear Gaussian reward functions Empirical results for these bandits is provided in Section A.2, while the stationary logistic bandit case is evaluated in Section A.3, where we also evaluate the impact of sample size M in the SMC-based bandit algorithms.In stationary bandits, there are no time-varying parameters, i.e., \u03b8 t = \u03b8, \u2200t."
    }
  ]
}
