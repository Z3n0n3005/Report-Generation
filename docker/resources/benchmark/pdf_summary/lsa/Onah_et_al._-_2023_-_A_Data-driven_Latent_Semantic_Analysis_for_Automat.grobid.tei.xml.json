{
  "id": "Onah_et_",
  "name": "Onah_et_al._-_2023_-_A_Data-driven_Latent_Semantic_Analysis_for_Automat.grobid.tei.xml",
  "segments": [
    {
      "header": "I. INTRODUCTION",
      "content": "With so many documents being extracted from social media, review comments from online plaWorms and microblogs as TwiVer, a huge amount of natural language data is being mined and are available to be analysed AutomaDc text summarizaDon is the process of performing specific NLP task by producing a concise summary of documents (single or mulDple) without any manual support while preserving the meaning or important points of the original document \u2022 How automated text summarizaDon techniques were used in an extracDve summary of arDcles?\u2022 How topic modelling models were used in producing emerging terms that are related to mulDple and different journal arDcles?"
    },
    {
      "header": "II. RELATED WORK",
      "content": "Unless these text data are extracted and make meaning, then the most important and relevant informaDon would be lost."
    },
    {
      "header": "A. Summariza-on",
      "content": "SummarizaDon is a technique in NLP that is used for condensing or summarising huge texts into smaller versions taking care not to omit the main relevant informaDon contained in the document 1) Extrac-ve approach: ExtracDve summarizaDon approach considers the top N sentences based on their score rankings for the summary generaDon 2) Abstrac-ve approach:In abstracDve text summarizaDon technique, this follows the convenDon of unsupervised approach where machine learning paradigms such as deep learning plays a big role in generaDng the document summary In this study, we decided to use extracDve approach for arDcle summarizaDon, because we wanted all parts of the sentences that will be summarised to be from the original document."
    },
    {
      "header": "B. Topic Modelling",
      "content": "Topic modelling is the process of labelling and describing documents into topics."
    },
    {
      "header": "C. Latent Dirichlet Alloca-on",
      "content": "Latent Dirichlet AllocaDon (LDA) is a technique applied in topic modelling introduced by"
    },
    {
      "header": "III. METHODS",
      "content": "One of the key approaches that was used in the experiment was the 'pyLDAvis.gensim.prepare' method which takes as an argument our LDA model, the corpus and the derived lexicon which contains the dicDonary terms for the study"
    },
    {
      "header": "A. Model Descrip-on",
      "content": "Most of the processing of the text was performed with the python Natural Language Toolkit (NLTK)"
    },
    {
      "header": "1)",
      "content": "Sentence scoring method: : In this study, a scoring funcDon is introduced to generate the sentence score dicDonary which hold the value assigned to each sentence"
    },
    {
      "header": "Sentscores[S] = Wordfreq[W]",
      "content": "Therefore, new sentences are added into the sentence dicDonary scores."
    },
    {
      "header": "Sentscores[S]+ = Wordfreq[W]",
      "content": "(2)2) Word frequency:: DicDonary of word frequency corpus was generated within the model."
    },
    { "header": "Word", "content": "(4)" },
    {
      "header": "B. Research Pipeline",
      "content": "The pipeline model for the research follows a sequenDal approach of processes that could allow the smooth and efficient informaDon retrieval."
    },
    {
      "header": "2)",
      "content": "Finally, these extracted paragraphs text are combined to form a single string to store the clean web content for further topic model processing (see Figure"
    },
    {
      "header": "3)",
      "content": "Stopwords: We further removed a list of stop-words from the propocessed arDcles."
    },
    {
      "header": "4) Topic Modelling & Visualiza-on:",
      "content": "This study was able to reveal prevalence of terms that emerged within the documents and show their relevance by how the projecDon of the topic modelling circle and the size of a word in the result visualisaDon."
    },
    {
      "header": "IV. MODEL",
      "content": "Defining seman-c significance we define the semanDc significance of term t to the topic n given the parameter weight of the (\u03bb) where(0 \u2264 \u03bb \u2264 1)"
    },
    {
      "header": "B. Defining Saliency Term",
      "content": "In this study we define saliency term as given a word 'w 0 , we compute its minimal probability P(TM/w)."
    },
    { "header": "U(6)", "content": "" },
    {
      "header": "=5",
      "content": "The frequency and populaDon of terms are denoted by the size of the topic circles and also the inter-topic distance denote how closely related the topics are."
    },
    {
      "header": "V. LATENT SEMANTIC ANALYSIS",
      "content": "Latent SemanDc Analysis (LSA) is a robust Algebraic and StaDsDcal method which extracts hidden semanDc structures of words and sentences."
    },
    {
      "header": "A. Sample Extracted Summary",
      "content": "We used the heap queue (heapq) library to select the most or very useful sentences."
    },
    {
      "header": "B. Findings",
      "content": "The summary result has revealed very interesDng findings of genes that are associated to some Cancerous and type 2 diabetes diseases (see Table"
    },
    {
      "header": "VI. ROUGE: RELIABILITY & VALIDITY OF MODEL",
      "content": "ROUGE is a metric evaluaDon model which stands for Recall Oriented Understudy for Gisting Evaluation."
    },
    {
      "header": "TABLE III ROUGE METRICS MEASUREMENT SUMMARIES",
      "content": "System and Human Annotated Summaries Type Summary SSummary 'Some of the genes in the BCAA metabolic pathway such as MLYCD (rank 164)HADHB (rank 354)IVD (rank 713)MUT (rank 921)and PCCB (rank 684) are also ranked highly by Hridaya."
    },
    {
      "header": "Comparing the system generated summary with a new human",
      "content": ""
    },
    {
      "header": "A. Procedure: Recall & Precision",
      "content": "The main reason why ROUGE-1 could be considered over others or in conjuncDon with ROUGE -2 or even other fine granularity measures is because it reveals the fluency of the summaries or if used in a translaDon task."
    },
    {
      "header": "VII. RESULTS & FINDINGS",
      "content": "The terms in the topic modelling show text which are mostly frequent in the document these were depicted by the size of the circle (as seen in Figures The distance between two or more topics is an approximaDon of their semanDc relaDonship."
    },
    {
      "header": "IX. DISCUSSION",
      "content": "We proposed and evaluated the model on unstructured datasets which show some results comparable to the current state-of-the-art topic modelling techniques without depending on modificaDons using any linguisDc informaDon models"
    },
    {
      "header": "X. CONCLUSION",
      "content": "AutomaDc summarizaDon is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document"
    }
  ]
}
