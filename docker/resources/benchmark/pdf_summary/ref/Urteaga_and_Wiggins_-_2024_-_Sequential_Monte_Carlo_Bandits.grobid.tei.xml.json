{
  "id": 9109274723410202958,
  "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content": "The multi-armed bandit problem involves sequentially choosing actions (e.g., prescribing treatments, allocating resources, displaying content) to maximize cumulative rewards, extending to real-world scenarios where decisions are guided by available context and adapting to non-stationary and nonlinear reward structures using Sequential Monte Carlo methods."
    },
    {
      "header": "Background and preliminaries",
      "content": "The stochastic multi-armed bandit framework encompasses both stationary bandits (where parameters remain constant over time) and non-contextual bandits (with fixed context), aiming to maximize cumulative rewards by choosing actions based on historical observations and Bayesian inference of uncertain reward parameters, utilizing techniques like Thompson Sampling and Bayesian Upper Confidence Bound policies, and overcoming challenges posed by dynamic parameters and non-stationary reward distributions through Sequential Monte Carlo methods, which include resampling to mitigate degenerate weight distributions in importance sampling."
    },
    {
      "header": "SMC for multi-armed bandits",
      "content": "We utilize Sequential Monte Carlo (SMC) to compute posterior distributions and sufficient statistics for a diverse range of multi-armed bandit (MAB) problems, specifically addressing non-stationary bandits modeled via linear dynamical systems with complex, nonlinear reward functions subject to non-Gaussian stochastic innovations. This approach facilitates Bayesian policies to update parameter uncertainties over time, crucial for optimizing actions in dynamic environments using algorithms like Thompson Sampling and Bayesian Upper Confidence Bound (Bayes-UCB)."
    },
    {
      "header": "Evaluation",
      "content": "Results demonstrate that Sequential Monte Carlo (SMC)-based policies achieve effective exploration-exploitation tradeoffs across various stationary bandit parameterizations and sizes, validating their performance through simulations of dynamic linear models and diverse MAB environments with Gaussian and logistic reward functions, showcasing sublinear cumulative regret and adaptability to non-stationary trends in personalized news article recommendations."
    },
    {
      "header": "Conclusion and discussion",
      "content": "We introduced a Sequential Monte Carlo (SMC)-based framework for multi-armed bandits (MABs), integrating SMC inference with Bayesian bandit policies like Thompson Sampling and Bayes-UCB, thereby extending their application to nonlinear and time-varying bandit environments. This framework enables interpretable modeling of complex reward functions, demonstrated through simulations and practical scenarios such as personalized news article recommendations, showing effective exploration-exploitation tradeoffs and adaptability to non-stationary dynamics, crucially relying on accurate computation of SMC random measures for posterior approximation and performance enhancement."
    }
  ]
}
