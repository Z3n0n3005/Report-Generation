{
  "id": 6245311140815938623,
  "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content": "The Mixture-of-Depths (MoD) technique in transformer models allows dynamic allocation of compute per token per layer, enabling similar or improved performance with less computational cost compared to traditional transformers by selectively applying computations or passing tokens through residual connections."
    },
    {
      "header": "Background",
      "content": "Recent work on making transformer architectures more efficient includes conditional computation methods like \"early exiting\" and the Mixture-of-Depths (MoD) technique, which dynamically decides when to skip or process tokens through layers, reducing computational cost while maintaining performance by learning efficient routing decisions."
    },
    {
      "header": "Implementing Mixture-of-Depths Transformers",
      "content": "To enforce a total compute budget per forward pass in transformers, we utilize capacity, which limits the number of tokens participating in computations, and employ learned routing schemes such as token-choice and expert-choice to dynamically route tokens, aiming to reduce the total flops by bypassing certain computations while maintaining prediction accuracy."
    },
    {
      "header": "Results",
      "content": "Model #3 achieves equal performance to the isoflop optimal baseline but is 66% faster due to fewer flops per forward pass, demonstrating that MoD transformers can perform as well or better than isoflop-optimal baselines while being faster, thanks to learned routing and efficient capacity usage."
    },
    {
      "header": "Discussion",
      "content": "Learned routing decisions in MoD transformers, similar to mixture-of-experts models, enable efficient allocation of computational resources by deciding whether tokens should undergo self-attention and MLP processing, thereby saving flops and allowing for model scaling or extended training, while addressing challenges in non-causal routing during autoregressive sampling."
    }
  ]
}
