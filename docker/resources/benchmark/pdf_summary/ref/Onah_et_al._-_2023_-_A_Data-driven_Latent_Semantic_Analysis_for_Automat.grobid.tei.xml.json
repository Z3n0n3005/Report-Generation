{
  "id": 5926491902012788021,
  "name": "Onah_et_al._-_2023_-_A_Data-driven_Latent_Semantic_Analysis_for_Automat.grobid.tei.xml",
  "segments": [
    {
      "header": "I. INTRODUCTION",
      "content": "The study introduces a novel approach to topic modeling by using extractive summarization on over 100 articles from PubMed Central discussing genes and associated diseases, feeding these summaries into a Latent Dirichlet Allocation (LDA) model to identify commonalities and themes among the articles."
    },
    {
      "header": "II. RELATED WORK",
      "content": "The exponential growth of global text data necessitates efficient extraction and summarization techniques to distill meaningful and essential information, crucial for retrieving relevant insights from vast datasets across diverse sources."
    },
    {
      "header": "A. Summariza-on",
      "content": "Summarization in NLP involves condensing large texts while preserving essential information, facilitating integration into various applications like classification, automatic question answering, computational journalism, financial and news summarization, and foreign language translation, primarily through extractive methods which select top-ranking sentences based on their relevance scores to generate concise summaries from original documents."
    },
    {
      "header": "B. Topic Modelling",
      "content": "Topic modeling involves categorizing and summarizing documents into distinct topics using unsupervised machine learning techniques, extracting core themes from weighted graphical representations of collections of documents to facilitate their application in NLP tasks such as information retrieval and analysis, implemented in this study using scikit-learn and gensim libraries to extract topics from text corpora using the `gensim.models.ldamodel.LdaModel` with parameters specifying the corpus, number of topics, and dictionary of terms."
    },
    {
      "header": "C. Latent Dirichlet Alloca-on",
      "content": "Latent Dirichlet Allocation (LDA) is a topic modeling technique that probabilistically generates topics based on the likelihood of terms appearing within documents, allowing for the representation of documents as mixtures of topics where words can appear in multiple topics, facilitating the discovery of underlying themes across datasets."
    },
    {
      "header": "III. METHODS",
      "content": "The study employed initial NLP techniques for preprocessing data for extractive summarization, utilizing sentence scoring to identify high-frequency content, followed by cosine similarity calculations among sentences to select top-ranking ones for input into LDA topic modeling, which was facilitated by methods like `pyLDAvis.gensim.prepare` and `gensim.models.ldamodel.LdaModel` to generate a dictionary of terms and construct a vectorized corpus for analysis."
    },
    {
      "header": "A. Model Descrip-on",
      "content": "The research model focused on utilizing NLP's topic modeling to extract specific information from extensive journal publications, employing an extractive summarization approach that systematically scores and selects sentences based on predefined criteria, ensuring grammatically natural summaries without deep linguistic analysis, applicable across various types of articles including media and blogs."
    },
    {
      "header": "1)",
      "content": "The study introduced a sentence scoring method to assign probabilities to sentences for inclusion in summaries, emphasizing the selection of top-ranking sentences based on their scores, crucial for determining the relevance and quality of extracted information from documents."
    },
    {
      "header": "Sentscores[S] = Wordfreq[W]",
      "content": "During sentence processing, sentences' lengths are adjusted within the sentence score dictionary by adding or removing specific values, ensuring newly processed sentences are integrated into the scoring system; if a processed sentence is not already in the dictionary, its word frequencies are incorporated to determine its score."
    },
    {
      "header": "Sentscores[S]+ = Wordfreq[W]",
      "content": "The model generated a dictionary of word frequencies based on automated selection from the corpus, prioritizing words by their prevalence, with sentences shorter than 30 words selected for summarization; the maximum weighted frequency of each word was calculated using the product of its frequency and a corresponding value, contributing to the final summary generation."
    },
    { "header": "Word", "content": "" },
    {
      "header": "B. Research Pipeline",
      "content": "The research followed a sequential pipeline for efficient information retrieval, beginning with data collection where approximately 100 papers related to medical science, specifically discussing diseases and mutated genes, were scraped from PubMed Central using a search combination of 'gene' and 'disease', necessitating preprocessing, cleaning, and summarization for subsequent topic modeling analysis."
    },
    {
      "header": "2)",
      "content": "The preprocessing and feature extraction involved cleaning raw, unstructured data scraped from PubMed journals by removing HTML tags, special characters, symbols, and numbers using NLP tools like BeautifulSoup, regular expressions, and NLTK, parsing the source code to extract textual content from paragraphs, and filtering additional special characters to prepare a consolidated, clean text string for subsequent topic modeling analysis."
    },
    {
      "header": "3)",
      "content": "Stopwords, non-essential words such as pronouns, were removed from the preprocessed articles to streamline the final summary generation."
    },
    {
      "header": "4) Topic Modelling & Visualiza-on:",
      "content": "This study utilized pyLDAvis for interactive visualization, illustrating the prevalence and relevance of terms within documents through topic modeling circles where larger circles denote higher topic projection, facilitating the extraction and display of information from the derived LDA topic models."
    },
    {
      "header": "IV. MODEL",
      "content": "Defining semantic significance involves determining the importance of term ( t ) to topic ( n ) based on its probability within the topic and across the entire vocabulary, using a parameter lambda (where ( 0 leq lambda leq 1 )), and considering the minimal probability ( p_t ) of the term in the lexical corpus."
    },
    {
      "header": "B. Defining Saliency Term",
      "content": "In this study, saliency of a term is defined as its minimal probability ( p(w mid tm) ) within an LDA topic model ( tm ), indicating the likelihood that the word ( w ) was generated by the model; uniqueness of each identified word ( w ) is determined by comparing ( p(w mid tm) ) with the marginal probability ( p(tm) ), computed across 5 topics and 10 passes in the LDA topic modeling process."
    },
    { "header": "U(6)", "content": "" },
    {
      "header": "=5",
      "content": "The uniqueness of each term is defined by its semantic relevance across topics, where terms may be associated with multiple topics, represented by their frequency and depicted by the size of topic circles and inter-topic distances, clarifying their distribution and removing ambiguity in topic association; for instance, prevalent terms like \"gene,\" \"disease,\" and \"expression\" are prominently visualized across different topics, indicating their saliency and semantic connection across topic compositions."
    },
    {
      "header": "V. LATENT SEMANTIC ANALYSIS",
      "content": "Latent Semantic Analysis (LSA) is an effective algebraic and statistical method for uncovering hidden semantic structures in words and sentences, used here to extract essential but non-original features from datasets, including generating a concise summary of 100 PubMed articles on genes associated with cancer, evaluated for efficacy using the ROUGE metric system."
    },
    {
      "header": "A. Sample Extracted Summary",
      "content": "The study utilized sentences with the highest sentence scores for summarization, employing the heapq library to prioritize sentences based on their weights, where the threshold determined the number of sentences included in the summary, demonstrating varied results despite a maximum word frequency constraint of less than 30."
    },
    { "header": "B. Findings", "content": ""},
    {
      "header": "VI. ROUGE: RELIABILITY & VALIDITY OF MODEL",
      "content": "ROUGE is an evaluation metric originally based on BLEU, used to assess the accuracy of system-generated summaries against human-created summaries (model summaries), employing measures such as ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, and ROUGE-S to validate the external quality of the model."
    },
    {
      "header": "TABLE III ROUGE METRICS MEASUREMENT SUMMARIES",
      "content": "The study found that the system-generated summary, aligned closely with the human-annotated reference summary, yielded better average results across all ROUGE metrics (recall, precision, and F1 score), demonstrating higher performance with recall slightly exceeding 83%, precision slightly over 85%, and F1 score slightly over 84%."
    },
    {
      "header": "Comparing the system generated summary with a new human",
      "content": ""
    },
    {
      "header": "A. Procedure: Recall & Precision",
      "content": "In this study, human-annotated reference summaries were generated from the cleanhtml.txt document, with recall in ROUGE metrics indicating how much of the human summary is captured by the system-generated summary, while precision measures the percentage overlap of bigrams between the system and reference summaries, culminating in the F1-score as a combined measure of their validity."
    },
    {
      "header": "VII. RESULTS & FINDINGS",
      "content": "The study utilized topic modeling to identify frequently occurring terms in the documents, represented by the size of circles in visualizations, indicating semantic relationships between closely related topics like topics 1, 2, and 3, particularly focusing on terms like \"gene\" and \"disease.\""
    },
    {
      "header": "IX. DISCUSSION",
      "content": "In this study, we presented a fully data-driven approach for automatic text summarization, evaluating the model on unstructured datasets with results comparable to state-of-the-art topic modeling techniques, emphasizing the importance of automation in alleviating the laborious task of manual summarization and its wide applicability across various NLP tasks."
    },
    {
      "header": "X. CONCLUSION",
      "content": "This study proposed fully automated text summarization methods using an LDA model, aiming to extract accurate summaries from single and multiple documents, including cross-language capabilities for translation into English and other languages, highlighting the challenge and importance of retaining original document meaning."
    }
  ]
}
