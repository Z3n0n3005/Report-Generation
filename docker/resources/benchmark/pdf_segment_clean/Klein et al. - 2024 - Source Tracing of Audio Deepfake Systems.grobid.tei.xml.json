{"id": 7346869386864968057, "name": "Klein et al. - 2024 - Source Tracing of Audio Deepfake Systems.grobid.tei.xml", "segments": [{"header": "Introduction", "content": "in recent years, deepfake generation and detection have attracted significant attention. on january 21, 2024, an advanced text-to-speech (tts) system was used to generate fake calls to manipulate the voice of us president, joe biden, encouraging voters to skip the 2024 primary election in the state of new hampshire. this incident underscores the critical need for deepfake detection that is reliable and trusted. thus, explainability in deepfake detection systems is crucial. within this research area, the task of deepfake audio source attribution has recently been gaining interest. the goal of this task is to predict the source system that generated a given utterance. for example, the study inaims to predict the specific attack systems used to produce utterances in asvspoof 2019. this approach of directly identifying the name of the system misses the opportunity to categorize the spoofing systems based on their attributes. such attribute-based categorization allows for better generalization to spoofing algorithms that are unseen in training but are composed of building blocks, such as acoustic models or vocoders, that are seen. along these lines, authors inpropose a more generalizable approach by classifying the vocoder used in the spoofing system. authors inexplore classifying both the acoustic model and vocoder, finding that the acoustic model is more challenging to predict. the work intakes this further by proposing to classify several attributes of spoofing systems in asvspoof 2019 la: conversionfigure: illustration of proposed frameworks for spoofing attribute-classification. top: end-to-end learning from audio. bottom: two-stage learning that includes a traditional countermeasure (cm) and an auxiliary classifier trained on embeddings.model 1 , speaker representation, and vocoder. however, their findings demonstrate accuracy challenges in discerning speaker representation. another drawback of their evaluation protocol is that the asvspoof 2019 dataset is relatively outdated as there have been many advancements in voice cloning techniques in the last five years. finally, their choice of categories for acoustic model and vocoder are very broad (e.g.\"rnn related\" for acoustic model and \"neural network\" for vocoder) and may not be that useful in narrowing down the identity of the spoofing system.in this work, we investigate two attribute classification strategies as illustrated in fig.: an end-to-end learning method which trains standalone systems for each attribute and a twostage learning method which leverages the learned representations of existing countermeasure systems. to this end, we leverage three state-of-the-art systems, namely resnet, self-supervised learning (ssl), and whisper. in addition to identifying the acoustic model and vocoder, we propose classifying the input type (i.e. speech, text, or bonafide) rather than speaker representation. this allows for distinguishing between tts and vc systems. as an anchor to previous work, we evaluate our methods on the asvspoof 2019 protocol designed by. to address the limitations of the outdated asvspoof-based protocol, we design a new protocol based on the recent mlaad dataset which consists of multilingual utter-"}, {"header": "Attribute classification of spoof systems", "content": "in this section, we describe our approaches for classifying the input type, acoustic model, and vocoder of the spoofing system used to generate a given audio.we present two strategies for leveraging existing state-of-theart (sota) spoofing countermeasure (cm) systems for the task of component classification:• our end-to-end (e2e) approach takes an existing cm architecture and trains the whole model for each of the multi-class component classification tasks separately, as depicted in the top part of fig.. • the two-stage approach, shown in the bottom of fig., splits training into two steps: first an existing cm is trained for the standard binary spoof detection task; next, the cm backbone is frozen and a lightweight classification head is trained on the cm's embeddings for each separate component classification task. for the classification head, we use the simple feed forward architecture from the back-end model of the resnet spoof detection system described in. while the second approach is limited to the information that the binary-trained cm learns, it is very attractive in practice: in addition to the reduction in computational costs, existing binary systems can be trained on significantly more data than we have component labels for and enhancing them with an auxiliary head rather than replacing them with a modified e2e system is much safer for models that run in production.we used three different cms to validate our hypothesis. these systems are well known and have reported excellent detection performance on several datasets. resnet. this system consists of a front-end spoof embedding extractor and a back-end classifier. the front-end model is known as the resnet18-l-fm model, as detailed in. to enhance the model's generalization capability, large margin cosine loss(lmcl) and random frequency masking augmentation are applied during training. the back-end model is trained using the spoof embedding vectors for the classification tasks described in section 2. the back-end classifier is a feed forward neural network with one fc layer described in. 2 mlaad protocol: doi.org/10.5281/zenodo.11593133self-supervised learning. ssl-based front-ends have attracted significant attention in the speech community, including spoofing and deepfake detection. the ssl-based cm architectureis a combination of ssl-based front-end feature extraction and an advanced graph neural network based backend, named aasist. the 160-dimensional cm embeddings are extracted prior to the final fully-connected output layer. the ssl feature extractor is a pre-trained wav2vec 2.0 model, the weights of which are fine-tuned during cm training.whisper. the whisper model is based on an off-the-shelf encoder-decoder transformer architecture for automatic speech recognition (asr). the whisper-based cm architectureis a combination of whisper-based front-end feature extraction and light convolution neural network (lcnn)as a back-end. for the front-end feature extraction, the whisper embedding is concatenated with 128-dimensional linear frequency cepstral coefficients (lfccs)along with their delta and double-delta features. the 768-dimensional cm embeddings are extracted prior to the final fully-connected output layer. the reader is referred tofor further technical details."}, {"header": "Datasets and protocols", "content": "two publicly available spoofing detection benchmarks are used in our study: the asvspoof 2019 laand the most recent mlaad dataset.the asvspoof 2019 la dataset has three independent partitions: train, development, and evaluation. spoofed utterances are generated using a set of different tts, vc, and hybrid tts-vc algorithms. to compare our methods against those presented in [5], we adopt their protocol partition as detailed in table. notably, it only includes a train and development set, so we do not do any hyper-parameter search on this protocol. while we use the same categories asfor the acoustic and vocoder tasks, we create a new \"input type\" task which is helpful to separate between tts and vc systems. tablesummarises the statistics for each partition used for the different attribute classification tasks on the asvspoof 2019 dataset.mlaad consists of tts attacks only, however it includes 52 different state-of-the-art spoofing algorithms. we manually label the acoustic models and vocoders based on the available metadata.since mlaad includes only tts systems, we focus on acoustic model and vocoder classification without any input-type prediction. for end-to-end systems such as vits and bark, we use the name of the full system as the acoustic model and vocoder labels. additionally, while the mlaad dataset labels 19 different architectures, our protocol groups several systems that are identical aside from their training data. for example, the systems \"jenny\", \"vits\", \"vits-neon\", and \"vits-mms\" are all labeled with the same acoustic model and vocoder category \"vits\". for the bonafide class, we include bonafide samples from the multilingual m-ailabs  dataset. we divide the data into train, development, and evaluation partitions while preventing speaker overlap. to enable this for the spoof samples, we assign voice labels using spherical k-means clustering on embeddings from the stateof-the-art speaker verification system, ecapa-tdnn.we use the elbow criteria on the inertia values to select k=75 clusters. we remove two vocoders, griffin-limand fullband-melgan, since they each have a cluster containing most of their samples. the resulting acoustic model and vocoder labels along with their number of examples in each partition are presented in tableand table, respectively."}, {"header": "Experimental Results", "content": "resnet and ssl models use 4 second (s) raw audio as input, whereas the whisper model processes on 30s audio. for resnet, lfcc features are extracted using 20ms window and 10ms frame-shift along with its delta and double delta features. since fine-tuning large ssl models requires high gpu computation, experiments with ssl are performed with a smaller batch-size of 16 and a lower learning rate of 10 -6 . we used the same set-up for ssl and whisper based models as describe inand, respectively. ssl and whisper based models are fine-tuned on asvspoof and mlaad datasets in their respective experiments, whereas the resnet model is trained from scratch. for the auxiliary classifier, a batch size of 256 and a learning rate of 10 -3 is used with no hyper-parameter tuning. the best model is chosen based on dev set accuracy and average f1-score for asvspoof and mlaad experiments, respectively.our results are compared with the previous studyon asvspoof 2019 in terms of unweighted accuracy in table.this study introduces a novel task, predicting input types, which the previous study did not explore. we train classification heads using fixed resnet, ssl, and whisper based binary spoof detection models named as, resnet (two-stage), ssl (two-stage), and whisper (two-stage). these experiments achieve 97.8%, 96.7% and 78.4% accuracy, respectively. our ssl model fine-tuned end-to-end, ssl (e2e), further improves accuracy to 99.9%.acoustic model classification: several of our models surpass the previous study's highest accuracy of 88.4%, achieved table: results in terms of accuracy (%) on the asvspoof 2019 la dataset. methods presented inare included in the top two rows for comparison with our methods. we show our results when training a classification head on top of fixed embeddings from the binary cm backbone (\"two-stage\") as well as when training the cm backbone end-to-end for this task (\"e2e\"). by the multi-task-trained rawnet2 model in. specifically, ssl (two-stage), resnet (two-stage), and ssl (e2e) achieve accuracies of 91.4%, 92.6%, and 99.4% (a 12.4% relative improvement over the previous study), respectively. the substantial increase in accuracy may be due to the fact that our models are specifically trained for these tasks, unlike the previous study's multi-task approach that jointly trained on acoustic, vocoder, and speaker representation tasks.vocoder classification: our ssl (e2e) model slightly outperforms the previous study with an accuracy of 84.6% (a 0.1% relative improvement). unlike the acoustic model, we do not see the same level of improvement. analyzing errors from our top-performing model, ssl (e2e), we find that 882 out of 896 mis-predictions occur from predicting attack a07 as \"neural network\". attack a07 uses a non-neural world vocoder, however it also uses a gan-based post filter that identifies areas of the waveform to mask out (seefor further details). this post-filter is not seen in training and must have consistently affected the final waveform in a way that mangled the resemblance to traditional vocoder audio. aside from this one kind of error, our ssl (e2e) model's accuracy is 99.7%.we report results in terms of macro-averaged f1 and accuracy scores in table. with the larger number of specific vocoder and acoustic model categories compared to the asvspoof pro- challenge may stem from overlapping voices among different models in the test set, as discussed in the previous error analysis section. additionally, we observe distinct clusters of acoustic models with similar architectures: xtts-v1 and xtts-v2; as well as neural-hmmand overflow(which combines neural-hmm with normalizing flows)."}, {"header": "Conclusions and Discussions", "content": "in this paper, we propose three multi-class classification tasks to give more explanatory predictions in the place of traditional binary spoof detection: input-type, acoustic model, and vocoder classification. we experiment with two methods of leveraging open source spoof detection systems to accomplish this task and evaluate them on a recently introduced asvspoof 2019 protocol as well as a new protocol that we design using the more modern mlaad dataset. our ssl (e2e) method outperforms the previous study on asvspoof that we compare to on the acoustic and vocoder tasks with relative improvements in accuracy of 12.4% and 0.1% respectively while achieving 99.9% accuracy on our newly introduced input-type classification task. on our mlaad protocol which includes a greater number of vocoder and acoustic categories from more modern tts systems, our resnet (e2e) model yields an average f1 score of 82.3% for the acoustic model and 93.3% for the vocoder classification task. our findings support existing literature that suggest that the vocoder is easier to distinguish than the acoustic model. additionally, we observe that the acoustic models of systems that produce similar voices are more challenging to discriminate. thus, a potential area of future study is to more explicitly ignore voice-specific information.our experiments with two-stage classification methods that leverage embeddings from binary spoof detection systems show promise, though they underperform on mlaad compared to the full model fine-tuning methods. future research in this area is crucial as models that augment rather than replace existing binary spoof detection systems are attractive, especially in industry where changes in the behavior of the binary detection system require thorough evaluation. thus, one possible future experiment is to assess where in the binary model contains the most useful information for discriminating the different spoof system components. additionally, assessing how the choice of loss function for the binary model affects the downstream multiclass performance could give insight into which existing models are best suited to being leveraged for two-stage learning."}]}