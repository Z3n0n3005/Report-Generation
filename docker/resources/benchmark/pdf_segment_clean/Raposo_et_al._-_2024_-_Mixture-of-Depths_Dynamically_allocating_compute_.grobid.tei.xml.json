{
  "id": "Raposo_e",
  "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content": "Not all problems require the same amount of time or effort to solve. Analogously, in language modeling not all tokens and sequences require the same time or effort to accurately make a prediction. And yet, transformer models expend the same amount of compute per token in a forward pass. Ideally, transformers would use smaller total compute budgets by not spending compute unnecessarily.Conditional computation is a technique that tries to reduce total compute by expending it only when needed Here we consider the problem of language modeling using a static compute budget that can be made less than that used by a vanilla transformer. The network must learn how to dynamically allocate the available compute by making decisions per-token, in each layer, about where to spend compute from the available budget. In our implementation total compute is user defined and unchanging prior to training, rather than being a function of the network's on-the-fly decisions. Thus, hardware efficiency gains-such as reduced memory footprint, or reduced FLOPs per forward pass-can be anticipated and exploited ahead of time. As we will show, these gains can be had without sacrificing overall performance.We leverage an approach akin to Mixture of Experts (MoE) transformers, in which dynamic token-level routing decisions are made across the network depth. Departing from MoE, we choose to either apply a computation to a token (as would be the case for a standard transformer), or pass it through a residual connection (remaining unchanged and saving compute). Also in contrast to MoE, we apply this routing to both forward MLPs and multi-head attention. Since this therefore also impacts the keys and queries we process, the routing makes decisions not only about which tokens to update, but also which tokens are made available to attend to. We refer to this strategy as Mixture-of-Depths (MoD) to emphasize how individual tokens pass through different numbers of layers, or blocks, through the depth of the transformer (see figure The MoD technique also allows one to trade-off performance with speed. On the one hand, one can train an MoD transformer that improves upon vanilla transformers by as much as 1.5% on the final log probability training objective for equivalent training FLOPs (isoFLOP), and while taking an equivalent amount of wall-clock time to train. On the other hand, one can train an MoD transformer that achieves training loss parity with an isoFLOP optimal vanilla transformer, but which uses a fraction of the FLOPs (upwards of 50%) per forward pass, and hence is faster to step. Together, these results imply that MoD transformers learn to route intelligently (i.e., skipping computations that are unnecessary) since they can achieve equal or better log probabilities per sequence despite a smaller FLOP footprint per forward pass."
    },
    {
      "header": "Background",
      "content": "The transformer architecture has become the workhorse of a revolution in practical artificial intelligence, bringing unprecedented capabilities at the cost of expensive training runs and serving procedures. This has spurred tremendous interest in making transformer architectures more efficient A wide variety of recent work has developed conditional computation methods for transformers. Some of this work focuses on \"early exiting\", that is, learning to decide when to end computation on a given token, allowing the token to skip any remaining transformer layers after the exit decision is made Other work has developed methods for iterating transformer layers with shared weights for an adaptive number of steps CoLT5 MoD focuses on the decoder-only setting, and so we propose a predictive router to enable efficient inference for conditional computation in transformers.One successful formulation of conditional computation is the the \"mixture-of-experts\" layer (MoE) as introduced by "
    },
    {
      "header": "Implementing Mixture-of-Depths Transformers",
      "content": "Our high-level strategy is as follows:\u2022 Set a static compute budget that is less than that of an equivalent vanilla transformer by limiting the number of tokens in a sequence that can participate in a block's computations (i.e., selfattention and subsequent MLP). For example, while a vanilla transformer might permit all the tokens in a sequence to participate in self-attention, we might limit the number to 50% of the tokens in a sequence. See section 3.1.\u2022 Use a per-block router to emit a scalar weight for each token, which expresses the router's preference for that token to participate in a block's computations or to route around it. See section 3.2. \u2022 Identify the top-\ud835\udc58 scalar weights (per sequence, per block) to select those tokens that will participate in a block's computations. Since precisely \ud835\udc58 tokens will participate in the block's computations, the computation graph and tensor sizes remain static throughout training; it is merely the tokens' participation that is dynamic and context-sensitive, as determined by the router. See section 3.3.We then discuss some complications when sampling post-training in section 3.5.To enforce a total compute budget per forward pass we leverage the notion of capacity, which defines the total number of tokens that comprise the input to a given computation (e.g., the tokens participating in self-attention, a given expert in MoE transformers, etc). For example, the self-attention and MLP in each vanilla transformer block have a capacity of \ud835\udc47-the total number of tokens across the sequence and batch. MoE transformers, on the other hand, use a capacity less than \ud835\udc47 per expert MLP so as to more evenly divide the total compute across each expert. But, since they use multiple experts per block, their total capacity is approximately equal to that of a vanilla transformer.Generally, it is the token capacity that determines the total FLOPs for transformers that use conditional computation, rather than the outcomes of any routing decisions. This is because staticgraph implementations account for the worst-case scenarios decisions; e.g., a computation's inputs will be padded to its capacity amount even if relatively few tokens actually end up routing to it, and/or tokens will be dropped from the computation if the capacity is exceeded.We can achieve our goal of using a smaller compute budget per forward pass compared to a vanilla transformer by lowering the capacity of the computations. However, using a smaller compute budget haphazardly will result in a performance degradation. We hypothesize that certain tokens might not require as much processing as others, and these tokens can be identified through learning. Therefore, if the network learns to choose the right tokens to fill up its capacities, then it may preserve its performance. In the following we describe routing schemes that can be used for this purpose.We consider the setting whereby we route tokens to one of two computational paths: (1) self-attention and MLP blocks, and (2) a residual connection. The latter is computationally cheap, and results in a block output that is entirely determined by the value of its input. The former path is computationally expensive.The total number of FLOPs per forward pass will be fewer than that in a vanilla transformer if we set the capacity for path (1) to be anything less than \ud835\udc47 (the total number of tokens across the sequence and batch). For example, if we were to set a block's capacity to \ud835\udc47 2 (i.e., half the number of tokens as would be the case in a vanilla transformer) then query-times-key matrix multiplication during self-attention becomes 25% as FLOP-intensive as in a vanilla transformer (( \ud835\udc47 2 ) 2 vs. \ud835\udc47 2 ). Similar calculations can determine the FLOP-savings for the MLP.Intuitively, the total FLOPs per forward pass decreases (and the time to complete a forward pass decreases) in proportion to how aggressively we shrink the blocks' capacities. However, downstream performance will also be affected by how aggressively we shrink the blocks capacities, and by the routing algorithm we implement.At one extreme, if we leave each block's capacity at \ud835\udc47 and route every token to (rather than around) each block, then we recover a vanilla transformer. At the other extreme, if we set each block's capacity to 0 and route all tokens around each block, then we're left with a very fast model that doesn't engage with the vast majority of the transformer's parameters, and undoubtedly has poor downstream performance. We hypothesize that somewhere between these two extremes is an optimal model that is faster than a vanilla Transformer and performs as well, if not better, all while being faster to step.Naively, one can leverage stochasticity to route tokens, akin to layer or block \"dropout\". We present this routing scheme as a control, and will show that it significantly under-performs relative to vanilla transformers.We hypothesize that learned routing is preferable. Intuitively, the network should be able to learn which tokens require more or less processing than others. If we are correct that Transformers often expend more compute than they need to make their predictions, then it is an empirical question as to how aggressively we can shrink each block's capacity, and hence, how many tokens we can afford to route around each block.There are two learned routing schemes we consider (see figure We decided to leverage expert-choice routing for a few reasons. First, it obviates the need for an auxiliary balancing loss. Second, since the top-\ud835\udc58 operation depends on the magnitude of the router weights, this routing scheme allows for relative routing weights to help determine which tokens most need the block's computations; routers can try to ensure that the most critical tokens are among the top-\ud835\udc58 by setting their weight appropriately, which is not possible with token-choice routing schemes. For our specific use-case, wherein one computational path is essentially a null operation, it might be critical that important tokens are routed away from the null operation. Third, because we only route through two paths, a single top-\ud835\udc58 operation can efficiently split the tokens into two mutually exclusive sets, one for each computational path, preventing the over-or under-processing problem mentioned above.Figure As a reminder of the high-level intuition, each token is processed by a router to produce a scalar weight, and the top-\ud835\udc58 weights are then used to choose the token identities that will route through a transformer's block, which comprises self-attention and the subsequent MLP. Suppose we have the set of token embeddings in a sequence of length \ud835\udc46 for a given layer \ud835\udc59; that is \ud835\udc4b \ud835\udc59 = {\ud835\udc65 \ud835\udc59 \ud835\udc56 |\ud835\udc56 is an integer, 1 \u2264 \ud835\udc56 \u2264 \ud835\udc46}. The router weight for a given token embedding is a scalar produced as a result of a linear projection, \ud835\udc5f \ud835\udc59 \ud835\udc56 = \ud835\udc64 \ud835\udc47 \ud835\udf03 \ud835\udc65 \ud835\udc59 \ud835\udc56 . Our goal is to use these router weights to determine the output of a block's computation of each token. Suppose \ud835\udc43 \ud835\udefd (\ud835\udc45 \ud835\udc59 ) is the \ud835\udefd-th percentile of the set of router weights \ud835\udc45 \ud835\udc59 , where \ud835\udefd = 1 -\ud835\udc36/\ud835\udc46 and \ud835\udc36 is the user-defined capacity per batch element (an integer < \ud835\udc46 that defines the number of tokens from a sequence that will be processed by a given function). A block's output for a given token is:Here, X \ud835\udc59 is the set of tokens whose router values \ud835\udc5f \ud835\udc59 \ud835\udc56 > \ud835\udc43 \ud835\udefd (\ud835\udc45 \ud835\udc59 ) (that is, the \"top-k\" tokens), and \ud835\udc53 comprises self-attention and the subsequent MLP. Note that the output for a given token \ud835\udc65 \ud835\udc59+1 \ud835\udc56 might depend on other tokens \ud835\udc65 \ud835\udc59 \ud835\udc56\u2260 \ud835\udc57 because of the self-attention operation. The cardinality of X \ud835\udc59 is \ud835\udc36 (or \ud835\udc58): the user-defined capacity. Therefore, the mixture-of-depths transformer accrues compute savings relative to the baseline because the input to the block's computations \ud835\udc53 comprise fewer tokens than usual (\ud835\udc36 < \ud835\udc46), rendering the self-attention and MLP less expensive.Notably, we multiply the output of the function \ud835\udc53 by the router weights. This puts the router weights along the \"gradient path\", thus subjecting them to the forces of gradient descent through the course of the language modeling task (We experimented with versions where the router weights are also included along the computational path for those tokens that bypass the block's computations, but it seems to be sufficient-and implementationally simpler-to only include the router weights along the computational path for those tokens that do not bypass the block's computations).While expert-choice routing has a number of advantages, it has one distinct problem: the top-\ud835\udc58 operation is non-causal. This means that whether a given token's routing weight is among the top-\ud835\udc58 for the sequence depends on the values of the routing weights for tokens that come after it, which we don't have access to when autoregressively sampling.We tested two methods to work around this problem. The first introduces a simple auxiliary loss that empirically affects the primary language modeling objective by approximately 0.2 -0.3%, but allows us to sample from the model autoregressively. We use a binary cross-entropy loss wherein the router's outputs provide the logits, and the top-\ud835\udc58 selections of these logits provide the targets (i.e. 1 if a token was among the top-\ud835\udc58, and 0 if not). Intuitively, this loss centers the sigmoid of the router's outputs around 0.5; those tokens that are selected among the top-k are pressured to produce router outputs above 0.5, and those not among the top-k will be pressured to produce router outputs below 0.5. The second method introduces a small auxiliary MLP predictor (akin to a second router) that receives the same inputs as the router (with a stop gradient), but whose output is a prediction whether that token will be among the top-\ud835\udc58 or not in the sequence. This method does not affect the language modeling objective, and empirically does not significantly impact the step speed.Equipped with these new methods, we can sample autoregressively by choosing to route tokens to or around a block based on the router's output, which does not depend on any information from future tokens. We provide empirical evidence that this is a relatively easy auxiliary task that quickly achieves 99% accuracy.All models use the same basic hyperparameter configurations (e.g. cosine schedules equal to 1\u00d7 the training steps, 128 batch size, 2048 sequence length) except for changes to the number of layers, heads, and embedding size to produce differently sized models during isoFLOP analyses."
    },
    {
      "header": "Results",
      "content": "We first trained models with a relatively small FLOP budget (6e18) to determine optimal hyperparameters (see figure the isoFLOP optimal baseline (also 220M, figure 3 model #1), but is upwards of 60% faster to step during training. Crucially, when run on equivalent hardware these two model variants take take approximately the same amount of wall-clock time to train (figure We tested routing every block or every other block, using capacities from 12.5% to 95% of the total sequence. While routing every other block was crucial for strong performance, we found that aggressive capacity reduction was best (gradual improvements were observed when reducing the capacity down to 12.5% of the total sequence, corresponding to 87.5% of tokens routing around blocks, with performance degrading beyond this point). So, it seems the network is robust to significant capacity reductions as long as there is frequent opportunity for full capacity self-attention and MLP computations.Learned routing is crucial, as MoD transformers that use stochastic routing (implemented using a top-\ud835\udc58 operation on router weights sampled from a Gaussian distribution) perform drastically worse than both the baseline and normal MoD transformer (figure Depicted in figure Step-wise speed gains come from two sources. First, the FLOP-per-parameter ratio in MoD Figure There exist MoD variants that are both faster to step (by virtue of requiring fewer FLOPs per forward pass) and better performing than the isoFLOP optimal baseline. transformers is less than in the baselines because some proportion of tokens are routed around blocks. So, for a given model size, a transformer requires fewer FLOPs per forward pass. Second, since isoFLOP-optimal MoD transformers are both bigger and achieve a lower loss than the isoFLOP-optimal baseline, there exist smaller MoD variants that perform as well or better than the isoFLOP-optimal baseline, and these variants are faster to step because they are smaller. Altogether, then, there exist MoD transformers that perform as well as isoFLOP-optimal baselines and are faster to step, both because they use fewer FLOPs per parameter and because they use fewer parameters.Figure We noticed that MoD transformers had memory savings relative to equivalently sized baseline models at larger sizes, with some variants requiring fewer total devices (i.e., a smaller TPU topology). We did not study this extensively, but we anticipate that as one scales to larger models, these savings could be an important consideration when choosing model variants to train, and could have significant positive effects in regards to the KV cache size during autoregressive sampling.Figure improvements relative to baselines. We observe patterns that might warrant further study; namely, some tokens appear to engage each block along the transformer's depth, while others decide to route around blocks whenever possible. Preliminary analyses suggest that the tokens that engage with blocks more frequently are correlated with output predictions that have higher entropy, which possibly corresponds to predictions that are more difficult to make.We evaluated MoD variants during auto-regressive sampling (see figure The MoD technique can be naturally integrated with MoE models (together comprising MoDE models) in addition to vanilla transformers. In figure "
    },
    {
      "header": "Discussion",
      "content": "Mixture-of-Depths transformers empirically demonstrate that one can improve on isoFLOP-optimal baseline performance with models that use fewer FLOPs per forward pass. This means that-for a given training FLOP budget-we can train models that are both faster and better performing than their baseline counterparts. Previously, to train models that are both faster and as-or better-performing than isoFLOP-optimal models, one would have to use surplus compute to overtrain smaller models (notably, this overtraining technique is still possible with MoD transformers, and speed gains should compound).While MoD transformers require fewer FLOPs per forward pass, one cannot forego FLOPs indiscriminately. Rather, it is crucial to use learned routing decisions-much like in Mixture-of-Experts transformers-to determine whether a token should participate in self-attention and the subsequent MLP (requiring FLOPs), or not (saving FLOPs).We can then use any saved FLOPs by, for example, making the model bigger or training it for longer. Our results show that indeed FLOPs may be inefficiently used in vanilla transformer models, and that there may be more efficient ways for them to be expended.Learned routing mechanisms are sometimes non-causal; that is, information about the future is used to determine a given token's routing decision. This is generally true for top-k routing mechanisms, which are useful because they forego the need for auxiliary balancing losses. However, top-k routing mechanisms present difficulties in post-training autoregressive sampling, where it is impossible to use information about future token identities to determine routing decisions. In this work we show that one can successfully use a top-k routing scheme during training, but not require it during later autoregressive sampling. Eiher a simple auxiliary classifier, or auxiliary loss on the router, is sufficient to learn the top-\ud835\udc58 routing decisions such that it can mimic the top-\ud835\udc58 decisions during autoregressive Intuitively, a token might learn to route around blocks because the prediction being made at that step is easier, and hence, does not require as much compute. However, this strategy is undoubtedly not all that the network learns. If a token does not participate in self-attention at a certain block, then later tokens will also not be able to attend to it. Thus, whether tokens decide to route or not impacts both the current step's prediction and future predictions via causal self-attention, and how the network balances these effects is guided by their influence on the overall language modeling objective.This insight opens the door to MoD variants that decouple the routing for queries, keys and values. For example, perhaps a token would prefer to be among the queries, but not the keys, for a given self-attention computation. One can imagine extending this idea even further into the domain of \"long-term memory\": perhaps there are tokens that would be extremely valuable as keys, regardless of whether it is useful for them to also be among the queries at the step of their occurrence. Learned routing could be a powerful mechanism for deciding which tokens these might be, perhaps funnelling them into a long-term memory buffer that is available during future self-attention. One advantage of such an approach to long-term memory is that tokens decide once, at the moment of \"memory encoding\", whether they should be retrieved in the future. This is more computationally efficient than performing a full content-based lookup across an entire memory buffer for each step in the future, and could be one step towards drastically increasing the context-length available for making a prediction.Unlike MoE transformers that route between effectively the same computation (usually MLPs), MoD transformers demonstrate the value of routing among different types of computations. In this work the types were either the conventional transformer block, or a null computation (functionally equivalent to multiplying by zero). However, one can imagine extending this idea further by routing between even more types of computation. For example, perhaps some tokens are routed to \"memory lookup\" functions, and others are routed to \"tool use\" functions. In general, the routing machinery we deployed provides a knob for adjusting the types of computations available to the network and their relative cost (in total FLOPs); if one wants to introduce an expensive computation, then this can be offset by setting its capacity to some small amount, and hence, by routing only a small number of tokens to it.Altogether, MoD transformers are another tool one can use to tune a model's compute per forward pass (and hence inference time). The machinery used to implement MoD is also generic, and opens the doors to many extensions and integration with other techniques, such as MoE."
    }
  ]
}
