{
  "id": -6139312584262813622,
  "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content": "the multi-armed bandit (mab) problem considers the sequential strategy one must devise when playing a row of slot machines: i.e., which arm to play next to maximize cumulative returns. this analogy extends to a wide range of real-world challenges that require online learning, while simultaneously maximizing some notion of reward.the arm may be a medicine a doctor must prescribe to a patient, the reward being the outcome of such treatment on the patient; or the set of resources a manager needs to allocate for competing projects, with the reward being the revenue attained at the end of the month; or the ad/product/content an online recommendation algorithm must display to maximize click-through rate in e-commerce.the contextual mab, where at each interaction with the world side information (known as 'context') is available, is a natural extension of this abstraction. the 'context' is the physiology of the patient, the type of resources available for each project, or the features of the website user.sequential decision processes have been studied for many decades, and interest has resurged incited by reinforcement learning (rl) advancements developed within the machine learning community. rl] has been successfully applied to a variety of domains, from monte carlo tree searchand hyperparameter tuning for complex optimization in science, engineering and machine learning problems, to revenue maximizationand marketing solutionsin business and operations research. rl is also popular in e-commerce and digital services, improving online advertising at linkedin, engagement with website services at amazon, recommending targeted news at yahoo, and enabling full personalization of content and art at.the techniques used in these success stories are grounded on statistical advances on sequential decision processes and multi-armed bandits. the mab crystallizes the fundamental trade-off between exploration and exploitation in sequential decision making. it has been studied throughout the 20th century, with important contributions byand later. over the years, several algorithms have been proposed -we provide an overview of state-of-the-art solutions in section 2.1. however, applied use cases raise challenges that these mab algorithms often fail to address.for instance, classic mab algorithms do not typically generalize to problems with nonlinear reward dependencies or non-gaussian reward distributions, as exact computation of their statistics of interest is intractable for distributions not in the exponential family. more importantly, these algorithms are commonly designed under the assumption of stationary reward distributions, i.e., the reward function does not change over-time, a premise often violated in practice.we hereby relax these constraints, and consider time-varying models and nonlinear reward functions. we propose to use sequential monte carlo (smc) for non-stationary bandits with nonlinear rewards, where the world -the reward function-is time-varying, and rewards are sequentially observed for the played arms.smc methods] have been widely used to estimate posterior densities and expectations in sequential problems with probabilistic models that are too complex to treat analytically, with many successful applications in science and engineering.in bayesian mab algorithms, the agent must compute sufficient statistics of each arm's rewards over time, for which sequential updates to the posterior of the parameters of interest must be computed. we here show that smc-based, sequentially updated random measures of per-arm parameter posteriors, enable computation of any statistic a bayesian mab policy might require.we generalize existing mab policies beyond their original stationary setting, and accommodate complex reward models: those for which sampling may be performed even if analytic computation of summary statistics is infeasible. we study latent dynamical systems with non-gaussian and nonlinear reward functions, for which smc computes accurate posterior approximations. consequently, we devise a flexible smc-based framework for solving non-stationary and nonlinear mabs.our contribution is a smc-based mab framework that:(i) computes smc-based random measure posterior mab densities utilized by bayesian mab policies;(ii) requires knowledge of the reward function only up to a proportionality constant, i.e., it accommodates nonlinear and non-gaussian bandit rewards; and, (iii) is applicable to time-varying reward models, i.e., to restless or non-stationary multiarmed bandits.the proposed smc-based mab framework (i) leverages smc for both posterior sampling and estimation of sufficient statistics utilized by bayesian mab policies, i.e.,addresses restless bandits via the general linear dynamical system, and accommodates unknown parameters via rao-blackwellization; and (iii) targets nonlinear and non-gaussian reward models, accommodating stateless and context-dependent, discrete and continuous reward distributions.we introduce in section 2 the preliminaries for our work, which combines sequential monte carlo techniques described in section 2.2, with multi-armed bandit algorithms detailed in section 2.1. we present the smc-based mab framework in section 3, and evaluate its performance for thompson sampling and bayes-upper confidence bound policies in section 4. we summarize and conclude with promising research directions in section 5."
    },
    {
      "header": "Background and preliminaries",
      "content": "the mab crystallizes the fundamental trade-off between exploration and exploitation in sequential decision making. it formulates the problem of maximizing rewards observed from sequentially chosen actions a \u2208 a -named arms in the bandit literature-when interacting with an uncertain environment. the reward generating process is stochastic, often parameterized with \u03b8 \u2208 \u03b8 to capture the intrinsic properties of each arm. it can potentially depend on context x \u2208 x ; e.g., a common choice is x = r d x . we use p ato indicate per-arm reward distributions -one for each of the |a| possible arms-where subscript a indicates the conditional reward distribution for each arm a.at each bandit interaction t, reward y t is observed for the played arm a t \u2208 a only, which is independently and identically drawn from its context-conditional distributionparameterized by true \u03b8 * t,a \u2208 \u03b8. we use y t for the stochastic reward variable with density p a (y |x t , \u03b8 * t,a ), and denote with y t its realization at time t. recall that we accommodate time-varying context and parameters via the subscript t in both.we denote with \u03b8 * t the union of all, per-arm, parameters at time t, \u03b8 * t \u2261 \u03b8 * t,0 , \u2022 \u2022 \u2022 , \u03b8 * t,|a|-1 , and with \u03b8 * 1:t \u2261 (\u03b8 * 1 , \u2022 \u2022 \u2022 , \u03b8 * t ), the union of parameters over bandit interactions or time t = 1, \u2022 \u2022 \u2022 , t . the above stochastic mab formulation covers stationary bandits (if parameters are constant over time, i.e., \u03b8 * t,a = \u03b8 * a , \u2200t) and non-contextual bandits, by fixing the context to a constant value x t = x, \u2200t.with knowledge of the true bandit model, i.e., the \u03b8 * t \u2208 \u03b8 that parameterizes the reward distribution of the environment, the optimal action to take iswhere \u00b5 t,a (x t , \u03b8 * t ) = e {y |a, x t , \u03b8 * t } is each arm's conditional reward expectation, given context x t and true parameters \u03b8 * t , at time t. the challenge in mabs is the lack of knowledge about the reward-generating distribution, i.e., uncertainty about \u03b8 * t induces uncertainty about the true optimal action a * t . namely, the agent needs to simultaneously learn properties of the reward distribution, and sequentially decide which action to take next. mab policies choose the next arm to play, with the goal of maximizing attained rewards, based upon the history observed so far.we use \u03c0(a) to denote a multi-armed bandit policy, which is in general stochastic -a is a random variable-on its choices of arms, and is dependent on previous history: \u03c0(a) = p (a = a|h 1:t ) , \u2200a \u2208 a. previous history h 1:t contains the set of contexts, played arms, and observed rewards up to time t, denoted asgiven history h 1:t , a mab policy \u03c0(a|h 1:t ) aims at maximizing its cumulative rewards, or equivalently, minimizing its cumulative regret (the loss incurred due to not knowing the best arm a * t at each time t), i.e., r t = t t=1 y t,a * t -y t,at , where a t denotes the realization of the policy \u03c0(a|h 1:t ) -the arm picked by the policy-at time t. due to the stochastic nature of the problem, we study the expected cumulative regret at time horizon t (not necessarily known a priori)where the expectation is taken over the randomness in the outcomes y , and the arm selection policy a t \u223c \u03c0(a) for the frequentist regret. in the bayesian setting, the uncertainty over the true model parameters \u03b8 * is also marginalized.over the years, many mab policies have been proposed to overcome the explorationexploitation tradeoff. \u03f5-greedy is a popular applied framework due to its simplicity (i.e., to be greedy with probability 1 -\u03f5, and to play the arm with best averaged rewards so far, otherwise to randomly pick any arm), while retaining often good performance. a more formal treatment was provided by, who devised the optimal strategy for certain bandit cases, by considering geometrically discounted future rewards. since the exact computation of the gittins index is complicated, approximations have also been developed.introduced a new class of algorithms, based on the upper confidence bound (ucb) of the expected reward of each arm, for which strong theoretical guarantees have been proven, and many extensions proposed.bayes-ucbis a bayesian approach to ucb algorithms, where quantiles are used as proxies for upper confidence bounds.have proven the asymptotic optimality of bayes-ucb's finite-time regret for the bernoulli case, and argued that it provides an unifying framework for several variants of the ucb algorithm. however, its application is limited to reward models where the quantile functions are analytically tractable.thompson sampling (ts)is an alternative mab policy that has been popularized in practice, and studied theoretically by many. ts is a probability matching algorithm that randomly selects an action to play according to the probability of it being optimal. it has been empirically proven to perform satisfactorily, and to enjoy provable optimality properties, both for problems with and without context.bayes-ucb and ts can be viewed as different approaches to a bayesian formulation of the mab problem. namely, the agent views the unknown parameter of the reward function \u03b8 t as a random variable, and as data from bandit interactions with the environment are collected, a bayesian policy updates its parameter posterior. because a bandit agent must take into account the uncertainty on the unknown parameters, prior knowledge on the reward model and its parameters can be incorporated into bayesian policies, capturing the full state of knowledge via the parameter posteriorwhere p at (y t |x t , \u03b8 t ) is the likelihood of the observed reward y t after playing arm a t at time t.computation of this posterior is critical for bayesian mab algorithms.in thompson sampling, one uses p(\u03b8 t |h 1:t ) to compute the probability of an arm being optimal, i.e., \u03c0(a|x t+1 , h 1:t ) = p a = a * t+1 |x t+1 , \u03b8 t , h 1:t , where the uncertainty over the parameters must be accounted for.namely, one marginalizes the posterior parameter uncertainty after observing history h 1:t up to time instant t, i.e.,(5)in bayes-ucb, p(\u03b8 t |h 1:t ) is critical to determine the distribution of the expected rewards, i.e.,which is required for computation of the expected reward quantile q t+1,a (\u03b1 t ), formally defined aswhere the quantile value \u03b1 t may depend on time, as proposed by. analytical expressions for the parameter posterior of interest p(\u03b8 t |h 1:t ) are available only for few reward functions (e.g., bernoulli and linear contextual gaussian models), but not for many other useful cases, such as logistic or categorical rewards. in addition, computation of equations () and () can be challenging for many distributions outside the exponential family. these issues become even more imperative when dealing with dynamic parameters, i.e., in environments that evolve over time, and with nonlinear reward distributions.to extend mab algorithms to more realistic scenarios, many have considered flexible reward functions and bayesian inference. for example, the use of laplace approximationsor polya-gamma augmentationsfor thompson sampling. these techniques however, are targeted to binary rewards only, modeled via the logistic function.to accommodate complex, continuous reward functions, the combination of bayesian neural networks with approximate inference has also been investigated. variational methods, stochastic mini-batches, and monte carlo techniques have been studied for uncertainty estimation of reward posteriors of these models.benchmarked some of these techniques, and reported that neural networks with approximate inference, even if successful for supervised learning, under-perform in the mab setting. in particular,emphasize the need for adapting the slow convergence uncertainty estimates of neural net based methods for a successful identification of the exploration-exploitation tradeoff.in parallel, others have investigated how to extend bayesian policies, such as thompson sampling, to complex online problemsby leveraging ensemble methods, generalized sampling techniques, or via bootstrapped sampling. solutions that approximate the unknown bandit reward function with finiteor countably infinite gaussian mixture modelshave also been proposed.however, all these algorithms for mabs with complex rewards assume stationary distributions.the study of bandits in a changing world go back to the work by whittle, with subsequent theoretical efforts by many on characterizing restless, or non-stationary, bandits. the special case of piecewisestationary, or abruptly changing environments, has attracted a lot of interest in general, and for ucband thompson samplingalgorithms, in particular. often, these impose a reward 'variation' constraint on the evolution of the arms, or target specific reward functions, such as bernoulli rewards in, where discounting parameters for the prior beta distributions can be incorporated.more flexible restless bandit models, based on the brownian motion or discrete random walks, and simple markov modelshave been proposed, showcasing the trade-off between the time horizon and the rate at which the reward function varies. besides, theoretical performance guarantees have been recently established for thompson sampling in restless environments where the bandit is assumed to evolve via a binary state markov chain, both in the episodicand non-episodicsetting.here, we overcome constraints on both the bandit's assumed reward function and its time-evolving model, by leveraging sequential monte carlo (smc). the use of smc in the context of bandit problems was previously considered for probitand softmaxreward models, and to update latent feature posteriors in a probabilistic matrix factorization model.showed that utilizing smc to compute posterior distributions that lack an explicit closed-form is a theoretically grounded approach for certain online learning problems, such as bandit subset arm selection or job scheduling tasks.these efforts provide evidence that smc can be successfully combined with thompson sampling, yet are different in scope from our work. the smc-based mab framework we present generalizes existing bayesian mab policies beyond their original setting. contrary to existing mab solutions, the smc-based bandit policies we propose (i) are not restricted to specific reward functions, but accommodate nonlinear and non-gaussian rewards, (ii) address non-stationary bandit environments, and (iii) are readily applicable to state-ofthe-art bayesian mab algorithms -thompson sampling and bayes-ucb policies-in a modular fashion.monte carlo (mc) methods are a family of numerical techniques based on repeated random sampling, which have been shown to be flexible enough for both numerical integration and drawing samples from complex probability distributions of interest.with importance sampling (is), one estimates properties of a distribution when obtaining samples from such distribution is difficult. the basic idea of is is to draw, from an alternative distribution, samples that are subsequently weighted to guarantee estimation accuracy (and often reduced variance). these methods are used both to approximate posterior densities, and to compute expectations in probabilistic models, i.e.,when these are too complex to treat analytically. is relies on a proposal distribution q(\u2022), from which one draws m samples \u03c6 (m) \u223c q(\u03c6), m = 1, \u2022 \u2022 \u2022 , m , weighted according to, with w (m) = w (m) m m=1 w (m) .(9)if the support of q(\u2022) includes the support of the distribution of interest p(\u2022), one computes the is estimator of a test function based on the normalized weights w (m) ,with convergence guarantees under weak assumptions. is can also be interpreted as a sampling method where the true posterior distribution is approximated by a random measure, i.e.,leading to estimates that integrate the test function with respect to such measure,the sequential counterpart of is, also known as sequential monte carlo (smc)or particle filtering (pf), provides a convenient solution to computing approximations to posterior distributions with sequential or recursive formulations. in smc, one considers a proposal distribution that factorizes -often, but not necessarily-over time, i.e., q(\u03c6 0:t ) = q(\u03c6 t |\u03c6 1:t-1 )q(\u03c6 1:t-1 ) = t \u03c4 =1 q(\u03c6 \u03c4 |\u03c6 1:\u03c4 -1 )q(\u03c6 0 ) ,which helps in matching the sequential form of the probabilistic model of interest p(\u03c6 t |\u03c6 1:t-1 ), to enable a recursive evaluation of the importance sampling weightsone problem with following the above weight update scheme is that, as time evolves, the distribution of the importance weights becomes more and more skewed, resulting in few (or just one) non-zero weights.to overcome this degeneracy, an additional selection step, known as resampling, is added. in its most basic setting, one replaces the weighted empirical distribution with an equally weighted random measure at every time instant, where the number of offspring for each sample is proportional to its weight. this is known as sequential importance resampling (sir).sir and its many variantshave been shown to be of great flexibility and value in many science and engineering problems, where data are acquired sequentially in time. in these circumstances, one needs to infer all the unknown quantities in an online fashion, where often, the underlying parameters evolve over time.smc provides a flexible and useful framework for these problems with probabilistic models and lax assumptions: i.e., when nonlinear observation functions, non-gaussian noise processes and uncertainty over model parameters must be accommodated. here, we leverage smc for flexible approximations to posterior of interest in non-stationary and nonlinear mab problems."
    },
    {
      "header": "SMC for multi-armed bandits",
      "content": "we use sequential monte carlo to compute posteriors and sufficient statistics of interest for a rich-class of mabs: non-stationary bandits (modeled via linear dynamical systems) with complex (stateless and context-dependent) nonlinear reward functions, subject to non-gaussian stochastic innovations.we model non-stationary, stochastic mabs in a state-space framework, where for a given reward distribution p a (y |x, \u03b8), and parameters that evolve in-time via a transition distribution p(\u03b8 t |\u03b8 t-1 ), we writewhere we explicitly indicate with \u03b8 * t the true yet unknown parameters of the non-stationary multi-armed bandit.within this bandit framework, a bayesian policy must characterize the posterior of the unknown parameters p(\u03b8 t |h 1:t ) as in equation (4), in which the time-varying dynamics of the underlying transition distribution are incorporated.the posterior of interest given observed reward y t can be written aswhererecall that the parameter predictive distribution p(\u03b8 t |h 1:t-1 ) and parameter posterior p(\u03b8 t |h 1:t ) in equation () have analytical, closed-form recursive solutions only for limited cases.we adhere to the standard mab formulation, in that each arm of the bandit is described by its own idiosyncratic parameters (no information is shared across arms)), yet we allow for such parameters to evolve independently per-arm in time:. therefore, the posterior of interest factorizes across armsthis standard mab formulation with independent, per-arm parameter dynamics enables bayesian mab policies to approximate each per-arm parameter posterior separately. given observed reward y t for played mab arm a t at time instant t, only the parameter posterior of the played arm is updated with this new observation, i.e.,while parameter posteriors of the non-played arms are only updated according to the latent parameter dynamics, i.e.,we here combine smc with bayesian bandit policies -thompson sampling and bayes-ucb, specifically-for non-stationary bandits as modeled in equation (). the challenge is on computing the posteriors in equations (), () and () for a variety of mab models, for which smc enables us to accommodate (i) any likelihood function that is computable up to a proportionality constant, and (ii) any time-varying model described by a transition density from which we can draw samples.we use smc to compute per-arm parameter posteriors at each bandit round, i.e., we approximate per-arm filtering densities p(\u03b8 t,a |h 1:t ) with smc-based random measures p m (\u03b8 t,a |h 1:t ), \u2200a, for which there are strong theoretical convergence guarantees.the dimensionality of this estimation problem depends on the size of per-arm parameters, and not on the number of bandit arms |a|. consequently, there will be no particle degeneracy due to increased number of arms.we present in section 3.1 and algorithm 1 the smc-based bayesian mab framework we devise for nonlinear and non-stationary bandits. we describe in section 3.2 how to draw samples from the transition densities, when modeling bandit non-stationarity via the general linear model; and in section 3.3, we present examples of non-gaussian and nonlinear (continuous and discrete) reward functions of interest in practice. throughout, we avoid assumptions on model parameter knowledge and resort to their bayesian marginalization.we combine smc with both thompson sampling and bayes-ucb policies, by sequentially updating, at each bandit interaction t, a smc-based random measure to approximate the time-varying posterior of interest,knowledge of p m (\u03b8 t,a |h 1:t ) enables computation of any per-arm reward statistic bayesian mab policies require.we present algorithm 1 with the sequential importance resampling (sir) methodas introduced by, where:\u2022 the smc proposal distribution q(\u2022) at each bandit interaction t obeys the assumed parameter dynamics: \u03b8\u2022 smc weights are updated based on the likelihood of the observed rewards: wt,a ) -step (9.c) in algorithm 1; and \u2022 the smc random measure is resampled at every time instant -step (9.a).independently of which smc technique is used to compute the posterior random measure p m (\u03b8 t,a |h 1:t ), the fundamental operation in the proposed smc-based mab algorithm 1 is to sequentially update the random measure p m (\u03b8 t,a |h 1:t ) to approximate the true per-arm posterior p(\u03b8 t,a |h 1:t ) over bandit interactions.this smc-based random measure is key, along with transition density p(\u03b8 t,a |\u03b8 t-1,a ), to sequentially propagate parameter posteriors per-arm, and to estimate their sufficient statistics for any bayesian bandit policy. more precisely:\u2022 in step 5 of algorithm 1, we estimate the predictive posterior of per-arm parameters, as a mixture of the transition densities conditioned on previous samples from p m (\u03b8 t,a |h 1:t ),\u2022 in step 9 of algorithm 1, we propagate forward the sequential random measure p m (\u03b8 t,a |h 1:t ) by drawing new samples from the transition density, conditioned on resampled particles, i.e.,in both cases, one draws with replacement according to the importance weights in p m (\u03b8 t,a |h 1:t ),i.e., from a categorical distribution with per-sample probabilities wt,a . we now describe in detail how to use the smc-based posterior random measure p m (\u03b8 t+1,a |h 1:t ) for both thompson sampling and bayes-ucb policies: i.e., which are the specific instructions to execute in steps 5 and 7 of algorithm 1.\u2022 smc-based thompson sampling: ts operates by drawing a sample parameter \u03b8 (s)t+1 from its updated posterior p(\u03b8 t+1 |h 1:t ), and picking the optimal arm for such sample, i.e.,we use the smc random measure p m (\u03b8 t |h 1:t ), and propagate it using the transition density p(\u03b8 t+1,a |\u03b8 t,a ), to draw samples from the parameter posterior predictive distribution: i.e., \u03b8t+1 \u223c p m (\u03b8 t+1 |h 1:t ) in equation (). this smc-based random measure provides an accurate approximation to the true posterior density with high probability.\u2022 smc-based bayes-ucb: we extend bayes-ucb to reward models where the quantile functions are not analytically tractable, by leveraging the smc-based parameter predictive posterior random measure p m (\u03b8 t+1 |h 1:t ).we compute the quantile function of interest by first evaluating the expected reward at each round t based on the available posterior samples, i.e., \u00b5the convergence of quantile estimators generated by smc methods has been explicitly proved in.random measure p m (\u03b8 t+1,a |h 1:t ) in equation () enables computation of the statistics bayesian mab policies require, extending their applicability from stationary to time-evolving bandits. the exposition that follows addresses dynamic bandits, and we illustrate how to process classic, stationary bandits within the proposed framework in appendix a.the dynamic linear model is a flexible and widely used framework to characterize timeevolving systems. here, we model the latent parameters of the bandit \u03b8 \u2208 r d\u03b8 to evolve over time according towith parameters l a \u2208 r d\u03b8 a \u00d7d\u03b8 a and \u03c3 a \u2208 r d\u03b8 a \u00d7d\u03b8 a -recall that we specify distinct transition densities per-arm.with known parameters l a and \u03c3 a , the transition distribution p(\u03b8 t,a |\u03b8 t-1,a ) is gaussian with closed-form updates, i.e., \u03b8 t,a \u223c n (\u03b8 t,a |l a \u03b8 t-1,a , \u03c3 a ).for the more interesting case of unknown parameters, we marginalize parameters l a and \u03c3 a of the transition distributions utilized by the proposed smc-based bayesian policies, i.e., we rao-blackwellizethem.require: a, p(\u03b8 a ), p(\u03b8 t,a |\u03b8 t-1,a ), p a (y |x, \u03b8), \u2200a \u2208 a. require: number of smc samples m (for ucb we also require \u03b1 t )1: draw initial samples from the parameter prior \u03b8 (m0,a) 0,a \u223c p(\u03b8 a ), and wfor a \u2208 a dot+1,a , estimate quantile q t+1,a (\u03b1 t+1 ) as in equation (). decide next action a t+1 to play for thompson sampling:forbayes-ucb:observe reward y t+1 for played arm 9:update the posterior smc random measure p m (\u03b8 t,a |h 1:t ) for all armsper arm a \u2208 a, where m \u2032 t,a is drawn with replacement according to the importance weights w(c) weight samples of the played arm a t+1 based on the likelihood of observed y t+1 w (mt+1,a t+1 )and normalize the weights10: end forthe marginalized transition density is a multivariate t-distribution:whereeach distribution above holds separately for each arm a, and subscript a,0 indicates assumed prior parameters for arm a.these transition distributions are used when propagating per-arm parameter densities in steps 5 and 9 of algorithm 1. they are fundamental for the accuracy of the sequential, random measure-based approximation to the posterior, and the downstream performance of the proposed smc-based mab policies.caution must be exercised when using smc to approximate the dynamic bandit model's posteriors. notably, the impact of non-markovian transition distributions in smc performance must be taken into consideration: the sufficient statistics in equations ()-() depend on the full history of the model dynamics. here, we use general linear models, for which it can be shown that, if stationarity conditions are met, the autocovariance function decays quickly, i.e., the dependence of general linear models on past samples decays exponentially.when exponential forgetting holds in the latent space -i.e., the dependence on past samples decays exponentially, and is negligible after a certain lag-one can establish uniform-in-time convergence of smc methods for functions that depend only on recent states, seeand references therein.more broadly, one can establish uniform-in-time convergence for path functionals that depend only on recent states, as the monte carlo error of p m (\u03b8 t-\u03c4 :t |h 1:t ) with respect to p(\u03b8 t-\u03c4 :t |h 1:t ) is uniformly bounded over time. this quick forgetting property is fundamental for the successful performance of smc methods for inference of linear dynamical states in practice.nevertheless, we acknowledge that any improved smc solution that mitigates pathdegeneracy issues can only be beneficial for the performance of the proposed smc-based policies.algorithm 1 is described in terms of a generic reward likelihood function p a (y |x t , \u03b8 t,a ) that must be computable up to a proportionality constant. we now introduce reward functions that are applicable in many mab use-cases, where the time subscript t has been suppressed for clarity of presentation, and subscript 0 indicates assumed prior parameters.for mab problems where observed rewards are discrete, i.e., y = c for c \u2208 {1, \u2022 \u2022 \u2022 , c}, and contextual information is available, the softmax function is a natural reward density model. in general, categorical variables assign probabilities to an unordered set of outcomes -not necessarily numeric. in this work, we refer to categorical rewards where, for each categorical outcome c \u2208 n, there is a numeric reward y = c associated with it.given a d-dimensional context vector x \u2208 r d , and per-arm parameterswhere we denote with \u03b8 a = {\u03b8 a,1 , \u2022 \u2022 \u2022 , \u03b8 a,c } the set of per-category parameters \u03b8 a,c for arm a. for this reward distribution, the posterior of the parameters can not be computed in closed form, and neither, the quantile function of the expected rewards \u00b5 t,a = y t \u2022 (x \u22a4 t \u03b8 t,a ). when returns are binary, i.e., y = {0, 1} (success or failure of an action), but dependent on a d-dimensional context vector x \u2208 r d , the softmax function reduces to the logistic reward modelwith per-arm parameters \u03b8 a \u2208 r d of same dimensionality d as the context x.the theoretical study of ucb and ts-based algorithms for logistic rewards is an active research area, which we here extend to the discretecategorical setting. we accommodate discrete-categorical mab problems by implementing algorithm 1 with likelihoods as in equations () or (32). namely, we compute p m (\u03b8 t,a |h 1:t ) for both stationary and non-stationary discrete-categorical bandits, by updating the weights of the posterior smc random measure in step 9-c. to the best of our knowledge, no existing work addresses non-stationary, discrete-categorical bandits.for bandits with continuous rewards, gaussian distributions are typically used, where contextual dependencies can easily be included. the contextual linear gaussian reward model is well studied in the bandit literature, where the expected reward of each arm is modeled as a linear combination of a d-dimensional context vector x \u2208 r d , and the idiosyncratic parameters of the arm w a \u2208 r d ; i.e.,we denote with \u03b8 \u2261 {w, \u03c3} the set of all parameters of the reward distribution, and consider the normal inverse-gamma conjugate prior distribution for these,after observing actions a 1:t and rewards y 1:t , the parameter posterior for each arm p(w a , \u03c3 2 a |a 1:t , y 1:t , u 0,a , v 0,a , \u03b1 0,a , \u03b2 0,a ) = p w a , \u03c3 2 a |u t,a , v t,a , \u03b1 t,a , \u03b2 t,afollows an updated normal inverse-gamma distribution with sequentially updated hyperparametersor, alternatively, batch updates of the formwhere t a = {t|a t = a} indicates the set of time instances when arm a is played.with these, we can compute the bayesian expected reward of each arm,and the quantile function for such distributionthe reward variance \u03c3 2 a is unknown in practice, so we marginalize it and obtain p(\u00b5 a |x, u t,a , v t,a ) = t \u00b5 a 2\u03b1 t,a , x \u22a4 u t,a , \u03b2t,a \u03b1t,awhich leads to quantile function computations based on a student's t-distributionequations () and () are needed in step 5 when implementing ts or bayes-ucb policies for the known \u03c3 2 a case; while equations () and () are used for the unknown \u03c3 2 a case. note that one can use the above results for gaussian bandits with no context, by replacing x = i and obtaining \u00b5 a = u t,a .when these equations are combined with the rao-blackwellized transition densities derived for the dynamic model in section 3.2, the proposed smc-based mab policies can be applied to non-stationary, linear gaussian bandit problems with minimal assumptions: i.e., only the functional form of the transition and reward functions is known.there are no competing mab algorithms for non-stationary bandit problems where no parameter knowledge is assumed."
    },
    {
      "header": "Evaluation",
      "content": "we empirically evaluate the proposed smc-based bayesian mab framework in non-stationary bandit scenarios with continuous, binary and discrete-categorical reward distributions.results in appendix a.2 validate the performance of smc-based policies in stationary bandits. we compare their performance to solutions based on analytically attainable posteriors with bernoulli and contextual linear gaussian reward functions, as well as for context-dependent binary rewards modeled with the logistic reward function;-appendix a.3. results showcase satisfactory performance across a wide range of stationary bandit parameterizations and sizes, as smc-based policies achieve the right exploration-exploitation tradeoff.for results we present below, we simulate different parameterizations of dynamic linear models described in section 3.2, and present results for a variety of mab environments with reward functions detailed in sections 4.1, 4.2 and 4.3. section 4.4 illustrates the ability of smc-based bandit policies to capture non-stationary trends in personalized news article recommendations.the main evaluation metric is the cumulative regret of the bandit agent, as defined in equation (), with results averaged over 500 realizations. we present results for smcbased policies with m = 2000 samples, and provide an evaluation of the impact of m in appendix b.we simulate the following two-armed, contextual (x t \u2208 r 2 , \u2200t), linear gaussian bandit:\u03b8 t,a=0,0 \u03b8 t,a=0,1 = 0.9 -0.1 -0.1 0.9where \u03f5 a=0 \u223c n (\u03f5|0, 0.01 \u2022 i) , p(\u03b8 t,a=1 |\u03b8 t-1,a=1 ) :\u03b8 t,a=1,0 \u03b8 t,a=1,1 = 0.9 0.1 0.1 0.9where \u03f5 a=1 \u223c n (\u03f5|0, 0.01\u03b8 t,a=0,0 \u03b8 t,a=0,1 = 0.5 0.0 0.0 0.5where \u03f5 a=0 \u223c n (\u03f5|0, 0.01 \u2022 i) , p(\u03b8 t,a=1 |\u03b8 t-1,a=1 ) :\u03b8 t,a=1,0 \u03b8 t,a=1,1 = 0.9 0.1 0.1 0.9where \u03f5 a=1 \u223c n (\u03f5|0, 0.01(43) the expected rewards driven by the dynamics of equations () and () change over time, inducing switches on the identity of the optimal arm. for instance, for a given realization of scenario a shown in figure, there is an optimal arm swap between time-instants t = (300, 550), with arm 1 becoming the optimal for all t \u2265 600; for a realization of scenario b illustrated in figure, there is an optimal arm change around t = 100, a swap around t = 600, with arm 1 becoming optimal again after t \u2265 1600.empirical results for smc-based bayesian policies in scenarios described by equations () and () are shown in figuresand.we study linear dynamics with gaussian reward distributions with known parameters in figure, of interest as it allows us to validate the smc-based random measure in comparison to the optimal, closed-form posterior -the kalman filter-under the assumption of known dynamic parameters.we observe satisfactory cumulative regret performance in figure: i.e., smc-based bayesian agents' cumulative regret is sublinear. policies that compute and use smc random measure posteriors incur in minimal regret loss in comparison to the optimal kalman filterbased agent. namely, the shape of the regret curves of ts and smc-based ts (bayes-ucb and smc based bayes-ucb, respectively) in figureis equivalent, with minimal differences in average cumulative regret when compared to the volatility across realizations. importantly, all policies are able to adapt to the changes over time of the identify of the optimal arm. we illustrate in figurea more realistic scenario, where the dynamic parameterization is unknown to the bandit agent.we observe in figuresthat, in the case of unknown reward variances (\u03c3 2 a , \u2200a), smc-based policies perform comparably well. in these cases, the agents' reward model is not gaussian, but student-t distributed, as per the marginalized posterior in equation (). the regret loss associated with the uncertainty about \u03c3 2 a is minimal for smc-based bayesian agents, and does not hinder the ability of the proposed smc-based policies to find the right exploration-exploitation balance: i.e., regret is sublinear, and the agents adapt to switches in the identity of the optimal arm.we illustrate in figures 2e-2f the most realistic, yet challenging, non-stationary contextual gaussian bandit case: one where none of the parameters of the model are known. in this case, the agent must sequentially learn both the underlying dynamics (l a , \u03c3 a ; \u2200a) and the conditional reward function's variance (\u03c3 2 a , \u2200a), in order to infer the posterior distribution over the latent, time-varying sufficient statistics of interest, to enable informed sequential decision making.)-(), when the bandit agent knows the latent dynamic parameterization. notice how in figuresregret increases when the optimal arms swap (as shown in figures). smc-based policies successfully find the right exploration-exploitation tradeoff, with minimal additional regret incurred in comparison to their analytical alternatives.cumulative regret results in figuresshowcase a regret performance loss due to the need to learn all these unknown parameters. we observe noticeable (almost linear) regret increases when the dynamics of the parameters swap the identity of the optimal arm. however, smc-based thompson sampling and bayes-ucb agents are able to learn the evolution of the dynamic latent parameters, and the corresponding time-varying expected rewards, with enough accuracy to attain good exploration-exploitation balance: i.e., sublinear regret curves indicate the agent identified and played the optimal arm repeatedly. figureis clear evidence of the smc-based agents' ability to recover from linear to no-regret regimes.we here evaluate non-stationary, contextual, binary reward bandits. we resort to the logistic reward function described in equation (), with time-varying, latent parameter dynamics as described in the following scenarios:\u03b8 t,a=0,0 \u03b8 t,a=0,1 = 0.9 -0.1 -0.1 0.9where \u03f5 a=0 \u223c n (\u03f5|0, 0.01 \u2022 i) , p(\u03b8 t,a=1 |\u03b8 t-1,a=1 ) :\u03b8 t,a=1,0 \u03b8 t,a=1,1 = 0.9 0.1 0.1 0.9where \u03f5 a=1 \u223c n (\u03f5|0, 0.01\u03b8 t,a=0,0 \u03b8 t,a=0,1 = 0.5 0.0 0.0 0.5where \u03f5 a=0 \u223c n (\u03f5|0, 0.01 \u2022 i) , p(\u03b8 t,a=1 |\u03b8 t-1,a=1 ) :\u03b8 t,a=1,0 \u03b8 t,a=1,1 = 0.9 0.1 0.1 0.9where \u03f5 a=1 \u223c n (\u03f5|0, 0.011 + e (x \u22a4 \u03b8 t,a ) . (45) for bandits with logistic rewards, there are no closed form posteriors; hence, one needs to resort to approximations, e.g., a laplace approximation as infor the stationary case. however, there are no bandit algorithms for the non-stationary logistic scenarios described above. on the contrary, smc-based bayesian policies can easily accommodate this setting, by updating posterior random measures p m (\u03b8 t |h 1:t ) as in algorithm 1, for both stationary (evaluated in appendix a.3) and non-stationary bandits we report here.figureillustrates how smc-based bayesian policies adapt to non-stationary optimal arm switches under contextual, binary reward observations, achieving sublinear regret. results in figures 3b-3d also showcase how a bandit agent's regret suffers when learning unknown parameters of the latent dynamics. even though this is a particularly challenging problem, presented evidence suggests that smc-based policies do learn the underlying latent dynamics from contextual binary rewards.notably, proposed policies are able to successfully identify which arm to play: i.e., both smc-based ts and smc-based ucb -with no dynamic parameter knowledge-are able to flatten their regret for t \u2265 650 in figureand)-(). notice the difference in early (t \u2248 600 in scenario c) and late (t \u2248 1650 in scenario d) optimal arm changes, as illustrated in figures, and their impact in regret, as showcased in figures. smc-based bayesian policies adapt and find the right exploration-exploitation tradeoff.we evaluate smc-based bayesian policies in a bandit setting that remains elusive to stateof-the-art bandit algorithms: non-stationary bandits with discrete-categorical, contextual rewards. we simulate the following two-and three-armed categorical bandit scenarios, where numerical rewards y = c, for c \u2208 {0, 1, 2}, depend on a two-dimensional context x t \u2208 r 2 , with time-varying parameters \u03b8 a,c,t obeying the following dynamics:\u03b8 t,a=0,c,0 \u03b8 t,a=0,c,1 = 0.9 -0.1 -0.1 0.9where \u03f5 a=0,c \u223c n (\u03f5|0, 0.01 \u2022 i) , p(\u03b8 t,a=1,c |\u03b8 t-1,a=1,c ) , \u2200c \u2208 {0, 1, 2} :\u03b8 t,a=1,c,0 \u03b8 t,a=1,c,1 = 0.9 0.1 0.1 0.9where \u03f5 a=1,c \u223c n (\u03f5|0, 0.01\u03b8 t,a=0,c,0 \u03b8 t,a=0,c,1 = 0.9 -0.1 -0.1 0.9where \u03f5 a=0,c \u223c n (\u03f5|0, 0.01 \u2022 i) , p(\u03b8 t,a=1,c |\u03b8 t-1,a=1,c ) , \u2200c \u2208 {0, 1, 2} :\u03b8 t,a=1,c,0 \u03b8 t,a=1,c,1 = 0.9 0.1 0.1 0.9where \u03f5 a=1,c \u223c n (\u03f5|0, 0.01 \u2022 i) , p(\u03b8 t,a=2,c |\u03b8 t-1,a=2,c ) , \u2200c \u2208 {0, 1, 2} : \u03b8 t,a=2,c,0 \u03b8 t,a=2,c,1 = 0.9 0.1 0.1 0.9these bandit scenarios accommodate a diverse set of expected reward dynamics, for each realization of the noise processes \u03f5 a,c , \u2200a, c, and depending on the initialization of parameters \u03b8 0,a . we illustrate per-arm, expected reward time-evolution for a realization of the two-armed bandit scenario e in figure, and for the three-armed bandit scenario f in figure.in all cases, expected rewards of each arm vary over time, resulting in transient and recurrent swaps of the optimal arm's identity. we show the corresponding cumulative regret of smc-based bayesian policies in figurefor scenario e, and in figurefor scenario f.)-(). notice how changes in per-arm expected rewards (t \u2248 1750 in scenario e and t > 1250 in scenario f) as illustrated in figuresimpact regret as showcased in figures. smc-based bayesian policies adapt to these changes and balance the exploration-exploitation tradeoff.we observe that smc-based thompson sampling and bayes-ucb are able to reach satisfactory exploitation-exploration balance, i.e., the algorithms dynamically adapt their choice of which arm to play, and attain sublinear cumulative regret.recall the linear increase in cumulative regret (i.e., exploration) when latent parameter dynamics result in changes in the optimal arm's identity: around t \u2208 (800, 1000) in figure, and around t \u2208 (1250, 1500) in figure. after updating the random measure posterior over the unknown latent parameters, and recomputing the expected rewards per-arm, smc-based policies are able to slowly adapt to the optimal arm changes, reaching a new exploitationexploration balance, i.e., flattening the cumulative regret curves.for the most interesting and challenging setting where the dynamic model's parameters are unknown, we observe an increase in cumulative regret for both smc-based policies. this is a direct consequence of the agent sequentially learning all the unknown model parameters, per-arm a, and discrete value c: l a,c , \u03c3 a,c , \u2200a, c. only when posteriors over these -used by the smc-based agents to propagate uncertainty to each bandit arms' expected reward estimates-are improved, can smc-based policies make informed decisions and attain sublinear regret.we observe that the impact of expected reward changes, when occurring later in time (e.g., t \u2248 1250 in figure) is more pronounced for smc-based bayes-ucb policies. namely, the average cumulative regret of smc-based bayes-ucb increases drastically, as well as its volatility, after t = 1250 in figure. we hypothesize that this deterioration over time is due to the shrinking quantile value \u03b1 t \u221d 1/t proposed by, originally designed for stationary bandits. confidence bounds for static reward models tend to shrink proportional to the number of observations per bandit arm. however, in non-stationary regimes, such assumption does not hold: shrinking \u03b1 t over time does not capture the time-evolving parameter posteriors' uncertainty in the long run.more generally, the need to determine appropriate quantile values \u03b1 t for each reward and non-stationary bandit model is a drawback of bayes-ucb, as its optimal value will depend on the specific combination of underlying dynamics and reward function. on the contrary, thompson sampling relies on samples from the posterior, which we here show smc is able to approximate accurately enough for smc-based thompson sampling to operate successfully in all studied cases, without any hyperparameter selection.we evaluate the application of smc-based policies in a real-life application of bandits: the recommendation of personalized news articles, as previously done by.we use a datasetthat contains a fraction of user click logs for news articles displayed in the featured tab of the today module on the yahoo! front page during the first ten days in may 2009. the articles to be displayed were originally chosen uniformly at random from a hand-picked pool of high-quality articles. from these pool of original candidates, we pick a subset of 20 articles shown at different times within may 6th, and collect all user interactions logged with these articles, for a total of 500,354 events. in the dataset, each user is associated with six features: a bias term and 5 features that correspond to the membership features constructed via the conjoint analysis with a bilinear model described in.the goal is to identify the most interesting article for each user, or in bandit terms, to maximize the total number of clicks on the recommended articles over all user interactions, i.e., the average click-through rate (ctr).we treat each article as a bandit arm (|a| = 20), and define whether the article is clicked or not by the user as a binary reward: y t = {1, 0}. hence, we pose the problem as a mab with logistic rewards, where we incorporate the user features as context, x t \u2208 r 6 .we implement smc-based thompson sampling only, due to the flexibility shown in simulated scenarios, and its lack of hyperparameter tuning.we argue that a news recommendation system should evolve over time, as the relevance of news might change during the course of the day. we evaluate both stationary and non-stationary bandits with logistic rewards.as shown in figure, we observe the flexibility of a non-stationary logistic bandit model, where we notice how the smc-based ts agent is able to pick up the dynamic popularity of certain articles over time -averaged ctr results are provided in table.ctr normalized ctr logistic rewards, static arms 0.0670 +/-0.0088 1.6095 +/-0.2115 logistic rewards, time-evolving arms 0.0655 +/-0.0082 1.5745 +/-0.2064 table: ctr results for smc-based policies on the news article recommendation dataset. the normalized ctr is with respect to a random recommendation baseline."
    },
    {
      "header": "Conclusion and discussion",
      "content": "we presented a sequential monte carlo (smc)-based framework for multi-armed bandits (mabs), where we combine smc inference with state-of-the-art bayesian bandit policies. we extend the applicability of bayesian mab policies -thompson sampling and bayes-ucbto previously elusive bandit environments, by accommodating nonlinear and time-varying models of the world, via smc-based inference of the sufficient statistics of interest.the proposed smc-based bayesian mab framework allows for interpretable modeling of nonlinear and time-evolving reward functions, as it sequentially learns the sufficient statistics and dynamics of the bandit from online data, to find the right exploration-exploitation balance. empirical results show good cumulative regret performance of the proposed policies in simulated mab environments that previous algorithms can not address, and in practical scenarios (personalized news article recommendation) where time-varying models of data are required.we show that smc-based posterior random measures are accurate enough for bayesian bandit policies to find satisfactory exploration-exploitation tradeoffs. the proposed smcbased bayesian agents do not only estimate the evolving latent parameters, but also quantify how their uncertainty maps to the uncertainty over the optimality of each arm, adjusting to non-stationary environments. careful computation of smc random measures is fundamental for the accuracy of the sequential approximation to the posteriors of interest, and the downstream performance of the proposed smc-based mab policies. the time-varying uncertainty of the sequentially updated parameter posterior encourages exploration of arms that have not been played recently, but may have reached new exploitable rewards. namely, as the posteriors of unobserved arms result in broader smc posteriors, smc-based bayesian mab policies are more likely to explore such arm, reduce their posterior's uncertainty, and in turn, update the exploration-exploitation balance.important future work remains on the theoretical understanding of thompson sampling and bayes-ucb within the proposed smc-based mab framework. given that smc posteriors converge to the true posterior under suitable conditions, we hypothesize that the proposed smc-based bandit policies can achieve sub-linear regret, under appropriate assumptions on the latent dynamics.on the one hand,have shown that a logarithmic regret bound holds for thompson sampling in complex problems, for bandits with discretely-supported priors over the parameter space without additional structural properties, such as conjugate prior structure or independence across arms. on the other, regret of a non-stationary bandit agent is linear if optimal arm changes occur continuously or adversarially. however, as long as the bandit's latent dynamics incur in a controlled number of optimal arm changes, smc can provide accurate enough posteriors to find the right exploration-exploitation tradeoff, as we show empirically here.a theoretical analysis of the dependency between the dynamic bandit model's characteristics, the resulting rate of optimal arm changes, and smc posterior convergence guarantees, leading to formal regret bounds for the proposed smc-based bayesian policies, is an open research direction.we here apply the proposed smc-based bayesian policies as in algorithm 1 to the original settings where thompson sampling and bayes-ucb were derived, i.e., for stationary bandits with bernoulli and contextual, linear gaussian reward functions;;;.empirical results for these bandits is provided in section a.2, while the stationary logistic bandit case is evaluated in section a.3, where we also evaluate the impact of sample size m in the smc-based bandit algorithms.in stationary bandits, there are no time-varying parameters, i.e., \u03b8 t = \u03b8, \u2200t. for these cases, sir-based parameter propagation becomes troublesome. to mitigate such issues, several alternatives have been proposed in the smc community: e.g., artificial parameter evolution, kernel smoothing, and density assisted techniques.we implement density assisted smc, rather than kernel based particle filters as in, where one approximates the posterior of the unknown parameters with a density of choice. density assisted importance sampling is a well studied smc approach that extends random-walking and kernel-based alternatives;;, with its asymptotic correctness guaranteed for the static parameter case. we acknowledge that any of the smc techniques that further mitigate the challenges of estimating constant parameters (e.g., parameter smoothing;;or nested smc methods;) can only improve the accuracy of the implemented smc-based policies.more precisely, we approximate the posterior of the unknown parameters, given the current state of knowledge, with a gaussian distributionwe present below cumulative regret results for different parameterizations of 5-armed bernoulli bandits."
    }
  ]
}
