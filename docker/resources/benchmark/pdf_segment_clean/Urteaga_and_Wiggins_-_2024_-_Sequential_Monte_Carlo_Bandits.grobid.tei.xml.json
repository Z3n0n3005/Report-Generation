{
  "id": "Urteaga_",
  "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content": "The multi-armed bandit (MAB) problem considers the sequential strategy one must devise when playing a row of slot machines: i.e., which arm to play next to maximize cumulative returns. This analogy extends to a wide range of real-world challenges that require online learning, while simultaneously maximizing some notion of reward.The arm may be a medicine a doctor must prescribe to a patient, the reward being the outcome of such treatment on the patient; or the set of resources a manager needs to allocate for competing projects, with the reward being the revenue attained at the end of the month; or the ad/product/content an online recommendation algorithm must display to maximize click-through rate in e-commerce.The contextual MAB, where at each interaction with the world side information (known as 'context') is available, is a natural extension of this abstraction. The 'context' is the physiology of the patient, the type of resources available for each project, or the features of the website user.Sequential decision processes have been studied for many decades, and interest has resurged incited by reinforcement learning (RL) advancements developed within the machine learning community The techniques used in these success stories are grounded on statistical advances on sequential decision processes and multi-armed bandits. The MAB crystallizes the fundamental trade-off between exploration and exploitation in sequential decision making. It has been studied throughout the 20th century, with important contributions by For instance, classic MAB algorithms do not typically generalize to problems with nonlinear reward dependencies or non-Gaussian reward distributions, as exact computation of their statistics of interest is intractable for distributions not in the exponential family We hereby relax these constraints, and consider time-varying models and nonlinear reward functions. We propose to use sequential Monte Carlo (SMC) for non-stationary bandits with nonlinear rewards, where the world -the reward function-is time-varying, and rewards are sequentially observed for the played arms.SMC methods In Bayesian MAB algorithms, the agent must compute sufficient statistics of each arm's rewards over time, for which sequential updates to the posterior of the parameters of interest must be computed. We here show that SMC-based, sequentially updated random measures of per-arm parameter posteriors, enable computation of any statistic a Bayesian MAB policy might require.We generalize existing MAB policies beyond their original stationary setting, and accommodate complex reward models: those for which sampling may be performed even if analytic computation of summary statistics is infeasible. We study latent dynamical systems with non-Gaussian and nonlinear reward functions, for which SMC computes accurate posterior approximations. Consequently, we devise a flexible SMC-based framework for solving non-stationary and nonlinear MABs.Our contribution is a SMC-based MAB framework that:(i) computes SMC-based random measure posterior MAB densities utilized by Bayesian MAB policies;(ii) requires knowledge of the reward function only up to a proportionality constant, i.e., it accommodates nonlinear and non-Gaussian bandit rewards; and, (iii) is applicable to time-varying reward models, i.e., to restless or non-stationary multiarmed bandits.The proposed SMC-based MAB framework (i) leverages SMC for both posterior sampling and estimation of sufficient statistics utilized by Bayesian MAB policies, i.e., We introduce in Section 2 the preliminaries for our work, which combines sequential Monte Carlo techniques described in Section 2.2, with multi-armed bandit algorithms detailed in Section 2.1. We present the SMC-based MAB framework in Section 3, and evaluate its performance for Thompson sampling and Bayes-Upper Confidence Bound policies in Section 4. We summarize and conclude with promising research directions in Section 5."
    },
    {
      "header": "Background and preliminaries",
      "content": "The MAB crystallizes the fundamental trade-off between exploration and exploitation in sequential decision making. It formulates the problem of maximizing rewards observed from sequentially chosen actions a \u2208 A -named arms in the bandit literature-when interacting with an uncertain environment. The reward generating process is stochastic, often parameterized with \u03b8 \u2208 \u0398 to capture the intrinsic properties of each arm. It can potentially depend on context x \u2208 X ; e.g., a common choice is X = R d X . We use p a At each bandit interaction t, reward y t is observed for the played arm a t \u2208 A only, which is independently and identically drawn from its context-conditional distributionparameterized by true \u03b8 * t,a \u2208 \u0398. We use Y t for the stochastic reward variable with density p a (Y |x t , \u03b8 * t,a ), and denote with y t its realization at time t. Recall that we accommodate time-varying context and parameters via the subscript t in both.We denote with \u03b8 * t the union of all, per-arm, parameters at time t, \u03b8 * t \u2261 \u03b8 * t,0 , \u2022 \u2022 \u2022 , \u03b8 * t,|A|-1 , and with \u03b8 * 1:T \u2261 (\u03b8 * 1 , \u2022 \u2022 \u2022 , \u03b8 * T ), the union of parameters over bandit interactions or time t = 1, \u2022 \u2022 \u2022 , T . The above stochastic MAB formulation covers stationary bandits (if parameters are constant over time, i.e., \u03b8 * t,a = \u03b8 * a , \u2200t) and non-contextual bandits, by fixing the context to a constant value x t = x, \u2200t.With knowledge of the true bandit model, i.e., the \u03b8 * t \u2208 \u0398 that parameterizes the reward distribution of the environment, the optimal action to take iswhere \u00b5 t,a (x t , \u03b8 * t ) = E {Y |a, x t , \u03b8 * t } is each arm's conditional reward expectation, given context x t and true parameters \u03b8 * t , at time t. The challenge in MABs is the lack of knowledge about the reward-generating distribution, i.e., uncertainty about \u03b8 * t induces uncertainty about the true optimal action a * t . Namely, the agent needs to simultaneously learn properties of the reward distribution, and sequentially decide which action to take next. MAB policies choose the next arm to play, with the goal of maximizing attained rewards, based upon the history observed so far.We use \u03c0(A) to denote a multi-armed bandit policy, which is in general stochastic -A is a random variable-on its choices of arms, and is dependent on previous history: \u03c0(A) = P (A = a|H 1:t ) , \u2200a \u2208 A. Previous history H 1:t contains the set of contexts, played arms, and observed rewards up to time t, denoted asGiven history H 1:t , a MAB policy \u03c0(A|H 1:t ) aims at maximizing its cumulative rewards, or equivalently, minimizing its cumulative regret (the loss incurred due to not knowing the best arm a * t at each time t), i.e., R T = T t=1 y t,a * t -y t,at , where a t denotes the realization of the policy \u03c0(A|H 1:t ) -the arm picked by the policy-at time t. Due to the stochastic nature of the problem, we study the expected cumulative regret at time horizon T (not necessarily known a priori)where the expectation is taken over the randomness in the outcomes Y , and the arm selection policy A t \u223c \u03c0(A) for the frequentist regret. In the Bayesian setting, the uncertainty over the true model parameters \u03b8 * is also marginalized.Over the years, many MAB policies have been proposed to overcome the explorationexploitation tradeoff Bayes-UCB Thompson sampling (TS) Bayes-UCB and TS can be viewed as different approaches to a Bayesian formulation of the MAB problem. Namely, the agent views the unknown parameter of the reward function \u03b8 t as a random variable, and as data from bandit interactions with the environment are collected, a Bayesian policy updates its parameter posterior. Because a bandit agent must take into account the uncertainty on the unknown parameters, prior knowledge on the reward model and its parameters can be incorporated into Bayesian policies, capturing the full state of knowledge via the parameter posteriorwhere p at (y t |x t , \u03b8 t ) is the likelihood of the observed reward y t after playing arm a t at time t.Computation of this posterior is critical for Bayesian MAB algorithms.In Thompson sampling, one uses p(\u03b8 t |H 1:t ) to compute the probability of an arm being optimal, i.e., \u03c0(A|x t+1 , H 1:t ) = P A = a * t+1 |x t+1 , \u03b8 t , H 1:t , where the uncertainty over the parameters must be accounted for Namely, one marginalizes the posterior parameter uncertainty after observing history H 1:t up to time instant t, i.e.,(5)In Bayes-UCB, p(\u03b8 t |H 1:t ) is critical to determine the distribution of the expected rewards, i.e.,which is required for computation of the expected reward quantile q t+1,a (\u03b1 t ), formally defined aswhere the quantile value \u03b1 t may depend on time, as proposed by To extend MAB algorithms to more realistic scenarios, many have considered flexible reward functions and Bayesian inference. For example, the use of Laplace approximations To accommodate complex, continuous reward functions, the combination of Bayesian neural networks with approximate inference has also been investigated. Variational methods, stochastic mini-batches, and Monte Carlo techniques have been studied for uncertainty estimation of reward posteriors of these models In parallel, others have investigated how to extend Bayesian policies, such as Thompson sampling, to complex online problems However, all these algorithms for MABs with complex rewards assume stationary distributions.The study of bandits in a changing world go back to the work by Whittle More flexible restless bandit models, based on the Brownian motion or discrete random walks Here, we overcome constraints on both the bandit's assumed reward function and its time-evolving model, by leveraging sequential Monte Carlo (SMC). The use of SMC in the context of bandit problems was previously considered for probit These efforts provide evidence that SMC can be successfully combined with Thompson sampling, yet are different in scope from our work. The SMC-based MAB framework we present generalizes existing Bayesian MAB policies beyond their original setting. Contrary to existing MAB solutions, the SMC-based bandit policies we propose (i) are not restricted to specific reward functions, but accommodate nonlinear and non-Gaussian rewards, (ii) address non-stationary bandit environments, and (iii) are readily applicable to state-ofthe-art Bayesian MAB algorithms -Thompson sampling and Bayes-UCB policies-in a modular fashion.Monte Carlo (MC) methods are a family of numerical techniques based on repeated random sampling, which have been shown to be flexible enough for both numerical integration and drawing samples from complex probability distributions of interest With importance sampling (IS), one estimates properties of a distribution when obtaining samples from such distribution is difficult. The basic idea of IS is to draw, from an alternative distribution, samples that are subsequently weighted to guarantee estimation accuracy (and often reduced variance). These methods are used both to approximate posterior densities, and to compute expectations in probabilistic models, i.e.,when these are too complex to treat analytically. IS relies on a proposal distribution q(\u2022), from which one draws M samples \u03c6 (m) \u223c q(\u03c6), m = 1, \u2022 \u2022 \u2022 , M , weighted according to, with w (m) = w (m) M m=1 w (m) .(9)If the support of q(\u2022) includes the support of the distribution of interest p(\u2022), one computes the IS estimator of a test function based on the normalized weights w (m) ,with convergence guarantees under weak assumptions leading to estimates that integrate the test function with respect to such measure,The sequential counterpart of IS, also known as sequential Monte Carlo (SMC) which helps in matching the sequential form of the probabilistic model of interest p(\u03c6 t |\u03c6 1:t-1 ), to enable a recursive evaluation of the importance sampling weightsOne problem with following the above weight update scheme is that, as time evolves, the distribution of the importance weights becomes more and more skewed, resulting in few (or just one) non-zero weights.To overcome this degeneracy, an additional selection step, known as resampling SIR and its many variants SMC provides a flexible and useful framework for these problems with probabilistic models and lax assumptions: i.e., when nonlinear observation functions, non-Gaussian noise processes and uncertainty over model parameters must be accommodated. Here, we leverage SMC for flexible approximations to posterior of interest in non-stationary and nonlinear MAB problems."
    },
    {
      "header": "SMC for multi-armed bandits",
      "content": "We use sequential Monte Carlo to compute posteriors and sufficient statistics of interest for a rich-class of MABs: non-stationary bandits (modeled via linear dynamical systems) with complex (stateless and context-dependent) nonlinear reward functions, subject to non-Gaussian stochastic innovations.We model non-stationary, stochastic MABs in a state-space framework, where for a given reward distribution p a (Y |x, \u03b8), and parameters that evolve in-time via a transition distribution p(\u03b8 t |\u03b8 t-1 ), we writewhere we explicitly indicate with \u03b8 * t the true yet unknown parameters of the non-stationary multi-armed bandit.Within this bandit framework, a Bayesian policy must characterize the posterior of the unknown parameters p(\u03b8 t |H 1:t ) as in Equation (4), in which the time-varying dynamics of the underlying transition distribution are incorporated.The posterior of interest given observed reward y t can be written aswhereRecall that the parameter predictive distribution p(\u03b8 t |H 1:t-1 ) and parameter posterior p(\u03b8 t |H 1:t ) in Equation ( We adhere to the standard MAB formulation, in that each arm of the bandit is described by its own idiosyncratic parameters (no information is shared across arms)), yet we allow for such parameters to evolve independently per-arm in time:. Therefore, the posterior of interest factorizes across armsThis standard MAB formulation with independent, per-arm parameter dynamics enables Bayesian MAB policies to approximate each per-arm parameter posterior separately. Given observed reward y t for played MAB arm a t at time instant t, only the parameter posterior of the played arm is updated with this new observation, i.e.,while parameter posteriors of the non-played arms are only updated according to the latent parameter dynamics, i.e.,We here combine SMC with Bayesian bandit policies -Thompson sampling and Bayes-UCB, specifically-for non-stationary bandits as modeled in Equation ( We use SMC to compute per-arm parameter posteriors at each bandit round, i.e., we approximate per-arm filtering densities p(\u03b8 t,a |H 1:t ) with SMC-based random measures p M (\u03b8 t,a |H 1:t ), \u2200a, for which there are strong theoretical convergence guarantees The dimensionality of this estimation problem depends on the size of per-arm parameters, and not on the number of bandit arms |A|. Consequently, there will be no particle degeneracy due to increased number of arms.We present in Section 3.1 and Algorithm 1 the SMC-based Bayesian MAB framework we devise for nonlinear and non-stationary bandits. We describe in Section 3.2 how to draw samples from the transition densities, when modeling bandit non-stationarity via the general linear model; and in Section 3.3, we present examples of non-Gaussian and nonlinear (continuous and discrete) reward functions of interest in practice. Throughout, we avoid assumptions on model parameter knowledge and resort to their Bayesian marginalization.We combine SMC with both Thompson sampling and Bayes-UCB policies, by sequentially updating, at each bandit interaction t, a SMC-based random measure to approximate the time-varying posterior of interest,Knowledge of p M (\u03b8 t,a |H 1:t ) enables computation of any per-arm reward statistic Bayesian MAB policies require.We present Algorithm 1 with the sequential Importance Resampling (SIR) method\u2022 The SMC proposal distribution q(\u2022) at each bandit interaction t obeys the assumed parameter dynamics: \u03b8\u2022 SMC weights are updated based on the likelihood of the observed rewards: wt,a ) -Step (9.c) in Algorithm 1; and \u2022 The SMC random measure is resampled at every time instant -Step (9.a).Independently of which SMC technique is used to compute the posterior random measure p M (\u03b8 t,a |H 1:t ), the fundamental operation in the proposed SMC-based MAB Algorithm 1 is to sequentially update the random measure p M (\u03b8 t,a |H 1:t ) to approximate the true per-arm posterior p(\u03b8 t,a |H 1:t ) over bandit interactions.This SMC-based random measure is key, along with transition density p(\u03b8 t,a |\u03b8 t-1,a ), to sequentially propagate parameter posteriors per-arm, and to estimate their sufficient statistics for any Bayesian bandit policy. More precisely:\u2022 In Step 5 of Algorithm 1, we estimate the predictive posterior of per-arm parameters, as a mixture of the transition densities conditioned on previous samples from p M (\u03b8 t,a |H 1:t ),\u2022 In Step 9 of Algorithm 1, we propagate forward the sequential random measure p M (\u03b8 t,a |H 1:t ) by drawing new samples from the transition density, conditioned on resampled particles, i.e.,In both cases, one draws with replacement according to the importance weights in p M (\u03b8 t,a |H 1:t ),i.e., from a categorical distribution with per-sample probabilities wt,a . We now describe in detail how to use the SMC-based posterior random measure p M (\u03b8 t+1,a |H 1:t ) for both Thompson sampling and Bayes-UCB policies: i.e., which are the specific instructions to execute in steps 5 and 7 of Algorithm 1.\u2022 SMC-based Thompson Sampling: TS operates by drawing a sample parameter \u03b8 (s)t+1 from its updated posterior p(\u03b8 t+1 |H 1:t ), and picking the optimal arm for such sample, i.e.,We use the SMC random measure p M (\u03b8 t |H 1:t ), and propagate it using the transition density p(\u03b8 t+1,a |\u03b8 t,a ), to draw samples from the parameter posterior predictive distribution: i.e., \u03b8t+1 \u223c p M (\u03b8 t+1 |H 1:t ) in Equation ( \u2022 SMC-based Bayes-UCB: We extend Bayes-UCB to reward models where the quantile functions are not analytically tractable, by leveraging the SMC-based parameter predictive posterior random measure p M (\u03b8 t+1 |H 1:t ).We compute the quantile function of interest by first evaluating the expected reward at each round t based on the available posterior samples, i.e., \u00b5The convergence of quantile estimators generated by SMC methods has been explicitly proved in Random measure p M (\u03b8 t+1,a |H 1:t ) in Equation ( The dynamic linear model is a flexible and widely used framework to characterize timeevolving systems with parameters L a \u2208 R d\u0398 a \u00d7d\u0398 a and \u03a3 a \u2208 R d\u0398 a \u00d7d\u0398 a -recall that we specify distinct transition densities per-arm.With known parameters L a and \u03a3 a , the transition distribution p(\u03b8 t,a |\u03b8 t-1,a ) is Gaussian with closed-form updates, i.e., \u03b8 t,a \u223c N (\u03b8 t,a |L a \u03b8 t-1,a , \u03a3 a ).For the more interesting case of unknown parameters, we marginalize parameters L a and \u03a3 a of the transition distributions utilized by the proposed SMC-based Bayesian policies, i.e., we Rao-BlackwellizeRequire: A, p(\u03b8 a ), p(\u03b8 t,a |\u03b8 t-1,a ), p a (Y |x, \u03b8), \u2200a \u2208 A. Require: Number of SMC samples M (for UCB we also require \u03b1 t )1: Draw initial samples from the parameter prior \u03b8 (m0,a) 0,a \u223c p(\u03b8 a ), and wfor a \u2208 A do t+1,a , Estimate quantile q t+1,a (\u03b1 t+1 ) as in Equation ( ForBayes-UCB:Observe reward y t+1 for played arm 9:Update the posterior SMC random measure p M (\u03b8 t,a |H 1:t ) for all armsper arm a \u2208 A, where m \u2032 t,a is drawn with replacement according to the importance weights w (c) Weight samples of the played arm a t+1 based on the likelihood of observed y t+1 w (mt+1,a t+1 )and normalize the weights10: end forThe marginalized transition density is a multivariate t-distribution whereEach distribution above holds separately for each arm a, and subscript a,0 indicates assumed prior parameters for arm a.These transition distributions are used when propagating per-arm parameter densities in Steps 5 and 9 of Algorithm 1. They are fundamental for the accuracy of the sequential, random measure-based approximation to the posterior, and the downstream performance of the proposed SMC-based MAB policies.Caution must be exercised when using SMC to approximate the dynamic bandit model's posteriors. Notably, the impact of non-Markovian transition distributions in SMC performance must be taken into consideration: the sufficient statistics in Equations ( When exponential forgetting holds in the latent space -i.e., the dependence on past samples decays exponentially, and is negligible after a certain lag-one can establish uniform-in-time convergence of SMC methods for functions that depend only on recent states, see More broadly, one can establish uniform-in-time convergence for path functionals that depend only on recent states, as the Monte Carlo error of p M (\u03b8 t-\u03c4 :t |H 1:t ) with respect to p(\u03b8 t-\u03c4 :t |H 1:t ) is uniformly bounded over time. This quick forgetting property is fundamental for the successful performance of SMC methods for inference of linear dynamical states in practice Nevertheless, we acknowledge that any improved SMC solution that mitigates pathdegeneracy issues can only be beneficial for the performance of the proposed SMC-based policies.Algorithm 1 is described in terms of a generic reward likelihood function p a (Y |x t , \u03b8 t,a ) that must be computable up to a proportionality constant. We now introduce reward functions that are applicable in many MAB use-cases, where the time subscript t has been suppressed for clarity of presentation, and subscript 0 indicates assumed prior parameters.For MAB problems where observed rewards are discrete, i.e., Y = c for c \u2208 {1, \u2022 \u2022 \u2022 , C}, and contextual information is available, the softmax function is a natural reward density model. In general, categorical variables assign probabilities to an unordered set of outcomes -not necessarily numeric. In this work, we refer to categorical rewards where, for each categorical outcome c \u2208 N, there is a numeric reward y = c associated with it.Given a d-dimensional context vector x \u2208 R d , and per-arm parameterswhere we denote with \u03b8 a = {\u03b8 a,1 , \u2022 \u2022 \u2022 , \u03b8 a,C } the set of per-category parameters \u03b8 a,c for arm a. For this reward distribution, the posterior of the parameters can not be computed in closed form, and neither, the quantile function of the expected rewards \u00b5 t,a = y t \u2022 (x \u22a4 t \u03b8 t,a ). When returns are binary, i.e., Y = {0, 1} (success or failure of an action), but dependent on a d-dimensional context vector x \u2208 R d , the softmax function reduces to the logistic reward modelwith per-arm parameters \u03b8 a \u2208 R d of same dimensionality d as the context x.The theoretical study of UCB and TS-based algorithms for logistic rewards is an active research area For bandits with continuous rewards, Gaussian distributions are typically used, where contextual dependencies can easily be included. The contextual linear Gaussian reward model is well studied in the bandit literature We denote with \u03b8 \u2261 {w, \u03c3} the set of all parameters of the reward distribution, and consider the normal inverse-gamma conjugate prior distribution for these,After observing actions a 1:t and rewards y 1:t , the parameter posterior for each arm p(w a , \u03c3 2 a |a 1:t , y 1:t , u 0,a , V 0,a , \u03b1 0,a , \u03b2 0,a ) = p w a , \u03c3 2 a |u t,a , V t,a , \u03b1 t,a , \u03b2 t,afollows an updated normal inverse-gamma distribution with sequentially updated hyperparametersor, alternatively, batch updates of the formwhere t a = {t|a t = a} indicates the set of time instances when arm a is played.With these, we can compute the Bayesian expected reward of each arm,and the quantile function for such distributionThe reward variance \u03c3 2 a is unknown in practice, so we marginalize it and obtain p(\u00b5 a |x, u t,a , V t,a ) = t \u00b5 a 2\u03b1 t,a , x \u22a4 u t,a , \u03b2t,a \u03b1t,awhich leads to quantile function computations based on a Student's t-distributionEquations ( When these equations are combined with the Rao-Blackwellized transition densities derived for the dynamic model in Section 3.2, the proposed SMC-based MAB policies can be applied to non-stationary, linear Gaussian bandit problems with minimal assumptions: i.e., only the functional form of the transition and reward functions is known.There are no competing MAB algorithms for non-stationary bandit problems where no parameter knowledge is assumed."
    },
    {
      "header": "Evaluation",
      "content": "We empirically evaluate the proposed SMC-based Bayesian MAB framework in non-stationary bandit scenarios with continuous, binary and discrete-categorical reward distributions.Results in Appendix A.2 validate the performance of SMC-based policies in stationary bandits. We compare their performance to solutions based on analytically attainable posteriors with Bernoulli and contextual linear Gaussian reward functions For results we present below, we simulate different parameterizations of dynamic linear models described in Section 3.2, and present results for a variety of MAB environments with reward functions detailed in Sections 4.1, 4.2 and 4.3. Section 4.4 illustrates the ability of SMC-based bandit policies to capture non-stationary trends in personalized news article recommendations.The main evaluation metric is the cumulative regret of the bandit agent, as defined in Equation ( We simulate the following two-armed, contextual (x t \u2208 R 2 , \u2200t), linear Gaussian bandit:\u03b8 t,a=0,0 \u03b8 t,a=0,1 = 0.9 -0.1 -0.1 0.9where \u03f5 a=0 \u223c N (\u03f5|0, 0.01 \u2022 I) , p(\u03b8 t,a=1 |\u03b8 t-1,a=1 ) :\u03b8 t,a=1,0 \u03b8 t,a=1,1 = 0.9 0.1 0.1 0.9where \u03f5 a=1 \u223c N (\u03f5|0, 0.01\u03b8 t,a=0,0 \u03b8 t,a=0,1 = 0.5 0.0 0.0 0.5where \u03f5 a=0 \u223c N (\u03f5|0, 0.01 \u2022 I) , p(\u03b8 t,a=1 |\u03b8 t-1,a=1 ) :\u03b8 t,a=1,0 \u03b8 t,a=1,1 = 0.9 0.1 0.1 0.9where \u03f5 a=1 \u223c N (\u03f5|0, 0.01(43) The expected rewards driven by the dynamics of Equations ( Empirical results for SMC-based Bayesian policies in scenarios described by Equations ( We study linear dynamics with Gaussian reward distributions with known parameters in Figure We observe satisfactory cumulative regret performance in Figure We observe in Figures We illustrate in Figures 2e-2f the most realistic, yet challenging, non-stationary contextual Gaussian bandit case: one where none of the parameters of the model are known. In this case, the agent must sequentially learn both the underlying dynamics (L a , \u03a3 a ; \u2200a) and the conditional reward function's variance (\u03c3 2 a , \u2200a), in order to infer the posterior distribution over the latent, time-varying sufficient statistics of interest, to enable informed sequential decision making.   Cumulative regret results in Figures We here evaluate non-stationary, contextual, binary reward bandits. We resort to the logistic reward function described in Equation ( \u03b8 t,a=0,0 \u03b8 t,a=0,1 = 0.9 -0.1 -0.1 0.9where \u03f5 a=0 \u223c N (\u03f5|0, 0.01 \u2022 I) , p(\u03b8 t,a=1 |\u03b8 t-1,a=1 ) :\u03b8 t,a=1,0 \u03b8 t,a=1,1 = 0.9 0.1 0.1 0.9where \u03f5 a=1 \u223c N (\u03f5|0, 0.01\u03b8 t,a=0,0 \u03b8 t,a=0,1 = 0.5 0.0 0.0 0.5where \u03f5 a=0 \u223c N (\u03f5|0, 0.01 \u2022 I) , p(\u03b8 t,a=1 |\u03b8 t-1,a=1 ) :\u03b8 t,a=1,0 \u03b8 t,a=1,1 = 0.9 0.1 0.1 0.9where \u03f5 a=1 \u223c N (\u03f5|0, 0.011 + e (x \u22a4 \u03b8 t,a ) . (45) For bandits with logistic rewards, there are no closed form posteriors; hence, one needs to resort to approximations, e.g., a Laplace approximation as in Figure Notably, proposed policies are able to successfully identify which arm to play: i.e., both SMC-based TS and SMC-based UCB -with no dynamic parameter knowledge-are able to flatten their regret for t \u2265 650 in Figure We evaluate SMC-based Bayesian policies in a bandit setting that remains elusive to stateof-the-art bandit algorithms: non-stationary bandits with discrete-categorical, contextual rewards. We simulate the following two-and three-armed categorical bandit scenarios, where numerical rewards Y = c, for c \u2208 {0, 1, 2}, depend on a two-dimensional context x t \u2208 R 2 , with time-varying parameters \u03b8 a,c,t obeying the following dynamics:\u03b8 t,a=0,c,0 \u03b8 t,a=0,c,1 = 0.9 -0.1 -0.1 0.9where \u03f5 a=0,c \u223c N (\u03f5|0, 0.01 \u2022 I) , p(\u03b8 t,a=1,c |\u03b8 t-1,a=1,c ) , \u2200c \u2208 {0, 1, 2} :\u03b8 t,a=1,c,0 \u03b8 t,a=1,c,1 = 0.9 0.1 0.1 0.9where \u03f5 a=1,c \u223c N (\u03f5|0, 0.01\u03b8 t,a=0,c,0 \u03b8 t,a=0,c,1 = 0.9 -0.1 -0.1 0.9where \u03f5 a=0,c \u223c N (\u03f5|0, 0.01 \u2022 I) , p(\u03b8 t,a=1,c |\u03b8 t-1,a=1,c ) , \u2200c \u2208 {0, 1, 2} :\u03b8 t,a=1,c,0 \u03b8 t,a=1,c,1 = 0.9 0.1 0.1 0.9where \u03f5 a=1,c \u223c N (\u03f5|0, 0.01 \u2022 I) , p(\u03b8 t,a=2,c |\u03b8 t-1,a=2,c ) , \u2200c \u2208 {0, 1, 2} : \u03b8 t,a=2,c,0 \u03b8 t,a=2,c,1 = 0.9 0.1 0.1 0.9These bandit scenarios accommodate a diverse set of expected reward dynamics, for each realization of the noise processes \u03f5 a,c , \u2200a, c, and depending on the initialization of parameters \u03b8 0,a . We illustrate per-arm, expected reward time-evolution for a realization of the two-armed bandit Scenario E in Figure In all cases, expected rewards of each arm vary over time, resulting in transient and recurrent swaps of the optimal arm's identity. We show the corresponding cumulative regret of SMC-based Bayesian policies in Figure We observe that SMC-based Thompson sampling and Bayes-UCB are able to reach satisfactory exploitation-exploration balance, i.e., the algorithms dynamically adapt their choice of which arm to play, and attain sublinear cumulative regret.Recall the linear increase in cumulative regret (i.e., exploration) when latent parameter dynamics result in changes in the optimal arm's identity: around t \u2208 (800, 1000) in Figure For the most interesting and challenging setting where the dynamic model's parameters are unknown, we observe an increase in cumulative regret for both SMC-based policies. This is a direct consequence of the agent sequentially learning all the unknown model parameters, per-arm a, and discrete value c: L a,c , \u03a3 a,c , \u2200a, c. Only when posteriors over these -used by the SMC-based agents to propagate uncertainty to each bandit arms' expected reward estimates-are improved, can SMC-based policies make informed decisions and attain sublinear regret.We observe that the impact of expected reward changes, when occurring later in time (e.g., t \u2248 1250 in Figure More generally, the need to determine appropriate quantile values \u03b1 t for each reward and non-stationary bandit model is a drawback of Bayes-UCB, as its optimal value will depend on the specific combination of underlying dynamics and reward function. On the contrary, Thompson sampling relies on samples from the posterior, which we here show SMC is able to approximate accurately enough for SMC-based Thompson sampling to operate successfully in all studied cases, without any hyperparameter selection.We evaluate the application of SMC-based policies in a real-life application of bandits: the recommendation of personalized news articles, as previously done by We use a datasetThe goal is to identify the most interesting article for each user, or in bandit terms, to maximize the total number of clicks on the recommended articles over all user interactions, i.e., the average click-through rate (CTR).We treat each article as a bandit arm (|A| = 20), and define whether the article is clicked or not by the user as a binary reward: y t = {1, 0}. Hence, we pose the problem as a MAB with logistic rewards, where we incorporate the user features as context, x t \u2208 R 6 .We implement SMC-based Thompson sampling only, due to the flexibility shown in simulated scenarios, and its lack of hyperparameter tuning.We argue that a news recommendation system should evolve over time, as the relevance of news might change during the course of the day. We evaluate both stationary and non-stationary bandits with logistic rewards.As shown in Figure CTR Normalized CTR Logistic rewards, static arms 0.0670 +/-0.0088 1.6095 +/-0.2115 Logistic rewards, time-evolving arms 0.0655 +/-0.0082 1.5745 +/-0.2064 Table "
    },
    {
      "header": "Conclusion and discussion",
      "content": "We presented a sequential Monte Carlo (SMC)-based framework for multi-armed bandits (MABs), where we combine SMC inference with state-of-the-art Bayesian bandit policies. We extend the applicability of Bayesian MAB policies -Thompson sampling and Bayes-UCBto previously elusive bandit environments, by accommodating nonlinear and time-varying models of the world, via SMC-based inference of the sufficient statistics of interest.The proposed SMC-based Bayesian MAB framework allows for interpretable modeling of nonlinear and time-evolving reward functions, as it sequentially learns the sufficient statistics and dynamics of the bandit from online data, to find the right exploration-exploitation balance. Empirical results show good cumulative regret performance of the proposed policies in simulated MAB environments that previous algorithms can not address, and in practical scenarios (personalized news article recommendation) where time-varying models of data are required.We show that SMC-based posterior random measures are accurate enough for Bayesian bandit policies to find satisfactory exploration-exploitation tradeoffs. The proposed SMCbased Bayesian agents do not only estimate the evolving latent parameters, but also quantify how their uncertainty maps to the uncertainty over the optimality of each arm, adjusting to non-stationary environments. Careful computation of SMC random measures is fundamental for the accuracy of the sequential approximation to the posteriors of interest, and the downstream performance of the proposed SMC-based MAB policies. The time-varying uncertainty of the sequentially updated parameter posterior encourages exploration of arms that have not been played recently, but may have reached new exploitable rewards. Namely, as the posteriors of unobserved arms result in broader SMC posteriors, SMC-based Bayesian MAB policies are more likely to explore such arm, reduce their posterior's uncertainty, and in turn, update the exploration-exploitation balance.Important future work remains on the theoretical understanding of Thompson sampling and Bayes-UCB within the proposed SMC-based MAB framework. Given that SMC posteriors converge to the true posterior under suitable conditions On the one hand, A theoretical analysis of the dependency between the dynamic bandit model's characteristics, the resulting rate of optimal arm changes, and SMC posterior convergence guarantees, leading to formal regret bounds for the proposed SMC-based Bayesian policies, is an open research direction.We here apply the proposed SMC-based Bayesian policies as in Algorithm 1 to the original settings where Thompson sampling and Bayes-UCB were derived, i.e., for stationary bandits with Bernoulli and contextual, linear Gaussian reward functions Empirical results for these bandits is provided in Section A.2, while the stationary logistic bandit case is evaluated in Section A.3, where we also evaluate the impact of sample size M in the SMC-based bandit algorithms.In stationary bandits, there are no time-varying parameters, i.e., \u03b8 t = \u03b8, \u2200t. For these cases, SIR-based parameter propagation becomes troublesome We implement density assisted SMC, rather than kernel based particle filters as in More precisely, we approximate the posterior of the unknown parameters, given the current state of knowledge, with a Gaussian distribution We present below cumulative regret results for different parameterizations of 5-armed Bernoulli bandits.    "
    }
  ]
}
