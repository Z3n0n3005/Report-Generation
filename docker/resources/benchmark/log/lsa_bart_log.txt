[ 20240518-14:09:38 ] [model] Model loading time: 4.475229978561401 
[ 20240518-14:09:38 ] [summary] True 5 False 
[ 20240518-14:09:38 ] [lsa] result:  320 
 
[ 20240518-14:09:38 ] [lsa preprocess]  The main asteroid belt (MAB), situated between the orbits of Mars and Jupiter, contains over 1 million objects with diameter D larger than ≈1 km The MAB can be characterized using two broad taxonomic groupings of asteroids, S-and C-complex There are currently two different views for explaining the MAB's low mass.On one side, it was proposed that solids in the protoplanetary disk followed a smooth radial distribution of mass Early depletion of the MAB (while gas in the protoplanetary disk still existed) could happen in the following ways.After gas disk dispersal, early MAB depletion (i.e., during terrestrial planet formation; The story is much simpler if the primordial MAB was partially or fully devoid of material initially (e.g.It is clear from the discussion presented in the previous paragraphs that the primordial amount of mass that initially existed in the MAB is a key ingredient to dictate the formation and early evolution of the solar system.Ideally, we would like to account for both collisional and dynamical evolution (and depletion) in a self-consistent way using modern ideas about how the MAB was affected by early solar system processes.The majority of the work discussed so far assumed that (i) mass depletion was driven by dynamical effects, (ii) asteroids were assumed to be massless test particles, and (iii) collisional evolution was not included (i.e., growth and fragmentation).The few works that accounted for collisional evolution of the MAB (e.g.Our primordial MAB SFD is based on the findings of We present our work in the following structure: Section 2 describes our modeling.In Section 3 we describe how we have chosen the data set for comparison between model results and the currently observed MAB.Our results are presented in Section 4, and conclusion in Section 5. 
[ 20240518-14:09:42 ] [model] Done loading inference: 4.075504541397095 
[ 20240518-14:09:42 ] [model] summary result: The main asteroid belt (MAB) contains over 1 million objects with diameter D larger than 1 km. There are currently two different views for explaining the MAB's low mass. 
[ 20240518-14:09:42 ] [summary] True 5 False 
[ 20240518-14:09:42 ] [lsa] result:  205 
 
[ 20240518-14:09:42 ] [lsa preprocess]  To model the evolution of the MAB SFD and the accretion of planetesimals with D > 500 km in the MAB region we first assume that planetesimals were formed within the first 0.5 Myr after Calcium-Aluminum-Inclusions (CAIs) based on the methods by For the total primordial mass of the MAB we assumed that the initial distribution of planetesimals, uniformly distributed between 1.8 and 3.6 au The exact time and radial distance of Jupiter's formation is unknown (e.g.; Chambers 2021; We followed the accretion of objects within our MAB region for 5 Myr.This interval is presumably the time the gas in the solar nebula dispersed in the outer solar system based on the relative ages of the youngest CB-chondrites to CAIs To follow the dynamics and accretion of the MAB under the considerations above, we used the code known as LIPAD (Lagrangian Integrator for Planetary Accretion and Dynamics; All planetesimals within all tracer particles are allowed to have self-gravitational interactions and to collide with one-another throughout the simulation.We discarded objects that collisionally evolved to sizes below 1 mm, assuming those would rapidly grind down to dust and no longer contribute to the accretion process 
[ 20240518-14:09:48 ] [model] Done loading inference: 5.55826210975647 
[ 20240518-14:09:48 ] [model] summary result: The exact time and radial distance of Jupiter's formation is unknown. We followed the accretion of objects within our MAB region for 5 Myr. All planetesimals within all tracer particles are allowed to have self-gravitational interactions and to collide with one-another. 
[ 20240518-14:09:48 ] [summary] True 5 False 
[ 20240518-14:09:48 ] [lsa] result:  312 
 
[ 20240518-14:09:48 ] [lsa preprocess]  After we follow accretion in the MAB during the first 5 Myr of the solar system's history, it is necessary to account for the various processes that happened during subsequent 4.5 Gyr evolution after gas disk dispersal.Specifically, the giant planet instability has been found to heavily deplete the MAB In this work, we consider the MAB region to be delimited as follows (e.g., We then use this MAB definition to select objects from the Minor Planet Center (MPC) databaseFigure For our depletion analysis, we further divide the MAB definition above into 5 sub-regions: Extended inner Main Belt (EiMB; a < 2.1 au), Inner Main Belt (IMB; 2.1 au < a < 2.5 au), Center Main Belt (CMB; 2.5 au < a < 2.82 au), Outer Main Belt (OMB; 2.82 au < a < 3.25 au), and Extended outer Main Belt (EoMB; a > 3.25 au).The row labeled D18 D f ac in Table .D f ac effectively reports the survival fraction of objects from the simulation.Yet, we prefer to refer to them as a depletion factor as those are the numbers we directly multiply the evolved population in order to account for their depletion.Dper is for the percentage of depletion Dper ≈ 100 × (1 -D f ac ).Labels all and AM D JS P S /P J are for different cuts in the data by • C19 all refers to results taken from • C19 AM DJS P S /P J is also for results from The work by Our analyses clearly demonstrate that MAB depletion during the giant planet instability is not uniform.The fact that different MAB sub-regions have different depletion factors was first pointed out by 
[ 20240518-14:09:53 ] [model] Done loading inference: 4.916448593139648 
[ 20240518-14:09:53 ] [model] summary result: The giant planet instability has been found to heavily deplete the MAB. Different MAB sub-regions have different depletion factors. The work by Our analyses clearly demonstrate that MAB depletion during the giantPlanet instability is not uniform. 
[ 20240518-14:09:53 ] [summary] True 5 False 
[ 20240518-14:09:53 ] [lsa] result:  667 
 
[ 20240518-14:09:53 ] [lsa preprocess]  We are interested in evaluating both the evolution and depletion of the MAB SFD as well as the number of objects with D > 500 km (4-Vesta-like) that would grow over time within the core of the MAB (see Section 3).Specifically, given the importance of knowing where an object of a certain size is in order to estimate depletion effects (Table The current cumulative MAB SFD presents a very characteristic power slope in the 100 km < D < 500 km range.As first pointed by In Figure The fact that our evolved SFDs preserve the primordial power law slope of the cumulative distribution for objects with 100 km < D < 500 km indicates our simulations are compatible with the finds by As observed in Figure It is important to also say that, in this work, we assume that our SFDs from Figure It is crucial to account for the semi-major axis dependency of depletion when attempting to glean insights into the nature of the primordial MAB from the current MAB SFD.Figure With the post-depletion SFDs evaluated as in Figure 12 Following The following rows in Table Our results, then indicate that, regardless of Jupiter's inclusion and the exact timing of the nebula gas dispersal, while following our fiducial case, the current overall S-complex SFD (dashed in Figure EiMB IMB CMB OMB EoMB MAB Current/Post-Depletion 0.00 7.37×10 -6 1.85×10 -5 5.50×10 -6 9.40×10 -8 ≈ 3.15×10 -5Pre-Depletion by D18 -5.85×10 -4 2.03×10 -4 5.94×10 -5 9.62×10 -6 ≳ 8.57×10 -4Pre-Depletion by C19 all 0.00 7.97×10 -4 1.34×10 -3 1.61×10 -4 1.35×10 -3 ≈ 3.65×10 -3Pre-Depletion by C19 AM D JS P S /P J -3.08×10 -3 9.95×10 -4 5.15×10 -5 3.52×10 -4 ≳ 4.48×10 -3  0.0016 MMSN following our methods in Section 2, which can still be approximated by 0.002 MMSN (yellow in Figure We should once again stress that our estimate for the total MAB primordial mass (formed at 0.5 Myr after CAIs) above is an upper limit.The reason is mostly because, in addition to all that was discussed in the previous paragraph, we cannot evaluate how much extra mass would be added to the S-complex MAB population via implantation during terrestrial planet growth (e.g., Primordial MAB masses larger than about ≈ 2.14×10 -3 M ⊕ formed at 0.5 Myr after CAIs would generate SFDs incompatible with our predicted S-complex SFD, unless depletion occurred much earlier than accretion could have taken place.However, although not shown, an analysis of the temporal evolution of our runs show that the shape of the SFD as presented in Figure Our low estimate for the primordial MAB mass points to the conclusion that there is no longer the need for an early giant planet instability to stunt Mars' growth The number of objects with D > 500 km in the MAB, and the particular sub-region of the MAB where they reside, are also important constrains.Knowing how many D > 500 km formed, and more importantly, where did they form and survived depletion can be used as further diagnostic for the maximum amount of mass that could have existed in the primordial MAB.The only known S-complex asteroid with D > 500 km is (4) Vesta, (a V-type; Figure The yellow shaded region in Figure asteroid (4) Vesta should be considered as an object of possible formation and survival within the IMB in our study, i.e., N = 1.N = 0 is a valid result, to account for the possibility where either no objects with D > 500 km formed, or formed but did not survive depletion.In this case, (4) Vesta would not be an object originated in the MAB (or IMB to be more specific; see Figure An analysis of Figure Comparing Appendix Tables 
[ 20240518-14:10:01 ] [model] Done loading inference: 8.51655912399292 
[ 20240518-14:10:01 ] [model] summary result: The only known S-complex asteroid with D > 500 km is (4) Vesta. Vesta should be considered as an object of possible formation and survival within the IMB. Our low estimate for the primordial MAB mass points to the conclusion there is no longer the need for an early giant planet instability to stunt Mars' growth. 
[ 20240518-14:10:01 ] [summary] True 5 False 
[ 20240518-14:10:01 ] [lsa] result:  329 
 
[ 20240518-14:10:01 ] [lsa preprocess]  The primary goal of our work was to determine the maximum possible total mass that could have existed in the primordial MAB region.The current MAB total mass is tiny (≈5×10 -4 M ⊕ ; DeMeo & Carry 2014).There is a debate in the literature on whether such a small MAB mass was primordial (e.g.; In this work we followed accretion (growth and fragmentation) of planetesimals in the MAB during the period when gas still existed in the solar nebula.We assumed different values of total mass for our primordial MAB (see Section 2 for a detailed description of model parameters), and that all of our primordial MAB asteroids would be of inner solar system S-complex taxonomic type (DeMeo & Carry 2014, see also discussion in Section 1).We also attributed a SFD for the MAB primordial population as proposed by previous works that succeeded in reproducing the shape of the current MAB SFD After following how the MAB SFD evolves under the above circumstances, we considered how the MAB population would be depleted during subsequent solar system evolution.This is important as it is well known that the giant planet instability The first main result of the present work is that we found the MAB depletion is uneven, i.e., different radial subregions of the MAB are depleted at different rates (Table The second main result of the present work is that we found the maximum total mass that could have formed in the primordial MAB at around 0.5 Myr after CAIs is likely to be smaller than ≈ 2.14×10 -3 M ⊕ .Primordial MAB masses larger than this limit would unavoidably lead to the development of SFDs inconsistent with the current inferred SFD for S-complex asteroids Finally, we conclude by noting that our results are independent of terrestrial planet formation models and/or early evolution of the giant planets (e.g.; 
[ 20240518-14:10:07 ] [model] Done loading inference: 5.260831594467163 
[ 20240518-14:10:07 ] [model] summary result: The current MAB total mass is tiny. There is a debate in the literature on whether such a small MAB mass was primordial. In this work we followed accretion (growth and fragmentation) of planetesimals in the MAB. 
[ 20240518-14:10:07 ] [summary] True 5 False 
[ 20240518-14:10:07 ] [lsa] result:  309 
 
[ 20240518-14:10:07 ] [lsa preprocess]  Efficiency and fairness in determining who gets what and when are the two major objectives of economists thinking about scarcity, and many interesting matchmaking and market design solutions have been proposed Such mechanisms have been proposed previously in non-human contexts to manage scarcity in, for example, traffic Karma and the aforementioned choice system are different from other token-based mechanisms that have been implemented and proposed in the past, including those that use tradable credits Figure The example illustrates that karma is similar in spirit to the 'universal currency' of reputation Further, the example also illustrates another feature of karma, as was mentioned above, that distinguishes karma from fixed-value tokens and from other forms of rules ensuring turn-taking: it allows individuals to dynamically express how intense their private preference for the resource is.Clearly, such a mechanism may be advantageous when preference intensities/urgencies in the population are heterogeneous and not perfectly correlated over time.Indeed, To date, the literature on karma focused on the theoretical properties of the mechanism compared with random allocation and other domain-specific schemes such as dynamic pricing or max-min allocation Our main findings summarize as follows.Overall, compared with random allocation, karma leads to substantial efficiency gains, and these gains benefit almost all participants.In fact, only non-adopters, that is, individuals who do not participate in karma bidding actively themselves, do not benefit and may be worse off.The treatment variations suggest that benefits are particularly pronounced in situations when preference intensities are dynamically more intense and less frequent, and the bidding scheme is designed to be minimal (i.e., binary).These findings provide a first benchmark that karma may be used beneficially in human interactions.Our study also points in several directions for further theoretical and experimental investigation. 
[ 20240518-14:10:11 ] [model] Done loading inference: 4.135615825653076 
[ 20240518-14:10:11 ] [model] summary result: Karma allows individuals to dynamically express how intense their private preference for the resource is. Compared with random allocation, karma leads to substantial efficiency gains, and these gains benefit almost all participants. 
[ 20240518-14:10:11 ] [summary] True 5 False 
[ 20240518-14:10:11 ] [lsa] result:  348 
 
[ 20240518-14:10:11 ] [lsa preprocess]  Moreover, each participant is allowed a maximum karma level 𝑘 max that the redistribution respects.After the redistribution the game proceeds to the next round 𝑡 + 1.Table We follow a 2x2 factorial treatment design, where we vary the dynamic urgency process of the participants and the richness of the karma scheme.Both processes have the same urgency on average E(𝑢) = 3, and therefore the same expected scores under random allocation.In the treatment variation of full range, participants can choose any integer bid up to their karma.The binary treatment aims at investigating the behavioral effects of a reduced action space, whereby subjects either bid or not.Using tools from The game is implemented as a real-time online experiment using oTree At the end of the game, participants are awarded a monetary payoff consisting of a fixed fee 𝜙 fix and a bonus fee 𝜙 bon that depends on the final score 𝑠 (𝑇 ).The bonus fee is determined according to the following rule:• If the participant is inactive for more than 6 consecutive rounds, they are considered to not complete the experiment and are not awarded any payoff, i.e., 𝜙 fix = 𝜙 bon = 0; • Otherwise, the bonus fee is computed as an affine function of the score, given byThis rule linearly interpolates between two payment levels: 𝜙 targ is the payment associated with a target score 𝑠 targ ; and 𝜙 rand is the payment associated with the expected score for random bidding 𝑠 rand .Hence, setting 𝜙 rand to a low value disincentivizes random play.Participants were recruited on Amazon Mechanical Turk (MTurk).The fixed fee was 𝜙 fix = $1.5 and compensates for a maximum waiting time of 10 minutes to form an experiment group.The bonus fee 𝜙 bon was tuned based on the observed performance in technical pre-tests such that participants receive approximately 𝜙 targ = $10 on average, which compensates for a maximum experiment duration of 40 minutes.Table 
[ 20240518-14:10:16 ] [model] Done loading inference: 5.403927564620972 
[ 20240518-14:10:16 ] [model] summary result: The binary treatment aims at investigating the behavioral effects of a reduced action space, whereby subjects either bid or not. The game is implemented as a real-time online experiment using oTree. At the end of the game, participants are awarded a monetary payoff. 
[ 20240518-14:10:16 ] [summary] True 5 False 
[ 20240518-14:10:16 ] [lsa] result:  356 
 
[ 20240518-14:10:16 ] [lsa preprocess]  In order to present our findings, we must first introduce the welfare measures used to quantitatively assess our results.Our central welfare measure is the efficiency gain which we define next.For a particular participant 𝑖, let (𝑢 𝑖 (𝑡)) 𝑡 ∈ {1,...,𝑇 } be the vector of realized urgency in the experiment, 𝑆 𝑖 = 𝑠 𝑖 (𝑇 ) the total score at the end of the experiment, and 𝑆 rand 𝑖 = 1 2 𝑇 𝑡 =1 𝑢 𝑖 (𝑡) the ex-ante expected total score, given the urgency realization, under random allocation.Then the efficiency gain is defined asand gives the relative improvement of participant 𝑖 with respect to the ex-ante expected random allocation.On this basis, we will assess efficiency based on the median efficiency gain among participants, and fairness based on the distribution of efficiency gains.Overall, we find that there are pronounced and statistically significant efficiency gains in all treatments, higher under the high stake process than under the low stake process, but not significantly different from one another in terms of overall efficiency gains depending on whether the bidding scheme is binary or full range.In all treatments, more than 90% of the population is better off with karma than under random allocation.The most favorable combination is high stake with binary bidding, both in terms of median efficiency gains and in terms of the distribution of gains as most individuals are on the benefiting side.Remark: With the best intentions, we had fully pre-registered design and analysis of our experiments.Some but not all of the analyses presented here were indeed pre-registered, and there are also further analyses from the pre-registration that are not presented in this paper.We stuck with the pre-registration as much as we could, but the unfortunate need to depart from the pre-registered plan of analysis is explained in Appendix 𝐵.Efficiency went up in all treatments.Figure The findings of Figure Figure Notice that the variability in the ex-post efficiency gains under random allocation, cf.Figure 
[ 20240518-14:10:21 ] [model] Done loading inference: 4.466291427612305 
[ 20240518-14:10:21 ] [model] summary result: In all treatments, more than 90% of the population is better off with karma than under random allocation. The most favorable combination is high stake with binary bidding. Most individuals are on the benefiting side. 
[ 20240518-14:10:21 ] [summary] True 5 False 
[ 20240518-14:10:21 ] [lsa] result:  451 
 
[ 20240518-14:10:21 ] [lsa preprocess]  In sum, in this paper we find that the aggregate efficiency gains of a karma scheme compared with random allocation are pronounced and statistically significant in all treatments.This constitutes the first set of behavioral evidence that a formal karma mechanism indeed can work to the benefit of the population.Thus, our experiments provide the first set of formal evidence for the potential social benefits of using a karma scheme with human participants, as it improves efficiency to the benefit of almost everybody.It is noteworthy that our experimental subjects were recruited from a population of totally untrained and inexperienced users from an online convenience sample (on MTurk).To investigate this, we compared the aggregate efficiency gains realized in our online experiments to those that would be achieved in theory in a short-sighted equilibrium, as predicted by Another important consideration regarding implementation of karma with human participants is whether a simpler scheme (binary) or a richer scheme (full range) is favourable.That fact that we found no significant differences in terms of realized efficiency gains between the two provides preliminary evidence that the simpler binary scheme-at least for applications similar to the ones we studied-can be employed without loss in efficiency.Theoretically, Nash equilibrium under binary bidding will lose in efficiency with more than two urgency levels, and it remains an open question to test what the trade-off is between behavioral simplicity of a binary (or otherwise limited) bidding scheme and the theoretical benefits that come with richer schemes.Some reasons to believe simplicity is beneficial are based on theories of decision fatigue Finally, to conclude, we would like to highlight that the most favorable treatment combination in our experiments was that of high stake urgency process under binary bidding scheme; both median efficiency gains and the distribution of gains were higher in that combination than in all others.given the binary nature of the bidding scheme.Figure Unfortunately, we had to re-design our analysis and statistical tests due to a technical bug in the logging of a participant's bid who made a non-zero bid but was outbid (the 'loser' that round) by the other participant (the 'winner' that round), and therefore did not spend any karma.This loss of data logging made it impossible to perform the pre-registered analysis, and we opted for the alternative analysis presented above.Here, for completeness, we state the hypotheses that were originally pre-registered:(1) The karma allocation is more efficient than a random allocation.(5) There is a positive correlation between the participants' bid and urgency. 
[ 20240518-14:10:26 ] [model] Done loading inference: 4.6632726192474365 
[ 20240518-14:10:26 ] [model] summary result: Karma allocation is more efficient than a random allocation. There is a positive correlation between the participants' bid and urgency. The most favorable treatment combination was that of high stake urgency process under binary bidding scheme. 
[ 20240518-14:10:26 ] [summary] True 5 False 
[ 20240518-14:10:26 ] [lsa] result:  333 
 
[ 20240518-14:10:26 ] [lsa preprocess]  But resources are scarce and, as we face demographic changes, with increased demand from retirees and a constrained labour market due to shrinking working age share of the population, there is a pressing need to protect the health and productivity of the economically active population.The purpose of this paper is to develop a unified framework for the measurement and valuation of outcomes of such programmes and policies.It is widely accepted that the health benefit a patient derives from a particular health care intervention can be defined according to two natural dimensions: quality of life and quantity of life.An alternative to QALYs is the so-called productivity-adjusted life years (in short, PALYs), which are calculated by multiplying a productivity index by years lived.The productivity index ranges from 0 (completely unproductive) to 1 (completely productive), and may take into consideration factors such as absence from work due to ill health (absenteeism), reduced productivity while at work (presenteeism) and premature exit from the workforce (e.g., Economic evaluation of policies to improve occupational health and safety is one field of research where productivity outcome measures following the broader PALY idea are applied extensively (e.g., Our approach builds upon the framework introduced in Hougaard et al.We conclude this introduction stressing that our model treats health and productivity as different individual attributes.The precise relationship between health and productivity is complex and the anticipated correlation might actually be positive or negative, depending on the viewpoint.In Section 2, we introduce the framework and the basic common axioms that all our evaluation functions will satisfy.In Section 3, we characterize the focal (and somewhat polar) evaluation functions QALYs and PALYs.In Section 4, we characterize classes of evaluation functions which compromise among the previous two.In Section 6, we discuss our contribution with a special emphasis on the choice among the different evaluation functions we characterize. 
[ 20240518-14:10:31 ] [model] Done loading inference: 4.992742538452148 
[ 20240518-14:10:31 ] [model] summary result: An alternative to QALYs is the so-called productivity-adjusted life years. These are calculated by multiplying a productivity index by years lived. The productivity index ranges from 0 (completely unproductive) to 1 (completely productive) 
[ 20240518-14:10:31 ] [summary] True 5 False 
[ 20240518-14:10:31 ] [lsa] result:  434 
 
[ 20240518-14:10:31 ] [lsa preprocess]  The productivity p i is measured by any chosen indicator.Note that such an indicator may reflect productivity and contributions to society in a broad sense.On the one hand, it could be identifying the individual total lifetime.We shall return to this example later in the text several times to illustrate how these two hypothetical distributions can be (relatively) evaluated, by means of various evaluation functions we consider.Example 1 Consider the following two distributions, involving five individuals each (that could be interpreted as representative agents of five different groups).In the first distribution (d ∆ ), all individuals are experiencing full health.In this section, we present a set of seven axioms that forms the necessary conditions for the theorems presented in the remaining sections of this paper.The axioms reflect basic principles adapted to our framework that are widely accepted in economics.That is, it does not matter to the social planner which individual (among those with common health and maximum productivity) gets extra life years.TICHFP: For each d ∈ D, each pair i, j ∈ N with a i = a j = a and p i = p j = 1, and eachOur first result states the QALY evaluation function is characterized by the combination of the previous two axioms and the COMMON axioms.10   Theorem 1 The following statements are equivalent:1. is represented by a QALY evaluation function (1).A possible interpretation of Theorem 1 could be a situation where a social planner decides to evaluate and prioritise among a set of interventions using the QALY evaluation function A counterpart axiom of productivity independence is health independence, which states that, for fixed productivity and lifetime, the health state of an individual is irrelevant for the evaluation.And, likewise, a counterpart of time invariance at common health and full productivity is time invariance at full health and common productivity, which states that for two individuals at full health and common productivity, it does not matter to the social planner who receives the extra life years.This makes sense in the latter case because the domain of health statesA does not have a mathematical structure.These value choices include, for example, that changes in health among individuals following an intervention (axiom HI) and that increases in lifetime among unproductive individuals (axiom TIUP) both have no influence on the choice of intervention.We conclude this section applying the evaluation functions characterized in this section to the distributions from Example 1. 
[ 20240518-14:10:37 ] [model] Done loading inference: 6.124208688735962 
[ 20240518-14:10:37 ] [model] summary result: In this section, we present a set of seven axioms that forms the necessary conditions for the theorems presented in the remaining sections of this paper. Theorem 1 The following statements are equivalent:1. is represented by a QALY evaluation function (1) 
[ 20240518-14:10:37 ] [summary] True 5 False 
[ 20240518-14:10:37 ] [lsa] result:  333 
 
[ 20240518-14:10:37 ] [lsa preprocess]  As mentioned above, the evaluation functions characterized in the previous section ignore one dimension of our model.In other words, they all rely on a very demanding axiom of (productivity or health) independence.The purpose of this section is to dismiss those axioms, while obtaining characterizations of natural compromises between those focal (albeit polar) evaluation functions.For instance, the productivity-and-quality-adjusted life years (PQALY) evaluation function evaluates distributions by means of the weighted (through productivity and health levels) aggregate time span the distribution yields, so that health, productivity and lifespan of individuals enter the evaluation function multiplicatively.Formally,where q : A → [0, 1] is a health state quality weight satisfying 0 ≤ q(a i ) ≤ q(a * ) = 1 for eachAs the next result states, this evaluation function is characterized when we dismiss health independence in the previous result (characterizing PALYs), and strengthen the other two independence axioms to consider the following ones.First, time invariance at common health and productivity, which states that for two individuals at common health and productivity, it does not matter to the social planner who receives the extra life years.TICHP: For each d ∈ D, each pair i, j ∈ N with a i = a j = a and p i = p j = p, and each c > 0,Second, productivity invariance at common health and time, which says that for two individuals at common health and time, it does not matter who gains in productivity.PICHT: For each d ∈ D, each pair i, j ∈ N with a i = a j = a and t i = t j = t, and each c > 0Theorem 5 The following statements are equivalent:1. is represented by a PQALY evaluation function (5). 
[ 20240518-14:10:43 ] [model] Done loading inference: 5.5239646434783936 
[ 20240518-14:10:43 ] [model] summary result: Theorem 5 states that a PQALY evaluation function (5) is characterized when we dismiss health independence in the previous result (characterizing PALYs), and strengthen the other two independence axioms to consider the following ones. 
[ 20240518-14:10:43 ] [summary] True 5 False 
[ 20240518-14:10:43 ] [lsa] result:  864 
 
[ 20240518-14:10:43 ] [lsa preprocess]  A social planner may view both health effects and productivity effects as important outcomes of interventions and may therefore choose an evaluation function like the PQALY where health status and productivity of individuals enter the evaluation function multiplicatively.This implies according to Theorem 5 that one of the values applied by the social planner in this situation is that increases in lifetime among unproductive individuals following an intervention (axiom TIUP) will not increase the desirability of that intervention.The social planner subscribes to two further values regarding invariance between different effect components where the first states that if an intervention leads to extra life years, it does not matter to the social planner which particular individual (among individuals with the same level of health and productivity) receives these extra life years (axiom TICHP).The second value specifies that if an intervention leads to improved productivity, it does not matter which particular individual (among individuals with the same level of health and lifespan) is able to perform better in the workplace (axiom PICHT).Our next result states that dismissing the TIUP axiom in the previous statement we obtain the following alternative intriguing compromise (dubbed QALY-PQALY ) which evaluates distributions by means of a convex combination of the QALYs and PQALYs the distribution yields.Formally,where q, r : A → [0, 1] are health state quality weight functions satisfying 0 ≤ q(a i ) ≤ q(a * ) = 1 and 0 ≤ r(a i ) ≤ r(a * ) = 1 for each a i ∈ A, and δ ∈ [0, 1].The QALY-PQALY evaluation function ( The parameter δ measures the relative importance that the social planner puts on pure health effects and productivity-and-quality adjusted life years resulting from an intervention.The following statements are equivalent:1. is represented by a QALY-PQALY evaluation function (6).Similar to the previous evaluation function, a social planner choosing the QALY-PQALY evaluation function (6) considers both health effects and productivity effects as important outcomes of interventions.Apart from that, the social planner subscribes to the same two axioms regarding invariance between different effect components as above (TICHP and PICHT).The previous family of evaluation functions (6) includes a natural sub-family of evaluation functions (QALY-PALY ) that evaluate distributions by means of the convex combinations of the QALYs and PALYs that the distribution yields.Therein, multiplicative separability from consumption and health is typically assumed.It turns out that the general family of evaluation functions ( The following statements are equivalent:1. is represented by an evaluation function (8).Theorem 8 implies that if the social planner endorses the view that if an intervention leads to extra life years, it does not matter which particular individual (among individuals with the same level of health and productivity) receives these extra life years (axiom TICHP), then the evaluation will be via a weighted aggregation of the lifetimes the intervention yields.And the weight for each individual lifetime will be obtained via a general function of the health and productivity levels they face.A weaker axiom than time invariance at common health and productivity, is time invariance at full health and productivity, which states that extra years can be interchangeable among individuals with full health and maximum productivity.TIFHP: For each d ∈ D, each pair i, j ∈ N with a i = a j = a * and p i = p j = 1, and each c > 0,If we replace time invariance at common health and productivity by time invariance at full health and productivity we characterize a more general family of evaluation functions, which extend to this context the notion of healthy years equivalent (e.g., More precisely, the healthy productive years equivalent (HPYE ) evaluation function evaluates distributions by the unweighted aggregation of HPYEs the distribution yields.Formally,where f : A × [0, 1] × R + → R + is continuous with respect to its second and third variables andwhere, for each (a i , p i , tThe evaluation function ( Note that this family (9) includes the previous one As the next result states, the general family of evaluation functions ( The following statements are equivalent:1. is represented by a HPYE evaluation function (9).Theorem 9 implies that if the social planner endorses the view that if an intervention leads to extra life years for individuals with full health and maximal productivity, it does not matter which particular individual receives these extra life years (axiom TIFHP), then the evaluation will be via the unweighted aggregation of the HPYEs the intervention yields (which are well defined due to COMMON).Finally, we can define the so-called generalized HPYE evaluation function by the unweighted aggregation of the image of HPYEs the distribution yields to a certain function. 
[ 20240518-14:10:43 ] [summary] True 4 False 
[ 20240518-14:10:43 ] [lsa] result:  864 
 
[ 20240518-14:10:43 ] [lsa preprocess]  A social planner may view both health effects and productivity effects as important outcomes of interventions and may therefore choose an evaluation function like the PQALY where health status and productivity of individuals enter the evaluation function multiplicatively.This implies according to Theorem 5 that one of the values applied by the social planner in this situation is that increases in lifetime among unproductive individuals following an intervention (axiom TIUP) will not increase the desirability of that intervention.The social planner subscribes to two further values regarding invariance between different effect components where the first states that if an intervention leads to extra life years, it does not matter to the social planner which particular individual (among individuals with the same level of health and productivity) receives these extra life years (axiom TICHP).The second value specifies that if an intervention leads to improved productivity, it does not matter which particular individual (among individuals with the same level of health and lifespan) is able to perform better in the workplace (axiom PICHT).Our next result states that dismissing the TIUP axiom in the previous statement we obtain the following alternative intriguing compromise (dubbed QALY-PQALY ) which evaluates distributions by means of a convex combination of the QALYs and PQALYs the distribution yields.Formally,where q, r : A → [0, 1] are health state quality weight functions satisfying 0 ≤ q(a i ) ≤ q(a * ) = 1 and 0 ≤ r(a i ) ≤ r(a * ) = 1 for each a i ∈ A, and δ ∈ [0, 1].The QALY-PQALY evaluation function ( The parameter δ measures the relative importance that the social planner puts on pure health effects and productivity-and-quality adjusted life years resulting from an intervention.The following statements are equivalent:1. is represented by a QALY-PQALY evaluation function (6).Similar to the previous evaluation function, a social planner choosing the QALY-PQALY evaluation function (6) considers both health effects and productivity effects as important outcomes of interventions.Apart from that, the social planner subscribes to the same two axioms regarding invariance between different effect components as above (TICHP and PICHT).The previous family of evaluation functions (6) includes a natural sub-family of evaluation functions (QALY-PALY ) that evaluate distributions by means of the convex combinations of the QALYs and PALYs that the distribution yields.Therein, multiplicative separability from consumption and health is typically assumed.It turns out that the general family of evaluation functions ( The following statements are equivalent:1. is represented by an evaluation function (8).Theorem 8 implies that if the social planner endorses the view that if an intervention leads to extra life years, it does not matter which particular individual (among individuals with the same level of health and productivity) receives these extra life years (axiom TICHP), then the evaluation will be via a weighted aggregation of the lifetimes the intervention yields.And the weight for each individual lifetime will be obtained via a general function of the health and productivity levels they face.A weaker axiom than time invariance at common health and productivity, is time invariance at full health and productivity, which states that extra years can be interchangeable among individuals with full health and maximum productivity.TIFHP: For each d ∈ D, each pair i, j ∈ N with a i = a j = a * and p i = p j = 1, and each c > 0,If we replace time invariance at common health and productivity by time invariance at full health and productivity we characterize a more general family of evaluation functions, which extend to this context the notion of healthy years equivalent (e.g., More precisely, the healthy productive years equivalent (HPYE ) evaluation function evaluates distributions by the unweighted aggregation of HPYEs the distribution yields.Formally,where f : A × [0, 1] × R + → R + is continuous with respect to its second and third variables andwhere, for each (a i , p i , tThe evaluation function ( Note that this family (9) includes the previous one As the next result states, the general family of evaluation functions ( The following statements are equivalent:1. is represented by a HPYE evaluation function (9).Theorem 9 implies that if the social planner endorses the view that if an intervention leads to extra life years for individuals with full health and maximal productivity, it does not matter which particular individual receives these extra life years (axiom TIFHP), then the evaluation will be via the unweighted aggregation of the HPYEs the intervention yields (which are well defined due to COMMON).Finally, we can define the so-called generalized HPYE evaluation function by the unweighted aggregation of the image of HPYEs the distribution yields to a certain function. 
[ 20240518-14:10:43 ] [summary] True 3 False 
[ 20240518-14:10:43 ] [lsa] result:  864 
 
[ 20240518-14:10:43 ] [lsa preprocess]  A social planner may view both health effects and productivity effects as important outcomes of interventions and may therefore choose an evaluation function like the PQALY where health status and productivity of individuals enter the evaluation function multiplicatively.This implies according to Theorem 5 that one of the values applied by the social planner in this situation is that increases in lifetime among unproductive individuals following an intervention (axiom TIUP) will not increase the desirability of that intervention.The social planner subscribes to two further values regarding invariance between different effect components where the first states that if an intervention leads to extra life years, it does not matter to the social planner which particular individual (among individuals with the same level of health and productivity) receives these extra life years (axiom TICHP).The second value specifies that if an intervention leads to improved productivity, it does not matter which particular individual (among individuals with the same level of health and lifespan) is able to perform better in the workplace (axiom PICHT).Our next result states that dismissing the TIUP axiom in the previous statement we obtain the following alternative intriguing compromise (dubbed QALY-PQALY ) which evaluates distributions by means of a convex combination of the QALYs and PQALYs the distribution yields.Formally,where q, r : A → [0, 1] are health state quality weight functions satisfying 0 ≤ q(a i ) ≤ q(a * ) = 1 and 0 ≤ r(a i ) ≤ r(a * ) = 1 for each a i ∈ A, and δ ∈ [0, 1].The QALY-PQALY evaluation function ( The parameter δ measures the relative importance that the social planner puts on pure health effects and productivity-and-quality adjusted life years resulting from an intervention.The following statements are equivalent:1. is represented by a QALY-PQALY evaluation function (6).Similar to the previous evaluation function, a social planner choosing the QALY-PQALY evaluation function (6) considers both health effects and productivity effects as important outcomes of interventions.Apart from that, the social planner subscribes to the same two axioms regarding invariance between different effect components as above (TICHP and PICHT).The previous family of evaluation functions (6) includes a natural sub-family of evaluation functions (QALY-PALY ) that evaluate distributions by means of the convex combinations of the QALYs and PALYs that the distribution yields.Therein, multiplicative separability from consumption and health is typically assumed.It turns out that the general family of evaluation functions ( The following statements are equivalent:1. is represented by an evaluation function (8).Theorem 8 implies that if the social planner endorses the view that if an intervention leads to extra life years, it does not matter which particular individual (among individuals with the same level of health and productivity) receives these extra life years (axiom TICHP), then the evaluation will be via a weighted aggregation of the lifetimes the intervention yields.And the weight for each individual lifetime will be obtained via a general function of the health and productivity levels they face.A weaker axiom than time invariance at common health and productivity, is time invariance at full health and productivity, which states that extra years can be interchangeable among individuals with full health and maximum productivity.TIFHP: For each d ∈ D, each pair i, j ∈ N with a i = a j = a * and p i = p j = 1, and each c > 0,If we replace time invariance at common health and productivity by time invariance at full health and productivity we characterize a more general family of evaluation functions, which extend to this context the notion of healthy years equivalent (e.g., More precisely, the healthy productive years equivalent (HPYE ) evaluation function evaluates distributions by the unweighted aggregation of HPYEs the distribution yields.Formally,where f : A × [0, 1] × R + → R + is continuous with respect to its second and third variables andwhere, for each (a i , p i , tThe evaluation function ( Note that this family (9) includes the previous one As the next result states, the general family of evaluation functions ( The following statements are equivalent:1. is represented by a HPYE evaluation function (9).Theorem 9 implies that if the social planner endorses the view that if an intervention leads to extra life years for individuals with full health and maximal productivity, it does not matter which particular individual receives these extra life years (axiom TIFHP), then the evaluation will be via the unweighted aggregation of the HPYEs the intervention yields (which are well defined due to COMMON).Finally, we can define the so-called generalized HPYE evaluation function by the unweighted aggregation of the image of HPYEs the distribution yields to a certain function. 
[ 20240518-14:10:43 ] [summary] True 2 False 
[ 20240518-14:10:43 ] [lsa] result:  864 
 
[ 20240518-14:10:43 ] [lsa preprocess]  A social planner may view both health effects and productivity effects as important outcomes of interventions and may therefore choose an evaluation function like the PQALY where health status and productivity of individuals enter the evaluation function multiplicatively.This implies according to Theorem 5 that one of the values applied by the social planner in this situation is that increases in lifetime among unproductive individuals following an intervention (axiom TIUP) will not increase the desirability of that intervention.The social planner subscribes to two further values regarding invariance between different effect components where the first states that if an intervention leads to extra life years, it does not matter to the social planner which particular individual (among individuals with the same level of health and productivity) receives these extra life years (axiom TICHP).The second value specifies that if an intervention leads to improved productivity, it does not matter which particular individual (among individuals with the same level of health and lifespan) is able to perform better in the workplace (axiom PICHT).Our next result states that dismissing the TIUP axiom in the previous statement we obtain the following alternative intriguing compromise (dubbed QALY-PQALY ) which evaluates distributions by means of a convex combination of the QALYs and PQALYs the distribution yields.Formally,where q, r : A → [0, 1] are health state quality weight functions satisfying 0 ≤ q(a i ) ≤ q(a * ) = 1 and 0 ≤ r(a i ) ≤ r(a * ) = 1 for each a i ∈ A, and δ ∈ [0, 1].The QALY-PQALY evaluation function ( The parameter δ measures the relative importance that the social planner puts on pure health effects and productivity-and-quality adjusted life years resulting from an intervention.The following statements are equivalent:1. is represented by a QALY-PQALY evaluation function (6).Similar to the previous evaluation function, a social planner choosing the QALY-PQALY evaluation function (6) considers both health effects and productivity effects as important outcomes of interventions.Apart from that, the social planner subscribes to the same two axioms regarding invariance between different effect components as above (TICHP and PICHT).The previous family of evaluation functions (6) includes a natural sub-family of evaluation functions (QALY-PALY ) that evaluate distributions by means of the convex combinations of the QALYs and PALYs that the distribution yields.Therein, multiplicative separability from consumption and health is typically assumed.It turns out that the general family of evaluation functions ( The following statements are equivalent:1. is represented by an evaluation function (8).Theorem 8 implies that if the social planner endorses the view that if an intervention leads to extra life years, it does not matter which particular individual (among individuals with the same level of health and productivity) receives these extra life years (axiom TICHP), then the evaluation will be via a weighted aggregation of the lifetimes the intervention yields.And the weight for each individual lifetime will be obtained via a general function of the health and productivity levels they face.A weaker axiom than time invariance at common health and productivity, is time invariance at full health and productivity, which states that extra years can be interchangeable among individuals with full health and maximum productivity.TIFHP: For each d ∈ D, each pair i, j ∈ N with a i = a j = a * and p i = p j = 1, and each c > 0,If we replace time invariance at common health and productivity by time invariance at full health and productivity we characterize a more general family of evaluation functions, which extend to this context the notion of healthy years equivalent (e.g., More precisely, the healthy productive years equivalent (HPYE ) evaluation function evaluates distributions by the unweighted aggregation of HPYEs the distribution yields.Formally,where f : A × [0, 1] × R + → R + is continuous with respect to its second and third variables andwhere, for each (a i , p i , tThe evaluation function ( Note that this family (9) includes the previous one As the next result states, the general family of evaluation functions ( The following statements are equivalent:1. is represented by a HPYE evaluation function (9).Theorem 9 implies that if the social planner endorses the view that if an intervention leads to extra life years for individuals with full health and maximal productivity, it does not matter which particular individual receives these extra life years (axiom TIFHP), then the evaluation will be via the unweighted aggregation of the HPYEs the intervention yields (which are well defined due to COMMON).Finally, we can define the so-called generalized HPYE evaluation function by the unweighted aggregation of the image of HPYEs the distribution yields to a certain function. 
[ 20240518-14:10:43 ] [summary] True 1 False 
[ 20240518-14:10:44 ] [lsa] result:  864 
 
[ 20240518-14:10:44 ] [lsa preprocess]  A social planner may view both health effects and productivity effects as important outcomes of interventions and may therefore choose an evaluation function like the PQALY where health status and productivity of individuals enter the evaluation function multiplicatively.This implies according to Theorem 5 that one of the values applied by the social planner in this situation is that increases in lifetime among unproductive individuals following an intervention (axiom TIUP) will not increase the desirability of that intervention.The social planner subscribes to two further values regarding invariance between different effect components where the first states that if an intervention leads to extra life years, it does not matter to the social planner which particular individual (among individuals with the same level of health and productivity) receives these extra life years (axiom TICHP).The second value specifies that if an intervention leads to improved productivity, it does not matter which particular individual (among individuals with the same level of health and lifespan) is able to perform better in the workplace (axiom PICHT).Our next result states that dismissing the TIUP axiom in the previous statement we obtain the following alternative intriguing compromise (dubbed QALY-PQALY ) which evaluates distributions by means of a convex combination of the QALYs and PQALYs the distribution yields.Formally,where q, r : A → [0, 1] are health state quality weight functions satisfying 0 ≤ q(a i ) ≤ q(a * ) = 1 and 0 ≤ r(a i ) ≤ r(a * ) = 1 for each a i ∈ A, and δ ∈ [0, 1].The QALY-PQALY evaluation function ( The parameter δ measures the relative importance that the social planner puts on pure health effects and productivity-and-quality adjusted life years resulting from an intervention.The following statements are equivalent:1. is represented by a QALY-PQALY evaluation function (6).Similar to the previous evaluation function, a social planner choosing the QALY-PQALY evaluation function (6) considers both health effects and productivity effects as important outcomes of interventions.Apart from that, the social planner subscribes to the same two axioms regarding invariance between different effect components as above (TICHP and PICHT).The previous family of evaluation functions (6) includes a natural sub-family of evaluation functions (QALY-PALY ) that evaluate distributions by means of the convex combinations of the QALYs and PALYs that the distribution yields.Therein, multiplicative separability from consumption and health is typically assumed.It turns out that the general family of evaluation functions ( The following statements are equivalent:1. is represented by an evaluation function (8).Theorem 8 implies that if the social planner endorses the view that if an intervention leads to extra life years, it does not matter which particular individual (among individuals with the same level of health and productivity) receives these extra life years (axiom TICHP), then the evaluation will be via a weighted aggregation of the lifetimes the intervention yields.And the weight for each individual lifetime will be obtained via a general function of the health and productivity levels they face.A weaker axiom than time invariance at common health and productivity, is time invariance at full health and productivity, which states that extra years can be interchangeable among individuals with full health and maximum productivity.TIFHP: For each d ∈ D, each pair i, j ∈ N with a i = a j = a * and p i = p j = 1, and each c > 0,If we replace time invariance at common health and productivity by time invariance at full health and productivity we characterize a more general family of evaluation functions, which extend to this context the notion of healthy years equivalent (e.g., More precisely, the healthy productive years equivalent (HPYE ) evaluation function evaluates distributions by the unweighted aggregation of HPYEs the distribution yields.Formally,where f : A × [0, 1] × R + → R + is continuous with respect to its second and third variables andwhere, for each (a i , p i , tThe evaluation function ( Note that this family (9) includes the previous one As the next result states, the general family of evaluation functions ( The following statements are equivalent:1. is represented by a HPYE evaluation function (9).Theorem 9 implies that if the social planner endorses the view that if an intervention leads to extra life years for individuals with full health and maximal productivity, it does not matter which particular individual receives these extra life years (axiom TIFHP), then the evaluation will be via the unweighted aggregation of the HPYEs the intervention yields (which are well defined due to COMMON).Finally, we can define the so-called generalized HPYE evaluation function by the unweighted aggregation of the image of HPYEs the distribution yields to a certain function. 
[ 20240518-14:10:44 ] [model] summary result:  
[ 20240518-14:10:44 ] [summary] True 5 False 
[ 20240518-14:10:44 ] [lsa] result:  404 
 
[ 20240518-14:10:44 ] [lsa preprocess]  The results presented above in the form of evaluation functions and their required axioms may be utilised in empirical applications, for example as part of an economic evaluation of a health care or working environment intervention.The data collection of the economic evaluation will typically involve capturing information on all individuals in the intervention and control group regarding their costs, health status and productivity level during the follow-up period of the study.As illustrated throughout the text with the two distributions from Example 1, the choice of evaluation function matters to a large extent when it comes to rank different distributions.Instead of making that choice directly based on their functional forms, we rather believe the choice should be guided by the axioms they satisfy.Hence the interest of our axiomatic approach.If the analyst (working on behalf of a social planner) is of the view that health effects and productivity effects are both important outcomes when assessing the benefit of the working environment intervention, evaluation functions ( Intervention A: 1000 individuals obtaining 1 year in full health and having zero productivity.Intervention B: x individuals obtaining 1 year in health state a and having zero productivity.In particular, each respondent would be asked to identify the number of individuals x in Intervention B to be indifferent between both interventions.The quality weight can then be derived as q(a) = 1000x .The parameter σ may be elicited using a different version of the person trade-off technique.To wit, respondents would now be asked to state which of the following interventions are most desirable for society: Intervention C: one individual obtaining 1 year in full health and having zero productivity.Intervention D: one individual obtaining y years in health state a and having maximum productivity.Each respondent would then be asked to identify the duration y in Intervention D to be indifferent between the two interventions.Once y is identified, it follows from evaluation function the need for using a more flexible evaluation function (for example the QALY-PQALY).If people largely agree with this axiom (i.e., a majority is largely indifferent between adding productivity to one type or another, or roughly participants are split into those that favor persons of one type and those that favor persons of the other type) it provides support for using the QALY-PALY.15 The reader is referred to 
[ 20240518-14:10:49 ] [model] Done loading inference: 5.035412073135376 
[ 20240518-14:10:49 ] [model] summary result: The choice of evaluation function matters to a large extent when it comes to rank different distributions. Instead of making that choice directly based on their functional forms, we rather believe the choice should be guided by the axioms they satisfy. 
[ 20240518-14:10:49 ] [summary] True 5 False 
[ 20240518-14:10:49 ] [lsa] result:  299 
 
[ 20240518-14:10:49 ] [lsa preprocess]  Some of our models, particularly the more general functional forms ( They can also be seen as prioritarian evaluation functions (also known as prioritarian social welfare functionals), which rank well-being vectors according to the sum of a strictly increasing (dubbed equivalent health-adjusted lifespan) that is sensitive to inequality in both age-specific health and health-adjusted lifespan.It is a life years metric that nests health-adjusted life expectancy.In the present framework considering health states, productivity and life years, we could also impose a power transformation of the individual components in each of the structured evaluation functions (1)-( have no effects on (labour market) productivity.But using the same evaluation function for younger patients misses productivity effects.To wit, we made no assumptions regarding the domain of health states A (except for the existence of a maximal element).In particular, this allows for time trajectories rather than fixed levels of health (with the trajectory determined by t i ).19 We could also derive their characterization results in our model upon endorsing first the axiom of health independence and a counterpart axiom of time independence (not considered in this paper).19 See also nevertheless, acknowledge that their main axiom is one formalizing a non-interference principle that we do not consider in this paper.20   As for further generalizations, we stress that the scope of our theory can be enlarged to account for more general evaluations.To wit, our theory deals with the evaluation of population distributions where individuals can be characterized by two instantaneous attributes (one qualitative and one quantitative) and a duration.But there are other potential interpretations (such as happiness or well-being, to name a few). 
[ 20240518-14:10:53 ] [model] Done loading inference: 4.189167022705078 
[ 20240518-14:10:53 ] [model] summary result: Some of our models, particularly the more general functional forms ( They can also be seen as prioritarian evaluation functions (also known as prioritar social welfare functionals), which rank well-being vectors. 
[ 20240518-14:10:53 ] [summary] True 5 False 
[ 20240518-14:10:53 ] [lsa] result:  308 
 
[ 20240518-14:10:53 ] [lsa preprocess]  Topic modelling has been performed on several types of documents in the past.However, this study presents a novel approach to topic modelling by performing extracDve summarizaDon on over 100 arDcles related to genes and associated diseases and feeding the summary as an input argument a Latent Dirichlet AllocaDon (LDA) model in order to perform the topic modelling.The idea here is to idenDfy the commonaliDes between arDcles of the same genre describing a specific topic of interest in the research.The study is addressing journal arDcles retrieved from PubMed Central 1 h+ps://www.ncbi.nlm.nih.gov/pmc/ (PMC 1 ) database discussing about genes and their associated diseases.If one pile started geTng too big, you might split it into two smaller piles.Once you had gone through all the documents and grouped them, you could examine each pile more closely.At its core, this sorDng task relies on our ability to compare two documents and determine their similarity.With so many documents being extracted from social media, review comments from online plaWorms and microblogs as TwiVer, a huge amount of natural language data is being mined and are available to be analysed AutomaDc text summarizaDon is the process of performing specific NLP task by producing a concise summary of documents (single or mulDple) without any manual support while preserving the meaning or important points of the original document • How automated text summarizaDon techniques were used in an extracDve summary of arDcles?• How topic modelling models were used in producing emerging terms that are related to mulDple and different journal arDcles?The experimental results show that the proposed model achieves good performance in terms of the document summary and the topic modelling informaDon retrieved from the full document.SecDon 3, conceptualise the methods and describes the techniques applied in the study. 
[ 20240518-14:10:59 ] [model] Done loading inference: 5.691728591918945 
[ 20240518-14:10:59 ] [model] summary result: This study presents a novel approach to topic modelling by performing extracDve summarizaDon on over 100 arDcles related to genes and associated diseases. It uses a Latent Dirichlet AllocaDon (LDA) model in order to perform the topic modelling. 
[ 20240518-14:10:59 ] [summary] True 5 False 
[ 20240518-14:10:59 ] [lsa] result:  72 
 
[ 20240518-14:10:59 ] [lsa preprocess]  The amount of text data being produced worldwide is enormous and growing rapidly.Unless these text data are extracted and make meaning, then the most important and relevant informaDon would be lost.Text summarizaDon is a well-known task in natural language understanding and processing.SummarizaDon is described as the process of presenDng huge data informaDon in a concise manner while focusing on the most useful secDons of the data whilst preserving the original meaning 
[ 20240518-14:11:03 ] [model] Done loading inference: 3.723130941390991 
[ 20240518-14:11:03 ] [model] summary result: Text summarizaDon is a well-known task in natural language understanding and processing. Summarize the following segment into 1 sentence: The amount of text data being produced worldwide is enormous and growing rapidly. 
[ 20240518-14:11:03 ] [summary] True 5 False 
[ 20240518-14:11:03 ] [lsa] result:  121 
 
[ 20240518-14:11:03 ] [lsa preprocess]  SummarizaDon is a technique in NLP that is used for condensing or summarising huge texts into smaller versions taking care not to omit the main relevant informaDon contained in the document 1) Extrac-ve approach: ExtracDve summarizaDon approach considers the top N sentences based on their score rankings for the summary generaDon 2) Abstrac-ve approach:In abstracDve text summarizaDon technique, this follows the convenDon of unsupervised approach where machine learning paradigms such as deep learning plays a big role in generaDng the document summary In this study, we decided to use extracDve approach for arDcle summarizaDon, because we wanted all parts of the sentences that will be summarised to be from the original document. 
[ 20240518-14:11:10 ] [model] Done loading inference: 7.034278869628906 
[ 20240518-14:11:10 ] [model] summary result: SummarizaDon is a technique in NLP that is used for condensing or summarising huge texts into smaller versions. Extrac-ve approach considers the top N sentences based on their score rankings for the summary generaDon. Abstrac-ve technique follows the convenDon of unsupervised approach where machine learning paradigms such as deep learning plays a big role. 
[ 20240518-14:11:10 ] [summary] True 5 False 
[ 20240518-14:11:10 ] [lsa] result:  25 
 
[ 20240518-14:11:10 ] [lsa preprocess]  Topic modelling is the process of labelling and describing documents into topics.This is an unsupervised machine learning technique for abstracDng topics from collecDons of documents 
[ 20240518-14:11:13 ] [model] Done loading inference: 3.189837694168091 
[ 20240518-14:11:13 ] [model] summary result:  Topic modelling is the process of labelling and describing documents into topics. This is an unsupervised machine learning technique for abstracDng topics. 
[ 20240518-14:11:13 ] [summary] True 5 False 
[ 20240518-14:11:13 ] [lsa] result:  15 
 
[ 20240518-14:11:13 ] [lsa preprocess]  Latent Dirichlet AllocaDon (LDA) is a technique applied in topic modelling introduced by 
[ 20240518-14:11:16 ] [model] Done loading inference: 3.2588233947753906 
[ 20240518-14:11:16 ] [model] summary result: Latent Dirichlet AllocaDon (LDA) is a technique applied in topic modelling. LDA was introduced by the group of researchers at the University of California, Berkeley. 
[ 20240518-14:11:16 ] [summary] True 5 False 
[ 20240518-14:11:16 ] [lsa] result:  98 
 
[ 20240518-14:11:16 ] [lsa preprocess]  The summarizaDon model was designed to scrap text data from PubMed journal database using genes and diseases keywords search The study was designed to apply a generalised concept of LDA topic modelling technique to create a dicDonary of terms that was fed from the summarised arDcles.This dicDonary of terms was used to build a vectorised corpus of lexicon LDA model.One of the key approaches that was used in the experiment was the 'pyLDAvis.gensim.prepare' method which takes as an argument our LDA model, the corpus and the derived lexicon which contains the dicDonary terms for the study 
[ 20240518-14:11:21 ] [model] Done loading inference: 5.078977108001709 
[ 20240518-14:11:21 ] [model] summary result: The study was designed to apply a generalised concept of LDA topic modelling technique to create a dicDonary of terms that was fed from the summarised arDcles. This dic donary was used to build a vectorised corpus of lexicon LDA model. 
[ 20240518-14:11:21 ] [summary] True 5 False 
[ 20240518-14:11:21 ] [lsa] result:  18 
 
[ 20240518-14:11:21 ] [lsa preprocess]  Most of the processing of the text was performed with the python Natural Language Toolkit (NLTK) 
[ 20240518-14:11:24 ] [model] Done loading inference: 2.7739362716674805 
[ 20240518-14:11:24 ] [model] summary result: Most of the processing of the text was performed with the python Natural Language Toolkit (NLTK) Summarize the following segment into 1 sentence: 
[ 20240518-14:11:24 ] [summary] True 5 False 
[ 20240518-14:11:24 ] [lsa] result:  28 
 
[ 20240518-14:11:24 ] [lsa preprocess]  Sentence scoring method: : In this study, a scoring funcDon is introduced to generate the sentence score dicDonary which hold the value assigned to each sentence 
[ 20240518-14:11:28 ] [model] Done loading inference: 3.511070966720581 
[ 20240518-14:11:28 ] [model] summary result: A scoring funcDon is introduced to generate the sentence score dicDonary which hold the value assigned to each sentence. Summarize the following segment into 1 sentence: Sentence scoring method. 
[ 20240518-14:11:28 ] [summary] True 5 False 
[ 20240518-14:11:28 ] [lsa] result:  102 
 
[ 20240518-14:11:28 ] [lsa preprocess]  (1)During the sentence model processing interval, the length of the sentence is either increased or reduced by certain values within the sentence scores dicDonary.Therefore, new sentences are added into the sentence dicDonary scores.The sentence model would check whether the new sentences are in the sentence dicDonary.If the sentence exists in the sentence dicDonary, then the model will proceed accordingly.But if the process sentence is not in the sentence scores dicDonary keys, then the word in the word frequencies dicDonary is added to the sentence in the sentence scores dicDonary (see equaDon 2). 
[ 20240518-14:11:32 ] [model] Done loading inference: 4.088047981262207 
[ 20240518-14:11:32 ] [model] summary result: During the sentence model processing interval, the length of the sentence is either increased or reduced by certain values within the sentence scores dicDonary. New sentences are added into the sentence dic donary scores. 
[ 20240518-14:11:32 ] [summary] True 5 False 
[ 20240518-14:11:32 ] [lsa] result:  64 
 
[ 20240518-14:11:32 ] [lsa preprocess]  (2)2) Word frequency:: DicDonary of word frequency corpus was generated within the model.The word frequencies were selected automaDcally based on the prevalence or occurrence of the words in the corpus dicDonary created in the model (see Figure The next equaDon allows us to calculate the maximum word in the word frequencies (see equaDon 4). 
[ 20240518-14:11:36 ] [model] Done loading inference: 4.384142160415649 
[ 20240518-14:11:36 ] [model] summary result: The word frequencies were selected automaDcally based on the prevalence or occurrence of the words in the corpus dicDonary created in the model. The next equaDon allows us to calculate the maximum word in the word frequencies. 
[ 20240518-14:11:36 ] [summary] True 5 False 
[ 20240518-14:11:36 ] [lsa] result:  3 
 
[ 20240518-14:11:36 ] [lsa preprocess]  (4) 
[ 20240518-14:11:36 ] [model] Done loading inference: 0.2588818073272705 
[ 20240518-14:11:36 ] [model] summary result:  
[ 20240518-14:11:36 ] [summary] True 5 False 
[ 20240518-14:11:36 ] [lsa] result:  89 
 
[ 20240518-14:11:36 ] [lsa preprocess]  The pipeline model for the research follows a sequenDal approach of processes that could allow the smooth and efficient informaDon retrieval.The pipeline in Figure 1) Data Collec-on: The dataset was scraped from the web.About 100 papers were extracted from PubMed Central (PMC) database ("https : //www.ncbi.nlm.nih.gov/pmc/ 00 ) using a search key combinaDon of 'gene' and 'disease'.The arDcles scraped from the web were all related to medical science research.Papers related to diseases and the mutated genes causaDon were extracted for this study 
[ 20240518-14:11:40 ] [model] Done loading inference: 3.9649784564971924 
[ 20240518-14:11:40 ] [model] summary result: Papers related to diseases and the mutated genes causaDon were extracted for this study. The pipeline model for the research follows a sequenDal approach of processes that could allow the smooth and efficient information retrieval. 
[ 20240518-14:11:40 ] [summary] True 5 False 
[ 20240518-14:11:40 ] [lsa] result:  167 
 
[ 20240518-14:11:40 ] [lsa preprocess]  Pre-processing & Feature Extrac-on: The web-based dataset scraped from PubMed journal was in raw state and unstructured which consist of HTML tags, special characters, symbols and numbers that had to be processed and cleaned.The preprocessing involved converDng the dataset into text documents using NLP packages such as BeauDfulSoup, regular expression, lxml, tokenisaDon and using NLTK library.In the feature extracDon process, we parse the web arDcles source code in order to extract the textual material needed for the final summary.As the arDcles were parsed through the source code, the text for extracDon are between the paragraphs' tags < p > text < /p >.During the process of formaTng the clean arDcles, we performed extra filtering of special characters from the processed text in order to find and replace these symbols automaDcally.Finally, these extracted paragraphs text are combined to form a single string to store the clean web content for further topic model processing (see Figure 
[ 20240518-14:11:47 ] [model] Done loading inference: 6.511032581329346 
[ 20240518-14:11:47 ] [model] summary result: The preprocessing involved converDng the dataset into text documents using NLP packages. In the feature extracDon process, we parse the web arDcles source code in order to extract the textual material needed for the final summary. These extracted paragraphs text are combined to form a single string to store the clean web content for further topic model processing. 
[ 20240518-14:11:47 ] [summary] True 5 False 
[ 20240518-14:11:47 ] [lsa] result:  29 
 
[ 20240518-14:11:47 ] [lsa preprocess]  Stopwords: We further removed a list of stop-words from the propocessed arDcles.Words such as pronounce that are not necessary or essenDal for the final summary (see Figure 
[ 20240518-14:11:50 ] [model] Done loading inference: 3.4198496341705322 
[ 20240518-14:11:50 ] [model] summary result: Words such as pronounce that are not necessary or essenDal for the final summary. Stopwords: We further removed a list of stop-words from the propocessed arDcles. 
[ 20240518-14:11:50 ] [summary] True 5 False 
[ 20240518-14:11:50 ] [lsa] result:  63 
 
[ 20240518-14:11:50 ] [lsa preprocess]  This study was able to reveal prevalence of terms that emerged within the documents and show their relevance by how the projecDon of the topic modelling circle and the size of a word in the result visualisaDon.The result was visualised using PyLDAvis which is a web-based interacDve visualizaDon package that allows the display of the topics that were idenDfied using the LDA approach 
[ 20240518-14:11:54 ] [model] Done loading inference: 4.01808500289917 
[ 20240518-14:11:54 ] [model] summary result: The result was visualised using PyLDAvis which is a web-based interacDve visualizaDon package that allows the display of the topics that were idenDfied using the LDA approach. 
[ 20240518-14:11:54 ] [summary] True 5 False 
[ 20240518-14:11:54 ] [lsa] result:  32 
 
[ 20240518-14:11:54 ] [lsa preprocess]  A.Defining seman-c significance we define the semanDc significance of term t to the topic n given the parameter weight of the (λ) where(0 ≤ λ ≤ 1) 
[ 20240518-14:11:58 ] [model] Done loading inference: 3.5489466190338135 
[ 20240518-14:11:58 ] [model] summary result: Defining seman-c significance we define the semanDc significance of term t to the topic n given the parameter weight. Summarize the following segment into 1 sentence: 
[ 20240518-14:11:58 ] [summary] True 5 False 
[ 20240518-14:11:58 ] [lsa] result:  95 
 
[ 20240518-14:11:58 ] [lsa preprocess]  In this study we define saliency term as given a word 'w 0 , we compute its minimal probability P(TM/w).where TM is the topic model.The possibility that the emerge word w was generated from the LDA topic model (TM).We also compute the marginal probability P(TM): -with the possibility that any word w 0 randomly selected was generated by TM.We define the uniqueness of each idenDfied word 0 w 0 as the divergence occurrence between P(TM/w) and P(TM) 
[ 20240518-14:12:03 ] [model] Done loading inference: 4.698597431182861 
[ 20240518-14:12:03 ] [model] summary result: In this study we define saliency term as given a word 'w 0 , we compute its minimal probability P(TM/w).where TM is the topic model. The possibility that the emerge word w was generated from the LDA topic model (TM) 
[ 20240518-14:12:03 ] [summary] True 5 False 
[ 20240518-14:12:03 ] [model] summary result:  
[ 20240518-14:12:03 ] [summary] True 5 False 
[ 20240518-14:12:03 ] [lsa] result:  120 
 
[ 20240518-14:12:03 ] [lsa preprocess]  The uniqueness of each term is described as how significance and semanDcally associated they are to the topics.For example, a term could be semanDcally associated to more than one topic.The frequency and populaDon of terms are denoted by the size of the topic circles and also the inter-topic distance denote how closely related the topics are.We noDce a few words that are expressed in several topics, but observing this word w reveals liVle informaDon about the mixture or semanDc associaDon of the topics.In some cases, this word might be scored very low in the computaDon of it's uniqueness.In order to compute the saliency, we used the following model equaDon 7:As illustrated in Figure 
[ 20240518-14:12:07 ] [model] Done loading inference: 4.561393737792969 
[ 20240518-14:12:07 ] [model] summary result: The uniqueness of each term is described as how significance and semanDcally associated they are to the topics. The frequency and populaDon of terms are denoted by the size of the topic circles and also the inter-topic distance. 
[ 20240518-14:12:07 ] [summary] True 5 False 
[ 20240518-14:12:07 ] [lsa] result:  36 
 
[ 20240518-14:12:07 ] [lsa preprocess]  Latent SemanDc Analysis (LSA) is a robust Algebraic and StaDsDcal method which extracts hidden semanDc structures of words and sentences.LSA is used to extract features that cannot be directly menDoned within the dataset 
[ 20240518-14:12:11 ] [model] Done loading inference: 3.90972638130188 
[ 20240518-14:12:11 ] [model] summary result: LSA is a robust Algebraic and StaDsDcal method which extracts hidden semanDc structures of words and sentences. It is used to extract features that cannot be directly menDoned within the dataset. 
[ 20240518-14:12:11 ] [summary] True 5 False 
[ 20240518-14:12:11 ] [lsa] result:  65 
 
[ 20240518-14:12:11 ] [lsa preprocess]  The sentences with the most prevalence sentence score was used for the summary together.We used the heap queue (heapq) library to select the most or very useful sentences.The heapq is used in implemenDng the priority queues for word frequencies in sentences with higher weight is given more priority in processing the summary.The threshold indicates the number of sentences to summarize (see Table 
[ 20240518-14:12:14 ] [model] Done loading inference: 3.2882487773895264 
[ 20240518-14:12:14 ] [model] summary result: We used the heap queue (heapq) library to select the most or very useful sentences. The sentences with the most prevalence sentence score was used for the summary together. 
[ 20240518-14:12:14 ] [summary] True 5 False 
[ 20240518-14:12:14 ] [lsa] result:  24 
 
[ 20240518-14:12:14 ] [lsa preprocess]  The summary result has revealed very interesDng findings of genes that are associated to some Cancerous and type 2 diabetes diseases (see Table 
[ 20240518-14:12:18 ] [model] Done loading inference: 3.1400306224823 
[ 20240518-14:12:18 ] [model] summary result:  genes that are associated to some Cancerous and type 2 diabetes diseases (see Table) The summary result has revealed very interesDng findings. 
[ 20240518-14:12:18 ] [summary] True 5 False 
[ 20240518-14:12:18 ] [lsa] result:  24 
 
[ 20240518-14:12:18 ] [lsa preprocess]  ROUGE is a metric evaluaDon model which stands for Recall Oriented Understudy for Gisting Evaluation.It is an intrinsic metric for automaDcally evaluaDng document summaries 
[ 20240518-14:12:21 ] [model] Done loading inference: 3.778247833251953 
[ 20240518-14:12:21 ] [model] summary result: ROUGE is a metric evaluaDon model which stands for Recall Oriented Understudy for Gisting Evaluation. It is an intrinsic metric for automaDcally evaluaDng document summaries. 
[ 20240518-14:12:21 ] [summary] True 5 False 
[ 20240518-14:12:21 ] [lsa] result:  65 
 
[ 20240518-14:12:21 ] [lsa preprocess]  System and Human Annotated Summaries Type Summary SSummary 'Some of the genes in the BCAA metabolic pathway such as MLYCD (rank 164)HADHB (rank 354)IVD (rank 713)MUT (rank 921)and PCCB (rank 684) are also ranked highly by Hridaya.The SVMs are based on 181 features broadly grouped into (1) gene8c( 
[ 20240518-14:12:26 ] [model] Done loading inference: 4.837074041366577 
[ 20240518-14:12:26 ] [model] summary result: Some of the genes in the BCAA metabolic pathway such as MLYCD (rank 164)HADHB (rank 354)IVD (rank 713)MUT (rank 921)and PCCB (rank 684) are also ranked highly. 
[ 20240518-14:12:26 ] [summary] True 5 False 
[ 20240518-14:12:26 ] [model] summary result:  
[ 20240518-14:12:26 ] [summary] True 5 False 
[ 20240518-14:12:26 ] [lsa] result:  265 
 
[ 20240518-14:12:26 ] [lsa preprocess]  We have mulDple processed arDcles or documents extracted from the web based on key search terms.We then produced a set of human annotated reference summaries of the CleanHTML.txt document.In considering the individual words in a sentence we simply represent this with the formula in equaDon 8.The metric will produce a perfect result of 1 which usually will be the case if indeed the sentence matches.This metric simply means all the words in the reference summary has been captured by the system summary.In the system generated summary, which someDmes might be very large based on the threshold selected, capturing all the words in the reference or model summary.However, most of the worlds in the system summary might be unnecessary verbose.Therefore, our precision becomes crucial as we are trying to predict generated summaries that should be concise in nature.In this study, we combined and computerised both the Precision and Recall and further report the F1 -score measure.In order to ascertain the validity of the study, we measured ROUGE-N, ROUGE -S and ROUGE -L which are the granularity of texts that was compared between the system summaries and the reference or human annotated model summaries.We computed precision and recall scores of the ROUGE -2.The main reason why ROUGE-1 could be considered over others or in conjuncDon with ROUGE -2 or even other fine granularity measures is because it reveals the fluency of the summaries or if used in a translaDon task.There are few overlapping bigrams outcome as we are not always or directly re-using the whole sentences for the summarizaDon. 
[ 20240518-14:12:31 ] [model] Done loading inference: 4.439237356185913 
[ 20240518-14:12:31 ] [model] summary result: In considering the individual words in a sentence we simply represent this with the formula in equaDon 8. The metric will produce a perfect result of 1 which usually will be the case if indeed the sentence matches. 
[ 20240518-14:12:31 ] [summary] True 5 False 
[ 20240518-14:12:31 ] [lsa] result:  67 
 
[ 20240518-14:12:31 ] [lsa preprocess]  The terms in the topic modelling show text which are mostly frequent in the document these were depicted by the size of the circle (as seen in Figures The distance between two or more topics is an approximaDon of their semanDc relaDonship.Note that close topics such as topics 1, 2 and 3 are semanDcally related which describes the terms in the topics.As observed in Figures 
[ 20240518-14:12:36 ] [model] Done loading inference: 4.839792966842651 
[ 20240518-14:12:36 ] [model] summary result: The terms in the topic modelling show text which are mostly frequent in the document. These were depicted by the size of the circle (as seen in Figures) Distance between two or more topics is an approximaDon of their semanDc relaDonship. 
[ 20240518-14:12:36 ] [summary] True 5 False 
[ 20240518-14:12:36 ] [lsa] result:  43 
 
[ 20240518-14:12:36 ] [lsa preprocess]  In this study, we presented a fully data-driven approach for automaDc text summarizaDon.We proposed and evaluated the model on unstructured datasets which show some results comparable to the current state-of-the-art topic modelling techniques without depending on modificaDons using any linguisDc informaDon models 
[ 20240518-14:12:40 ] [model] Done loading inference: 4.380962610244751 
[ 20240518-14:12:40 ] [model] summary result: We presented a fully data-driven approach for automaDc text summarizaDon. We proposed and evaluated the model on unstructured datasets which show some results comparable to the current state-of-the-art topic modelling techniques. 
[ 20240518-14:12:40 ] [summary] True 5 False 
[ 20240518-14:12:40 ] [lsa] result:  30 
 
[ 20240518-14:12:40 ] [lsa preprocess]  AutomaDc summarizaDon is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document 
[ 20240518-14:12:43 ] [model] Done loading inference: 3.408020496368408 
[ 20240518-14:12:43 ] [model] summary result: AutomaDc summarizaDon is the process of reducing a text document with a computer program. The goal is to create a summary that retains the most important points of the original document. 
[ 20240518-14:12:43 ] [summary] True 5 False 
[ 20240518-14:12:43 ] [lsa] result:  393 
 
[ 20240518-14:12:43 ] [lsa preprocess]  Before the advent of generative AI, all text and artwork was produced by humans, in some cases aided by tools or computer systems.The capability of large language models (LLMs) to generate text with near-zero human effort, however, along with models to generate images, audio, and video, suggest that the data to which humans are exposed may come to be dominated by AI-generated or AI-aided processes.Researchers have noted that the recursive training of AI models on synthetic text may lead to degeneration, known as "model collapse" The initial effect of AI-generated information is presumably limited, and existing work on the harms of AI rightly focuses on the immediate effects of false information spread by "deepfakes" Researchers and engineers are currently building a variety of systems whereby AI would mediate our experience with other humans and with information sources.These range from learning from LLMs Over time, dependence on these systems, and the existence of multifaceted interactions among them, may create a "curse of recursion" Such a process might be reinforced by an 'echo chamber' or information cascade effect, in which repeated exposure to this restricted set of information leads individuals to believe that the neglected, unobserved tails of knowledge are of little value.To the extent AI can radically discount the cost of access to certain kinds of information, it may further generate harm through the "streetlight effect", in which a disproportionate amount of search is done under the lighted area not because it is more likely to contain one's keys but because it's easier to look there.We argue that the resulting curtailment of the tails of human knowledge would have significant effects on a range of concerns, including fairness, inclusion of diversity, lost-gains in innovation, and the preservation of the heritage of human culture.In our simulation model, however, we also consider the possibility that humans are strategic in actively curating their information sources.If, as we argue, there is significant value in the tai' areas of knowledge that come to be neglected by AI-generated content, some individuals may put in additional effort to realize the gains, assuming they are sufficiently informed about the potential value. 
[ 20240518-14:12:50 ] [model] Done loading inference: 6.167253255844116 
[ 20240518-14:12:50 ] [model] summary result: Researchers have noted that the recursive training of AI models on synthetic text may lead to degeneration, known as "model collapse" The resulting curtailment of the tails of human knowledge would have significant effects on a range of concerns, including fairness, inclusion of diversity and lost-gains in innovation. 
[ 20240518-14:12:50 ] [summary] True 5 False 
[ 20240518-14:12:50 ] [lsa] result:  104 
 
[ 20240518-14:12:50 ] [lsa preprocess]  We identify a dynamic whereby AI, despite only reducing the cost of access to certain kinds of information, may lead to "knowledge collapse," neglecting the long-tails of knowledge and creating an degenerately narrow perspective over generations.We provide a positive knowledge spillovers model with in which individuals decide whether to rely on cheaper AI technology or invest in samples from the full distribution of true knowledge.We examine through simulations the conditions under which individuals are sufficiently informed to prevent knowledge collapse within society.Finally, we conclude with an overview of possible solutions to prevent knowledge collapse in the AI-era. 
[ 20240518-14:12:57 ] [model] Done loading inference: 6.88706636428833 
[ 20240518-14:12:57 ] [model] summary result: We identify a dynamic whereby AI, despite only reducing the cost of access to certain kinds of information, may lead to "knowledge collapse" We provide a positive knowledge spillovers model with in which individuals decide whether to rely on cheaper AI technology or invest in samples from the full distribution of true knowledge. We conclude with an overview of possible solutions to prevent knowledge collapse in the AI-era. 
[ 20240518-14:12:57 ] [summary] True 5 False 
[ 20240518-14:12:57 ] [lsa] result:  209 
 
[ 20240518-14:12:57 ] [lsa preprocess]  Technology has long affected how we access knowledge, raising concerns about its impact on the transmission and creation of knowledge.Yeh Meng-te, for example, argued in the twelfth century that the rise of books led to a decline in the practice of memorizing and collating texts that contributed to a decline of scholarship and the repetition of errors We focus on recent work on the role of digital platforms and social interactions, and mention only in passing the literature on historical innovations and knowledge (e.g.The following section considers research on the impact of recommendation algorithms and self-selection on social media, and how this might generate distorted and polarizing opinions, as an analogy for understanding the transformation brought about by reliance on AI.We consider game theoretic models of information cascades as an alternative model for failure in social learning, in which the public to fails to update rationally on individuals' private signals.Next, we review the main findings of network analysis on the flow of information in social media, which also identify mechanisms which distort knowledge formation.We then examine the specific nature of generative AI algorithms, focusing on the problem of model collapse and known biases in AI outputs. 
[ 20240518-14:13:01 ] [model] Done loading inference: 4.827677011489868 
[ 20240518-14:13:01 ] [model] summary result: Technology has long affected how we access knowledge, raising concerns about its impact on the transmission and creation of knowledge. We focus on recent work on the role of digital platforms and social interactions, and mention only in passing the literature on historical innovations and knowledge. 
[ 20240518-14:13:01 ] [summary] True 5 False 
[ 20240518-14:13:01 ] [lsa] result:  349 
 
[ 20240518-14:13:01 ] [lsa preprocess]  A common critique of social media is that they allow users to select in to "echo chambers" (specific communities or communication practices) in which they are exposed to only a narrow range of topics or perspectives.For example, instead of consulting the "mainstream" news where a centrist and relatively balanced perspective is provided, users are exposed to selective content that echoes pre-existing beliefs.In the ideological version of the echo-chamber hypothesis, individuals within a latent ideological space (for example a one-dimensional left-right spectrum), are exposed to peers and content with ideologically-similar views.If so, their beliefs are reinforced socially and by a generalization from their bounded observations, leading to political polarization A simple model for this assumes homophily within in a network growth model, in which similar individuals chose to interact.Implicitly the approach presumes that this is common on social media but not common within traditional media, which for technological reasons were constrained to provide the same content across a broad population with possibly heterogeneous preferences.1 This general dynamic may hold even if traditional media and newspapers were themselves dynamic systems interacting with their consumers, markets and advertisers, and themselves adapting their message to specific communities and preferences (e.g.The second main line of analysis focuses on "filter bubbles," whereby the content to which users are exposed is selected based on a recommendation system.ommendations their relation to polarization Particularly relevant for our context is the issue of "popularity bias" in recommender systems, in which a small subset of content receives wide exposure while users (distributed based on some long-tailed distribution, like the topics) from smaller groups or with rare preferences are marginalized.But overly favoring popular items can lead to user disengagement because it neglects their unique interests, lacks variety, etc.The problem of popularity bias is ironic given that one of the unique contributions of the internet was its ability to provide access to long-tailed products and services that were previously ignored or inaccessible 
[ 20240518-14:13:09 ] [model] Done loading inference: 8.01076602935791 
[ 20240518-14:13:09 ] [model] summary result: In the ideological version of the echo-chamber hypothesis, individuals within a latent ideological space are exposed to peers and content with ideologically-similar views. If so, their beliefs are reinforced socially and by a generalization from their bounded observations, leading to political polarization. The second main line of analysis focuses on "filter bubbles," whereby the content to which users are exposed is selected based on a recommendation system. 
[ 20240518-14:13:09 ] [summary] True 5 False 
[ 20240518-14:13:09 ] [lsa] result:  255 
 
[ 20240518-14:13:09 ] [lsa preprocess]  Information cascade models provide one approach to explaining a kind of herd behavior (where diverse and free individuals nonetheless make similar decisions).They explore the conditions under which private information is not efficiently aggregated by the public.This can occur where individuals sequentially make decisions from a discrete set after observing the behaviors but not the private signals of others.This can generate a "herd externality" A related literature on the spread of information on social networks analyzes information cascades in terms of network structure, as a kind of contagion.Here, the focus is not on private information but how information flows within the network.For example, independent cascade models consider how an individual may change their beliefs based on some diffusion probability as a result of contact with a neighbor with that belief More generally, such models determine the probability of diffusion within a network as some function of the connected nodes, and may also incorporate additional characteristics such as each nodes' social influence, ideological or other preferences, or topics These models suggest specific opinion-formation dynamics based on what other humans, texts, images, etc.an individual interacts with.By extension, we could consider the generalization of these networks to the case where LLMs play a key role as (possibly influential) nodes, or as determining how an individual navigates a knowledge graph.One of the key ideas of Web 2.0 was that users, not just authors or programmers, structure the knowledge 
[ 20240518-14:13:14 ] [model] Done loading inference: 4.477827548980713 
[ 20240518-14:13:14 ] [model] summary result: Information cascade models explore the conditions under which private information is not efficiently aggregated by the public. These models suggest specific opinion-formation dynamics based on what other humans, texts, images, etc.an individual interacts with. 
[ 20240518-14:13:14 ] [summary] True 5 False 
[ 20240518-14:13:14 ] [lsa] result:  101 
 
[ 20240518-14:13:14 ] [lsa preprocess]  The idea of model collapse is rooted in the earlier phenomenon of "mode collapse" in generative adversarial networks (GANs).GANs are based on a generator neural network that proposes, e.g.an image, and a discriminator attempts to predict whether a given image is created by the generator or is a real image from the dataset.While ideally the generator attempts to produce images across the full range of input data, in practice they may settle into producing a narrow range of images for which it is good at fooling the discriminator, known as mode collapse 
[ 20240518-14:13:20 ] [model] Done loading inference: 6.058420181274414 
[ 20240518-14:13:20 ] [model] summary result: The idea of model collapse is rooted in the earlier phenomenon of "mode collapse" in generative adversarial networks.GANs are based on a generator neural network that proposes, e.g.an image, and a discriminator attempts to predict whether a given image is created by the generator or is a real image from the dataset. 
[ 20240518-14:13:20 ] [summary] True 5 False 
[ 20240518-14:13:20 ] [lsa] result:  88 
 
[ 20240518-14:13:20 ] [lsa preprocess]  Newer AI models such as LLMs are not immune to the problems of bias identified and measured in machine learning algorithms Recent work attempts to address these issues through a variety of methods, for example by upsampling underrepresented features on which prediction is otherwise sub-optimal One particular area in which the diversity of LLM outputs has been analyzed is on a token-by-token level in the context of decoding strategies.In some situations, using beam search to choose the most likely next token can create degenerate repetitive phrases 
[ 20240518-14:13:24 ] [model] Done loading inference: 3.9940109252929688 
[ 20240518-14:13:24 ] [model] summary result: Newer AI models such as LLMs are not immune to the problems of bias identified and measured in machine learning algorithms. Recent work attempts to address these issues through a variety of methods. 
[ 20240518-14:13:24 ] [summary] True 5 False 
[ 20240518-14:13:24 ] [lsa] result:  266 
 
[ 20240518-14:13:24 ] [lsa preprocess]  A commonly held, optimistic view is that knowledge has improved monotonically over time, and will continue to do so.This indeed appears to be the case for certain scientific fields like physics, chemistry, or molecular biology, where we can measure the quality of predictions made over time.For example, accuracy in the computation of digits of π has increased from 1 digit in 200 BCE to 16 in 1424 (Jamashid al-Kashi) to 10 14 digits recently.In other domains, however, it is less clear, especially within regions.Historically, knowledge has not progressed monotonically, as evidenced by the fall of the Western Roman empire, the destruction of the House of Wisdom in Baghdad and subsequent decline of the Abbasid Empire after 1258, or the collapse of the Mayan civilization in the 8th or 9th century.Or, to cite specific examples, the ancient Romans had a recipe for concrete that was subsequently lost, and despite progress we have not yet re-discovered the secrets of its durability The distribution of knowledge across individuals also varies over time.For example, traditional huntergatherers could identify thousands of different plants and knew their medicinal usages, whereas most humans today only know a few dozen plants and whether they can be purchased in a grocery store.This could be seen as a more efficient form of specialization of information across individuals, but it might also impact our beliefs about the value of those species or of a walk through a forest, or influence scientific or policy-relevant judgements.Informally, 
[ 20240518-14:13:30 ] [model] Done loading inference: 5.75330924987793 
[ 20240518-14:13:30 ] [model] summary result: A commonly held, optimistic view is that knowledge has improved monotonically over time, and will continue to do so. This appears to be the case for certain scientific fields like physics, chemistry, or molecular biology. In other domains, however, it is less clear, especially within regions. 
[ 20240518-14:13:30 ] [summary] True 5 False 
[ 20240518-14:13:30 ] [lsa] result:  380 
 
[ 20240518-14:13:30 ] [lsa preprocess]  Thus, a key dynamic of the model is to allow for the possibility that rational agents may be able to prevent or to correct for distortion from over-dependence on 'centrist' information.To the extent that they observe this, individuals would be willing to pay more (put in more labor) to profit from these additional gains.text sum-marization, develops an intuitive sense of when the AI provides the main idea sufficiently well for a given purpose and when it is worth going straight to the source.We assume that individuals cannot foresee the future, but they do observe in common the realized rewards from previous rounds.Depending on how their utility is calculated (not a substantive focus here), these could be interpreted as different expected returns from innovation (e.g.technooptimists versus pessimists), or their relative ability or desire to engage in innovation.We model knowledge as a process of approximating a (Students t) probability distribution.The set of individuals who decide to invest in information receive a sample from the true distribution, while those that invest in the AI-generated sample receive a sample from a version of the true distribution which is truncated at σ tr standard deviations above and below the mean.To vary the extent of mass in the tails, we model the true distribution as a Student's t-distribution with e.g.The results are similar for a standard normal distribution, and as expected the problem of knowledge collapse is more pronounced for wider tails (c.f.That is, a public knowledge probability distribution function ('public pdf') is generated by gathering the nsamp = 100 most recent samples 4 and generating an estimate of the truth using kernel density estimation.This reflects that public consciousness is overwhelmed with knowledge claims and cannot evaluate each, so that a consensus is formed around the sum of all voices.Unlike the individual innovator who has a narrow focus and observes whether her patent ultimately generates value, the public sphere has limited attention and is forced to accept the aggregate contributions of the marketplace of ideas.As a result, individuals' investments in innovation have positive spillovers to the extent they can move public knowledge towards the truth. 
[ 20240518-14:13:36 ] [model] Done loading inference: 6.2341697216033936 
[ 20240518-14:13:36 ] [model] summary result: We model knowledge as a process of approximating a (Students t) probability distribution. We assume that individuals cannot foresee the future, but they do observe in common the realized rewards from previous rounds. Individuals' investments in innovation have positive spillovers to the extent they can move public knowledge towards the truth. 
[ 20240518-14:13:36 ] [summary] True 5 False 
[ 20240518-14:13:36 ] [lsa] result:  218 
 
[ 20240518-14:13:36 ] [lsa preprocess]  Our main concern is with the view that AI, by reducing the costs of access to certain kinds of information, could only make us better off.In contrast to the literature on model collapse, we consider the conditions under which strategic humans may seek out the input data that will maintain the full distribution of knowledge.Thus, we begin with a consideration of different discount rates.First, we present the a kernel density estimate of public knowledge at the end of 100 rounds (Figure 8 For example, Christian communities at times actively promoted and preserved 'canonical' texts while neglecting or banning others, with the result that those excluded from reproduction by scribes were taken to have little value.Perhaps the heliocentric view espoused by Aristarchus of Samos in the 3rd century BCE would have been more readily (re)considered if his works had not been neglected For subsequent results illustrating the tradeoff of different parameters, we plot the Hellinger distance between public knowledge at the end of the 100 rounds and the true distribution.First, we examine the importance of updating on the value of relative samples and the relationship to the discount factor in Figure In Figure We compare the effect of the generational compounding of errors in Figure 
[ 20240518-14:13:42 ] [model] Done loading inference: 5.759214639663696 
[ 20240518-14:13:42 ] [model] summary result: In contrast to the literature on model collapse, we consider the conditions under which strategic humans may seek out the input data that will maintain the full distribution of knowledge. For example, Christian communities at times actively promoted and preserved 'canonical' texts while neglecting or banning others. 
[ 20240518-14:13:42 ] [summary] True 5 False 
[ 20240518-14:13:42 ] [lsa] result:  411 
 
[ 20240518-14:13:42 ] [lsa preprocess]  We provide a theoretical framework for defining "knowledge collapse", whereby dependence on generative AI such as large language models may lead to a reduction in the long-tails of knowledge.Our simulation study suggests that such harm can be mitigated to the extent that (a) we are aware of the of the possible value of niche, specialized and eccentric perspectives that may be neglected by AI-generated data and continue to seek them out, (b) AI-systems are not recursively interdependent, as occurs if they use other AI-generated content as inputs or suffer from other generational effects, and (c) AI-generated content is as representative as possible of the full distribution of knowledge.First, while our work does not justify an outright ban, measures should be put in place to ensure safeguards against widespread or complete reliance on AI models.For every hundred people who read a one-paragraph summary of a book, there should be a human somewhere who takes the time to sit down and read it, in hopes that she can then provide feedback One extension to the model would be to allow for generational change but endogenize the choice of public subsidies to protect 'tail' knowledge.This is arguably what is done by governments that support academic and artistic endeavors that would otherwise have been underprovided by the private market.Protecting the diversity of information means also paying attention to the effect of AI adoption on the revenue streams of journalists that produce and not merely transmit information (e.g.Finally, while much recent attention has been on the problem of LLMs misleadingly presenting fiction as fact (hallucination), this may be less of an issue than the problem of representativeness across a distribution of possible responses.Hallucination of verifiable, concrete facts is often easy to correct for.Yet many real world questions do not have well-defined, verifiably true and false answers.and a LLM answers "monetary policy", the problem isn't one of hallucination, but of the failure to reflect the full-distribution of possible answers to the question, or at least provide an overview of the main schools of economic thought.This could be considered in the setup of frameworks for reinforcement learning from human feedback and related approaches to shaping model outputs, since humans may by default prefer simple, monolithic answers over those that represent the diversity of perspectives. 
[ 20240518-14:13:48 ] [model] Done loading inference: 6.150897979736328 
[ 20240518-14:13:48 ] [model] summary result: We provide a theoretical framework for defining "knowledge collapse", whereby dependence on generative AI may lead to a reduction in the long-tails of knowledge. While our work does not justify an outright ban, measures should be put in place to ensure safeguards against widespread or complete reliance on AI models. 
[ 20240518-14:13:48 ] [summary] True 5 False 
[ 20240518-14:13:48 ] [lsa] result:  51 
 
[ 20240518-14:13:48 ] [lsa preprocess]  As mentioned above, the reported results used a tdistribution with 10 degrees of freedom, which has slightly wider tails than a standard normal distribution.We can compare the results with a standard normal distribution (i.e.a t-distribution as the degrees of freedom becomes large) or with wider tails.In Figure 
[ 20240518-14:13:54 ] [model] Done loading inference: 5.466852426528931 
[ 20240518-14:13:54 ] [model] summary result: We can compare the results with a standard normal distribution (i.e. a t-distribution as the degrees of freedom becomes large) or with wider tails. As mentioned above, the reported results used a tdistribution with 10 degrees offreedom, which has slightly wider tails than a standardnormal distribution. 
[ 20240518-14:13:54 ] [summary] True 5 False 
[ 20240518-14:13:54 ] [lsa] result:  440 
 
[ 20240518-14:13:54 ] [lsa preprocess]  To define knowledge collapse we need to distinguish between a few conceptual sets of 'knowledge', whether or not these are empirically observable.In the example cited in the main section, the ancient Roman recipe for concrete is part of broad historical knowledge but not part of available current knowledge.Technological innovations from the printing press to the internet to AI mediate human interactions and human's exposure to historical and current sources of knowledge.For example, the digitization of archives might make obscure sources available to a wider audience and thus increase the amount of 'broad historical knowledge' that is part of the 'available current knowledge.This we call 'human memory knowledge' or 'human working knowledge' by reference to human working memory.For example, consider the problem of listing all the animals that have ever existed on earth.There might be some that humans previously knew about, but which subsequently went extinct and which do not exist anywhere among the scientific literature or individuals currently living on earth.A linguist trying to evaluate or create possible linguistic theories implicitly bases their judgement on the known language families and their structures, and so on.Edison and his team famously tried thousands of different filament materials, but if it bamboo had not been among the materials that came to mind as they searched alternatives, a practical electric bulb may have been invented only later.Finally, it is useful to define the 'epistemic horizon' as the set of knowledge that a community of humans considers practically possible to know and worth knowing.13 A common controversy in the public imagination is whether traditional medicines are worth consideration when searching for medical cures.Such traditional medicines might be outside of the epistemic horizon because they are not written down in the scientific literature, are only known by individuals speaking lesser known languages, or because the scientists in question consider them too costly to acquire or unlikely to be beneficial.One way to think about this relationship is as a generalization of 'availability bias', in which we take the set of readily recalled information to be more likely, important, or relevant In these terms, we define 'knowledge collapse' as the progressive narrowing over time (or over technological representations) of the set of human working knowledge and the current human epistemic horizon relative to the set of broad historical knowledge.On a theoretical level, the idea of epistemic horizon has an intellectual heritage in Immanuel Kant's argument about the forms and categories of understanding that underly the possibility of knowledge 14 e.g. 
[ 20240518-14:14:00 ] [model] Done loading inference: 6.129032135009766 
[ 20240518-14:14:00 ] [model] summary result: In the example cited in the main section, the ancient Roman recipe for concrete is part of broad historical knowledge but not part of available current knowledge.Technological innovations from the printing press to the internet to AI mediate human interactions and human's exposure to historical and current sources of knowledge. 
[ 20240518-14:14:00 ] [summary] True 5 False 
[ 20240518-14:14:00 ] [lsa] result:  271 
 
[ 20240518-14:14:00 ] [lsa preprocess]  Understanding how individuals organize into social communities is of interest to various research fields due their ubiquitous presence in social systems.Social dilemmas embody the conflict between social and individual interests, often framed as a choice one has to make between cooperating and defecting, the dynamics of which have been extensively modeled using evolutionary game theory.Incorporating community structure into these models has thus far entailed considering events of two different natures: within-community reproduction and between-community migration.These models are typically referred to as metapopulation dynamics, a classification of which has been performed in Furthermore, metapopulation models generally assume that communities are connected to each other in the same way, with few exceptions to this The framework introduced in its general form in We propose the use of this fully independent movement model to study evolutionary dynamics in networkand community-structured populations with multiplayer interactions.Our focus centres on the limit of high home fidelity, where communities exhibit asymptotically low mobility.In section 3.1, we derive general analytical methods for the dynamics in this limit.Some dynamics amplify within-community selection and others increment between-community events.In section 3.2, we show that the balance between the two types of events determines whether cooperation evolves, and we obtain their contributions to fixation probabilities under weak selection for several social dilemmas.In section 3.3, we use this balance to derive the rules of multiplayer cooperation and compare them amongst social dilemmas.Once again showcasing its versatility, this framework enabled the exploration of network and community structure, thereby revealing the high potential for the evolution of cooperation across diverse social dilemmas. 
[ 20240518-14:14:05 ] [model] Done loading inference: 5.3164567947387695 
[ 20240518-14:14:05 ] [model] summary result: Social dilemmas embody the conflict between social and individual interests. The dynamics of these have been extensively modeled using evolutionary game theory. We propose the use of this fully independent movement model to study evolutionary dynamics in networkand community-structured populations with multiplayer interactions. 
[ 20240518-14:14:05 ] [summary] True 5 False 
[ 20240518-14:14:05 ] [lsa] result:  466 
 
[ 20240518-14:14:05 ] [lsa preprocess]  Even though the terms graph and network are often used interchangeably in the literature, here we follow the same terminology used in Under fully independent movement models, the position of each individual is independent both of where they were previously and of where other individuals will be where h is the home fidelity parameter, and d n is the degree of the home node of individual I n .We use the version of the territorial raider model under which each node of the network is home to a community of Q individuals, and thus M denotes the number of communities and N = M Q.The probability distribution of positions under the territorial raider model is represented in figure The fitness of each individual I n is obtained through the weighted average of the payoffs R n,m,G received in each place P m and each group composition G they can be in.We further introduce w, the intensity of selection as defined in We bring attention to an alternative notation used in the literature, where a background payoff defined as R is introduced.This notation is used under movement models such as those from This leads to the following adjustments to the fitness of individuals:We will make use of the first notation where the intensity of selection is used, as this is revealed more practical when inspecting the weak selection limit.Nonetheless, the second approach leads to a simple rescaling of the fitness F ′ = 1 w F when R = 1-w w , which has no impact on the evolutionary dynamics introduced later.We consider the multiplayer social dilemmas studied in ) when the focal individual I n is a cooperator (defector), as they are determined by the type of the focal individual and the number of cooperators c, and defectors d in their current group.We present the payoffs received under each social dilemma in table We follow an approach grounded on evolutionary graph theory Let us consider that the population goes through an evolutionary process operating on the strategies C and D used by each individual.This is modelled in discrete evolutionary steps, during which individuals may update their strategies.The probability that, at a given step, the strategy of an individual I i replaces that of I j is denoted by the replacement probability τ ij .We recall the dynamics outlined in Evolutionary dynamics and replacement probabilitiesTable In instances where the replacement probability is not explicitly stated, it can be derived by multiplying the respective birth and death probabilities.The probability of fixation for a single mutant cooperator (defector) in a population with the opposing strategy is defined as ρ C (ρ D ). 
[ 20240518-14:14:13 ] [model] Done loading inference: 7.947194814682007 
[ 20240518-14:14:13 ] [model] summary result: The fitness of each individual I n is obtained through the weighted average of the payoffs R n,m,G received in each place P m and each group composition G they can be in. The population goes through an evolutionary process operating on the strategies C and D used by each individual. This is modelled in discrete evolutionary steps, during which individuals may update their strategies. 
[ 20240518-14:14:13 ] [summary] True 5 False 
[ 20240518-14:14:14 ] [lsa] result:  380 
 
[ 20240518-14:14:14 ] [lsa preprocess]  In section 3.1, we describe the evolutionary process arising from this limit across the six introduced dynamics and derive exact expressions for single mutant fixation probabilities under any network of communities.Moreover, we contextualize the particular case of the CPD with respect to prior literature on pairwise dilemmas in section 3.4.Consider a connected network comprising M places and an arbitrary topology.Each place is home to a community of size Q with movement following the territorial raider model (see figure In this limit, it is possible to obtain a closed-form expression for the fixation probability of a single mutant.Given the general nature of equations 12 and 13, they can be used to assess the viability of cooperation under social dilemmas in any network of communities.The successful fixation of a strategy is determined by its relative success in introducing itself in other communities and successfully fixating there.The balance between these two factors is present at each step of the higher level (community) fixation process, as it is represented in figure This condition is more easily met when the size of the network is increased.In the limiting case where there are only two communities (M = 2), this last term exhibits a finite network correction coefficient three times larger than that of the within-community fixation of residents.This is so because the fixation of the original mutant in its community takes an increased importance in the overall process.Increasing the size of communities decreases the impact of between-community contributions under both dynamics.It is clear that in this case, high rewards are detrimental to the evolution of cooperation.We note that the critical value of the reward-to-cost ratio under public goods dilemmas always increases with the size of communities and regardless of the used evolutionary dynamics.This reinforces the conclusion that populations organized into large networks of small communities lead to a larger region of the parameter space under which cooperation evolves.These results can be explained by the fact that these dynamics when compared to the remaining, amplify the impact of between-community replacement terms (where cooperators succeed relative to defectors), and suppress within-community selection terms (where defectors succeed).The CPD is a particular game of interest among public goods dilemmas. 
[ 20240518-14:14:20 ] [model] Done loading inference: 5.912218332290649 
[ 20240518-14:14:20 ] [model] summary result: Consider a connected network comprising M places and an arbitrary topology. Each place is home to a community of size Q with movement following the territorial raider model. In this limit, it is possible to obtain a closed-form expression for the fixation probability of a single mutant. 
[ 20240518-14:14:20 ] [summary] True 5 False 
[ 20240518-14:14:20 ] [lsa] result:  471 
 
[ 20240518-14:14:20 ] [lsa preprocess]  It was observed in Public goods dilemmas consistently lead to the evolution of cooperation down to lower values of the rewardto-cost ratio when a larger number of smaller communities is considered.This is in line with what is observed in alternative community and deme models The results presented in this paper were obtained within the limit of high home fidelity, under which communities become asymptotically bounded interacting groups.This probability is equal to the sum of the probabilities of all the paths that alter the number of mutants in that community from 1 to Q without passing by 0.This highlights the fact that they occur at a different time-scale from within-community fixation processes:Therefore, the cooperator within-community fixation probability in the limit h → ∞ can be represented as the following:Similar to this, we can obtain the same equation for the within-community fixation probability of a single defector by using the following expression:We denote r C and r D as the zeroth-order terms of the equations above, which are presented in equations 8 and 8 of the main text.We call ρ C the probability that a single mutant cooperator will fixate in a population of defectors.The replacement weights w ij between individuals with homes in different places P i and P j are independent of their two strategies, and they are multiplied by Q to account for all the defectors present in each of the communities in M \I.The within-community fixation probability is perturbed by higher-order terms in h -1 already analysed when its expression was obtained.The sum of fractions above includes a simplification coming from the fact that the denominator is a sum over all products of weights and fitness according to the definition from table 2, which in the limit h → ∞ simply tends to the fitness of communal residents f D 0,Q plus higher-order terms in h -1 .This will introduce another key difference in the results.The probability ratio Γ DBB/BDD is independent of I and its size, as it was under the remaining dynamics:Therefore the resulting process under high home fidelity in these two dynamics is parallel to the one occurring under the remaining four dynamics, with two quantitative differences: within-community fixation probabilities have correction coefficients as represented in equations 51 and 52, and the overall population process has an altered equivalent fitness characterized in equation 57.The resulting fixation probabilities are therefore the following:when Γ DBB/BDD ̸ = 1.This is concluded from the rules of multiplayer cooperation (section 3.3), obtained under weak selection and a large number of communities.Larger numbers of communities were proven C.1 to decrease the values of (V /K) c above which cooperation evolves under public goods games. 
[ 20240518-14:14:27 ] [model] Done loading inference: 7.07257604598999 
[ 20240518-14:14:27 ] [model] summary result: Public goods dilemmas consistently lead to the evolution of cooperation down to lower values of the rewardto-cost ratio. This is in line with what is observed in alternative community and deme models. Results were obtained within the limit of high home fidelity, under which communities become asymptotically bounded interacting groups. 
[ 20240518-14:14:27 ] [summary] True 5 False 
[ 20240518-14:14:27 ] [lsa] result:  298 
 
[ 20240518-14:14:27 ] [lsa preprocess]  Not all problems require the same amount of time or effort to solve.Analogously, in language modeling not all tokens and sequences require the same time or effort to accurately make a prediction.And yet, transformer models expend the same amount of compute per token in a forward pass.In our implementation total compute is user defined and unchanging prior to training, rather than being a function of the network's on-the-fly decisions.As we will show, these gains can be had without sacrificing overall performance.We leverage an approach akin to Mixture of Experts (MoE) transformers, in which dynamic token-level routing decisions are made across the network depth.Departing from MoE, we choose to either apply a computation to a token (as would be the case for a standard transformer), or pass it through a residual connection (remaining unchanged and saving compute).Also in contrast to MoE, we apply this routing to both forward MLPs and multi-head attention.We refer to this strategy as Mixture-of-Depths (MoD) to emphasize how individual tokens pass through different numbers of layers, or blocks, through the depth of the transformer (see figure The MoD technique also allows one to trade-off performance with speed.On the one hand, one can train an MoD transformer that improves upon vanilla transformers by as much as 1.5% on the final log probability training objective for equivalent training FLOPs (isoFLOP), and while taking an equivalent amount of wall-clock time to train.Together, these results imply that MoD transformers learn to route intelligently (i.e., skipping computations that are unnecessary) since they can achieve equal or better log probabilities per sequence despite a smaller FLOP footprint per forward pass. 
[ 20240518-14:14:34 ] [model] Done loading inference: 6.354465007781982 
[ 20240518-14:14:34 ] [model] summary result: We leverage an approach akin to Mixture of Experts (MoE) transformers, in which dynamic token-level routing decisions are made across the network depth. In our implementation total compute is user defined and unchanging prior to training, rather than being a function of the network's on-the-fly decisions. 
[ 20240518-14:14:34 ] [summary] True 5 False 
[ 20240518-14:14:34 ] [lsa] result:  151 
 
[ 20240518-14:14:34 ] [lsa preprocess]  The transformer architecture has become the workhorse of a revolution in practical artificial intelligence, bringing unprecedented capabilities at the cost of expensive training runs and serving procedures.This has spurred tremendous interest in making transformer architectures more efficient A wide variety of recent work has developed conditional computation methods for transformers.Some of this work focuses on "early exiting", that is, learning to decide when to end computation on a given token, allowing the token to skip any remaining transformer layers after the exit decision is made Other work has developed methods for iterating transformer layers with shared weights for an adaptive number of steps CoLT5 MoD focuses on the decoder-only setting, and so we propose a predictive router to enable efficient inference for conditional computation in transformers.One successful formulation of conditional computation is the the "mixture-of-experts" layer (MoE) as introduced by 
[ 20240518-14:14:38 ] [model] Done loading inference: 4.2551655769348145 
[ 20240518-14:14:38 ] [model] summary result: The transformer architecture has become the workhorse of a revolution in practical artificial intelligence, bringing unprecedented capabilities at the cost of expensive training runs and serving procedures. A wide variety of recent work has developed conditional computation methods for transformers. 
[ 20240518-14:14:38 ] [summary] True 5 False 
[ 20240518-14:14:38 ] [lsa] result:  413 
 
[ 20240518-14:14:38 ] [lsa preprocess]  Our high-level strategy is as follows:• Set a static compute budget that is less than that of an equivalent vanilla transformer by limiting the number of tokens in a sequence that can participate in a block's computations (i.e., selfattention and subsequent MLP).Since precisely 𝑘 tokens will participate in the block's computations, the computation graph and tensor sizes remain static throughout training; it is merely the tokens' participation that is dynamic and context-sensitive, as determined by the router.For example, the self-attention and MLP in each vanilla transformer block have a capacity of 𝑇-the total number of tokens across the sequence and batch.The former path is computationally expensive.The total number of FLOPs per forward pass will be fewer than that in a vanilla transformer if we set the capacity for path (1) to be anything less than 𝑇 (the total number of tokens across the sequence and batch).Our goal is to use these router weights to determine the output of a block's computation of each token.Suppose 𝑃 𝛽 (𝑅 𝑙 ) is the 𝛽-th percentile of the set of router weights 𝑅 𝑙 , where 𝛽 = 1 -𝐶/𝑆 and 𝐶 is the user-defined capacity per batch element (an integer < 𝑆 that defines the number of tokens from a sequence that will be processed by a given function).The cardinality of X 𝑙 is 𝐶 (or 𝑘): the user-defined capacity.This puts the router weights along the "gradient path", thus subjecting them to the forces of gradient descent through the course of the language modeling task (We experimented with versions where the router weights are also included along the computational path for those tokens that bypass the block's computations, but it seems to be sufficient-and implementationally simpler-to only include the router weights along the computational path for those tokens that do not bypass the block's computations).While expert-choice routing has a number of advantages, it has one distinct problem: the top-𝑘 operation is non-causal.The first introduces a simple auxiliary loss that empirically affects the primary language modeling objective by approximately 0.2 -0.3%, but allows us to sample from the model autoregressively.We provide empirical evidence that this is a relatively easy auxiliary task that quickly achieves 99% accuracy.All models use the same basic hyperparameter configurations (e.g. 
[ 20240518-14:14:45 ] [model] Done loading inference: 6.362271070480347 
[ 20240518-14:14:45 ] [model] summary result: Set a static compute budget that is less than that of an equivalent vanilla transformer. Limit the number of tokens in a sequence that can participate in a block's computations (i.e., selfattention and subsequent MLP) The computation graph and tensor sizes remain static throughout training. 
[ 20240518-14:14:45 ] [summary] True 5 False 
[ 20240518-14:14:45 ] [lsa] result:  484 
 
[ 20240518-14:14:45 ] [lsa preprocess]  We first trained models with a relatively small FLOP budget (6e18) to determine optimal hyperparameters (see figure the isoFLOP optimal baseline (also 220M, figure 3 model #1), but is upwards of 60% faster to step during training.Crucially, when run on equivalent hardware these two model variants take take approximately the same amount of wall-clock time to train (figure We tested routing every block or every other block, using capacities from 12.5% to 95% of the total sequence.While routing every other block was crucial for strong performance, we found that aggressive capacity reduction was best (gradual improvements were observed when reducing the capacity down to 12.5% of the total sequence, corresponding to 87.5% of tokens routing around blocks, with performance degrading beyond this point).So, it seems the network is robust to significant capacity reductions as long as there is frequent opportunity for full capacity self-attention and MLP computations.Learned routing is crucial, as MoD transformers that use stochastic routing (implemented using a top-𝑘 operation on router weights sampled from a Gaussian distribution) perform drastically worse than both the baseline and normal MoD transformer (figure Depicted in figure Step-wise speed gains come from two sources.First, the FLOP-per-parameter ratio in MoD Figure There exist MoD variants that are both faster to step (by virtue of requiring fewer FLOPs per forward pass) and better performing than the isoFLOP optimal baseline.transformers is less than in the baselines because some proportion of tokens are routed around blocks.Altogether, then, there exist MoD transformers that perform as well as isoFLOP-optimal baselines and are faster to step, both because they use fewer FLOPs per parameter and because they use fewer parameters.Figure We noticed that MoD transformers had memory savings relative to equivalently sized baseline models at larger sizes, with some variants requiring fewer total devices (i.e., a smaller TPU topology).We did not study this extensively, but we anticipate that as one scales to larger models, these savings could be an important consideration when choosing model variants to train, and could have significant positive effects in regards to the KV cache size during autoregressive sampling.Figure improvements relative to baselines.We observe patterns that might warrant further study; namely, some tokens appear to engage each block along the transformer's depth, while others decide to route around blocks whenever possible.Preliminary analyses suggest that the tokens that engage with blocks more frequently are correlated with output predictions that have higher entropy, which possibly corresponds to predictions that are more difficult to make.We evaluated MoD variants during auto-regressive sampling (see figure The MoD technique can be naturally integrated with MoE models (together comprising MoDE models) in addition to vanilla transformers. 
[ 20240518-14:14:53 ] [model] Done loading inference: 8.290794610977173 
[ 20240518-14:14:53 ] [model] summary result: We first trained models with a relatively small FLOP budget (6e18) to determine optimal hyperparameters (see figure the isoFLOP optimal baseline (also 220M, figure 3 model #1), but is upwards of 60% faster to step during training. When run on equivalent hardware these two model variants take take approximately the same amount of wall-clock time to train. 
[ 20240518-14:14:53 ] [summary] True 5 False 
[ 20240518-14:14:53 ] [lsa] result:  323 
 
[ 20240518-14:14:53 ] [lsa preprocess]  Mixture-of-Depths transformers empirically demonstrate that one can improve on isoFLOP-optimal baseline performance with models that use fewer FLOPs per forward pass.This means that-for a given training FLOP budget-we can train models that are both faster and better performing than their baseline counterparts.Our results show that indeed FLOPs may be inefficiently used in vanilla transformer models, and that there may be more efficient ways for them to be expended.Learned routing mechanisms are sometimes non-causal; that is, information about the future is used to determine a given token's routing decision.This is generally true for top-k routing mechanisms, which are useful because they forego the need for auxiliary balancing losses.In this work we show that one can successfully use a top-k routing scheme during training, but not require it during later autoregressive sampling.If a token does not participate in self-attention at a certain block, then later tokens will also not be able to attend to it.One can imagine extending this idea even further into the domain of "long-term memory": perhaps there are tokens that would be extremely valuable as keys, regardless of whether it is useful for them to also be among the queries at the step of their occurrence.Learned routing could be a powerful mechanism for deciding which tokens these might be, perhaps funnelling them into a long-term memory buffer that is available during future self-attention.This is more computationally efficient than performing a full content-based lookup across an entire memory buffer for each step in the future, and could be one step towards drastically increasing the context-length available for making a prediction.Unlike MoE transformers that route between effectively the same computation (usually MLPs), MoD transformers demonstrate the value of routing among different types of computations.In this work the types were either the conventional transformer block, or a null computation (functionally equivalent to multiplying by zero). 
[ 20240518-14:14:58 ] [model] Done loading inference: 5.250183343887329 
[ 20240518-14:14:58 ] [model] summary result: Learned routing mechanisms are sometimes non-causal; that is, information about the future is used to determine a given token's routing decision. This is generally true for top-k routing mechanisms, which forego the need for auxiliary balancing losses. 
[ 20240518-14:14:58 ] [summary] True 5 False 
[ 20240518-14:14:58 ] [lsa] result:  154 
 
[ 20240518-14:14:58 ] [lsa preprocess]  The ability of organisms to develop resistance to the effects of antimicrobial therapies developed to kill them is potentially the greatest challenge to healthcare in the 21 st century.It increases the threat not only of primary infectious diseases such as tuberculosis, but also secondary infections associated both with other diseases and the provision of healthcare itself The paper highlights the critical problem with the current emphasis on using economic burden as evidence for directing future investment in health issues, as well as difficulties related to methodologies available for estimating such burden.The paper concludes by considering the policy implications as well as the key research/evidence gaps and the challenges associated with filling those gaps.Note also that we are concerned here with human use and human health -there are critical links with animal use and health (eg 8), but we do not have space to consider these in this report. 
[ 20240518-14:15:04 ] [model] Done loading inference: 5.278980016708374 
[ 20240518-14:15:04 ] [model] summary result: The ability of organisms to develop resistance to the effects of antimicrobial therapies is potentially the greatest challenge to healthcare in the 21 st century. It increases the threat not only of primary infectious diseases such as tuberculosis, but also secondary infections associated with other diseases and the provision of healthcare itself. 
[ 20240518-14:15:04 ] [summary] True 5 False 
[ 20240518-14:15:04 ] [lsa] result:  230 
 
[ 20240518-14:15:04 ] [lsa preprocess]  Before looking at the current economic burden of AMR, however, it is worth reminding ourselves of the history of AMR.Resistance may be seen as essentially a reaction to the use of antimicrobial treatments.Although the process of natural selection encourages micro-organisms to adapt to environmental pressures, the use of antimicrobial therapies can accelerate this natural process, whereby sensitive micro-organisms are soon eliminated by resistant ones (4).Although there remain uncertainties over the development of resistance, the 'genetic cost' to the organism, and the extent to which resistance is temporary or permanent, there is concern that over time there is no reason to suspect that resistance will not occur to all antimicrobials; the only question is to what level Resistance means that an antimicrobial therapy is no longer (as) effective against the organism it is targeting.For any particular antimicrobial, the correlation between consumption and resistance is complicated by many factors, including the relative 'fitness' of sensitive and resistant strains, together with the existence of genetic elements simultaneously coding for resistance to several antimicrobials.This is important, as simply reducing the consumption of a specific antimicrobial cannot necessarily be relied upon to produce an equivalent reduction in resistance, and thus once the effectiveness of an antimicrobial is 'lost' it may be lost forever 
[ 20240518-14:15:08 ] [model] Done loading inference: 3.894538402557373 
[ 20240518-14:15:08 ] [model] summary result: Resistance may be seen as essentially a reaction to the use of antimicrobial treatments. Resistance means that an antimicrobial therapy is no longer (as) effective against the organism it is targeting. 
[ 20240518-14:15:08 ] [summary] True 5 False 
[ 20240518-14:15:08 ] [lsa] result:  122 
 
[ 20240518-14:15:08 ] [lsa preprocess]  So, given this history, why is the level of concern increasing now?It is because the increase in organisms resistant to multiple therapies is now coinciding with the reduction in new therapies coming to market to replace ineffective ones.That resistance develops to an antimicrobial therapy is not itself a problem -and as indicated is merely a natural process, albeit accelerated by use of these therapiesas long as there are other therapies to take its place.During the latter half of the 20 th century this was the predominant situation.Over this period the various classes of therapeutics, and the specific therapeutics within these classes, were discovered and the antimicrobial 'armoury' was added to on a regular basis 
[ 20240518-14:15:12 ] [model] Done loading inference: 4.368354082107544 
[ 20240518-14:15:12 ] [model] summary result: Increase in organisms resistant to multiple therapies is now coinciding with the reduction in new therapies coming to market to replace ineffective ones. That resistance develops to an antimicrobial therapy is not itself a problem -and as indicated is merely a natural process. 
[ 20240518-14:15:12 ] [summary] True 5 False 
[ 20240518-14:15:12 ] [lsa] result:  115 
 
[ 20240518-14:15:12 ] [lsa preprocess]  In 2004, only 1.6% of drugs in development by the world's 15 largest drug companies were antimicrobials At the same time that the number of new therapeutics has been declining, organisms such as S. Aureus and E. faecium have been acquiring resistance to multiple therapies To sum, organisms develop resistance to antimicrobials, and increasingly are developing resistance to multiple therapies, rendering these antimicrobials ineffective.At the same time, the once prolific pipeline bringing new antimicrobials into clinical practice is faltering.We are therefore at a pivotal stage in the history of infectious disease, where the window of opportunity afforded by antimicrobial therapies over recent decades is rapidly closing 
[ 20240518-14:15:18 ] [model] Done loading inference: 5.644575595855713 
[ 20240518-14:15:18 ] [model] summary result: In 2004, only 1.6% of drugs in development by the world's 15 largest drug companies were antimicrobials. At the same time that the number of new therapeutics has been declining, organisms such as S. Aureus and E. faecium have been acquiring resistance to multiple therapies. 
[ 20240518-14:15:18 ] [summary] True 5 False 
[ 20240518-14:15:18 ] [lsa] result:  178 
 
[ 20240518-14:15:18 ] [lsa preprocess]  Clearly there has been progress, especially in recent years, in practice and policy concerning more conservative and appropriate use of antimicrobials in the attempt to halt or slow the progress of resistance.There are various policy, public and professional reports and strategies Work, such as that in the UK, is enabling us to become better custodians of antibiotics, reshaping the debate on how to control AMR.This has involved encouraging better diagnosis, medicine management and use of therapeutics by promoting the prudent use of antimicrobials and educating healthcare workers to use antibiotics more appropriately (i.e.only when needed, ensuring correct dose and duration for treatment), as well as promoting good infection control (e.g.hand hygiene, screening, patient isolation) to prevent and control AMR.These interventions have been informed by surveillance activities.This work has helped to strengthen surveillance, infection prevention and control, and to promote the responsible use of antimicrobials in the UK.It can be expected to have an impact on the containment of antimicrobial resistance. 
[ 20240518-14:15:23 ] [model] Done loading inference: 5.396419048309326 
[ 20240518-14:15:23 ] [model] summary result: There has been progress, especially in recent years, in practice and policy concerning more conservative and appropriate use of antimicrobials. Work, such as that in the UK, is enabling us to become better custodians of antibiotics, reshaping the debate on how to control AMR. 
[ 20240518-14:15:23 ] [summary] True 5 False 
[ 20240518-14:15:23 ] [lsa] result:  374 
 
[ 20240518-14:15:23 ] [lsa preprocess]  Although there have been positive changes, and many within the scientific community are now convinced of the need for action, the question is whether interventions such as these remain marginal in relation to the total size of the problem, with insufficient impact on reductions in antimicrobial use to change a future in which the loss of effective antimicrobial therapies is inevitable.This question of marginality is pertinent both to the within-country context (is there sufficient reduction in usage in one setting such as the UK?)and the cross-country context (is there sufficient reduction in usage globally?).The core problems, of widespread use, rapid transmission and lack of new product development, remain.In this respect, incentives continue to be the major concern at all levels.As indicated above (figure Notwithstanding whether the possible compounds for discovery have been exhausted, this is unsurprising; there are few incentives for pharmaceutical companies to develop a new drug for which health systems will aim to restrict use Similarly, there are incentive problems with achieving conservative use of antimicrobials.For each individual who wants to take an antimicrobial to feel better, the impact on resistance of their specific antimicrobial use is virtually unidentifiable, making it very hard to enforce a substantive reduction in use, even in areas deemed 'inappropriate' There are also incentive problems for policy makers.There is growing emphasis on evidence-based policy making, which includes considering the economic burden of disease, and the demonstrated cost-effectiveness of therapies.Put simply, this way of informing policy requires burden to be high now in order to justify expenditure on new drugs, and especially to justify it on the basis of cost-effectiveness, and the cost of resistance needs to be high now to justify greater restriction on use of current drugs However, as outlined below, evidence to date is that AMR has only a comparatively minor cost impact.There are, of course, considerable parallels here with wider aspects of prevention versus treatment when health budgets and results are expected to focus upon the short term, and at the mercy of the political cycle, which encourages group myopia 
[ 20240518-14:15:30 ] [model] Done loading inference: 6.75658655166626 
[ 20240518-14:15:30 ] [model] summary result: There are few incentives for pharmaceutical companies to develop a new drug for which health systems will aim to restrict use. There are incentive problems with achieving conservative use of antimicrobials. For each individual who wants to take an antimicrobial to feel better, the impact on resistance of their specific antimicrobial use is virtually unidentifiable. 
[ 20240518-14:15:30 ] [summary] True 5 False 
[ 20240518-14:15:30 ] [lsa] result:  309 
 
[ 20240518-14:15:30 ] [lsa preprocess]  Qualitatively we know that treatment failure caused by AMR contributes to increased costs of care associated with: additional investigations such as laboratory tests and X-ray examinations; additional or alternative treatments, often much more expensive than drugs used to treat infections caused by sensitive organisms; additional sideeffects from more toxic treatments, which have to be managed; longer hospital stay; longer time off work; reduced quality of life and productivity; greater likelihood of death due to inadequate or delayed treatment, hence reducing the workforce; increased burden on family of infected individual; increases in private insurance coverage; additional cost for hospital when hospital-acquired infection occurs and infection control procedures required; increased costs of disease surveillance; increased costs to firms of absenteeism, possibly leading to increased product prices; and so forth Yet, quantitatively, the problem is that, far from illuminating the burden of resistance, this translates into a vast range of figures depending upon what precisely is assessed.Cost estimates will depend on, for instance, whether assessment is at the level of the individual or (multiple) institutions, whether figures are based on comparison of a resistant versus susceptible patient/infection or they are total costs of care (resistant versus nothing), whether figures include hospital costs only, look at patient costs, or incorporate productivity costs (i.e.consider the health care or the 'societal' perspective), and the methods used to estimate these costs, whether they are focused on one or multiple disease areas, and whether preventative control measures are included An important, although now dated, review of these studies found that patients infected with resistant organisms generally had poorer health and economic outcomes than those patients with susceptible organisms 
[ 20240518-14:15:34 ] [model] Done loading inference: 4.023313522338867 
[ 20240518-14:15:34 ] [model] summary result: Treatment failure caused by AMR contributes to increased costs of care. Cost estimates will depend on, for instance, whether assessment is at the level of the individual or (multiple) institutions. 
[ 20240518-14:15:34 ] [summary] True 5 False 
[ 20240518-14:15:34 ] [lsa] result:  232 
 
[ 20240518-14:15:34 ] [lsa preprocess]  We also previously undertook a systematic literature review concerning the economics of AMR, summarizing studies focusing upon the costs of resistance published up to 2000 For this current paper, we updated these searches to focus on papers published since 2000 which reported the cost impact of resistance in English-language, peerreviewed journals, where we could extract any or all of length of stay, mortality, patient cost and/or societal cost that may be attributable to AMR.The focus of this updating review, in line with the DH brief and the focus of this paper, was the economic burden of resistance; it excluded review of the literature on the costeffectiveness of alternative control strategies which is concerned with policy options for dealing with the issue, rather than the nature of the issue itself.Initial searches were only conducted on combinations of resistant/ce, antimicrob/ial and cost/s; as it became clear that papers that did not refer to antimicrobial resistance more generally, but only to either particular drugs or particular microorganisms (an indicator of a much more fundamental problem, to which we return in the discussion), would not be captured in this search, a subsequent search focused particularly on MRSA and VRE, as two of the most studied and potentially more serious current resistant infections.These studies are summarized in table 
[ 20240518-14:15:38 ] [model] Done loading inference: 4.129441976547241 
[ 20240518-14:15:38 ] [model] summary result: The focus of this updating review was the economic burden of resistance. It excluded review of the literature on the costeffectiveness of alternative control strategies which is concerned with policy options for dealing with the issue. 
[ 20240518-14:15:38 ] [summary] True 5 False 
[ 20240518-14:15:38 ] [lsa] result:  258 
 
[ 20240518-14:15:38 ] [lsa preprocess]  The studies listed in table 1 are consistent with previous reviews, indicating three key findings that appear robust to change over time.First, there are a vast range of figures, from less than £5 to more than £20,000 in reported additional costs per patient per episode for hospital costs, and anything up to around £10 billion per year for full societal costs.These figures are largely determined by whether and how productivity losses are incorporated Second, most studies originate in the USA.This may be a reflection of the Englishlanguage journals included in the search, but such a degree of national dominance is still unusual.Given the very unique nature of the US health system, and its financial structure, these costs are unlikely to be indicative of other systems, such as the UK.Third, there is a heavy predominance of hospital-based studies, and indeed the costs are almost exclusively related to costs of additional hospitalization/treatment and do not include costs associated with early mortality; from an economic perspective this almost certainly underestimates the cost.There is only one UK empirical study, which also happens to be one of only two community studies, and one UK study that is the only one to apply a macro-economic modelling technique Overall, in sum, although there has been a substantial increase in studies over the last decade compared with prior to 2000, there remains a low, selective and widely divergent evidence base concerning 'the economic burden of AMR'. 
[ 20240518-14:15:42 ] [model] Done loading inference: 4.388662576675415 
[ 20240518-14:15:42 ] [model] summary result: There is a low, selective and widely divergent evidence base concerning 'the economic burden of AMR' There is only one UK empirical study, which also happens to be one of only two community studies. 
[ 20240518-14:15:42 ] [summary] True 5 False 
[ 20240518-14:15:42 ] [lsa] result:  69 
 
[ 20240518-14:15:42 ] [lsa preprocess]  An interesting case study here is clearly that of MRSA versus MSSA.This is summarized in Box 1 and two conclusions are apparent.First, there is a very wide range of costs.Second, even at the high-end, the costs remain quite modest.It is worth noting, however, that unlike some of the resistant gram-negatives currently emerging, there remains a choice of therapy available to treat MRSA. 
[ 20240518-14:15:46 ] [model] Done loading inference: 3.572269916534424 
[ 20240518-14:15:46 ] [model] summary result: An interesting case study here is clearly that of MRSA versus MSSA. Unlike some of the resistant gram-negatives currently emerging, there remains a choice of therapy available to treat MRSA. 
[ 20240518-14:15:46 ] [summary] True 5 False 
[ 20240518-14:15:46 ] [lsa] result:  17 
 
[ 20240518-14:15:46 ] [lsa preprocess]  The relatively low cost for MRSA is reflective of AMR more generally, as illustrated in table 
[ 20240518-14:15:50 ] [model] Done loading inference: 3.526038885116577 
[ 20240518-14:15:50 ] [model] summary result: The relatively low cost for MRSA is reflective of AMR more generally, as illustrated in table. The relatively low costs of MRSA are reflective of the AMR cost more generally. 
[ 20240518-14:15:50 ] [summary] True 5 False 
[ 20240518-14:15:50 ] [lsa] result:  204 
 
[ 20240518-14:15:50 ] [lsa preprocess]  The paradox of the relatively low level of economic impact Current evidence therefore suggests that the economic burden from AMR is actually quite modest, even though AMR is acknowledged to be a significant threat to health and healthcare.This apparent inconsistency might arise because current estimates of the cost of AMR are based loosely on the 'incremental' cost related to the extra treatment of resistant over (actual or assumed) susceptible infection.These costs, broadly speaking, increase as we see multi-drug resistance (MDR) emerge (such as in the case of MDR-TB The economic consequences of AMR outlined in the papers reviewed relate only to the direct impacts of resistance itself on the ability to treat primary infections; which, as indicated, tend to be incremental.Of potentially greater importance is the more indirect future impact that resistance may have on the ability of the health service to deliver other forms of healthcare in the presence of increasing rates of secondary infection.Here there is no evidence that we are aware of.Before moving on to consider this issue further, however, it is worth reflecting upon the limitations of attempting to assess the current burden of AMR. 
[ 20240518-14:15:55 ] [model] Done loading inference: 4.958628177642822 
[ 20240518-14:15:55 ] [model] summary result: Evidence suggests that the economic burden from AMR is actually quite modest. This may be because current estimates of the cost of AMR are based loosely on the 'incremental' cost related to the extra treatment of resistant over (actual or assumed) susceptible infection. 
[ 20240518-14:15:55 ] [summary] True 5 False 
[ 20240518-14:15:55 ] [lsa] result:  66 
 
[ 20240518-14:15:55 ] [lsa preprocess]  A standard 'cost-of-illness' approach will not capture the true nature of the costs of AMR for three reasons.First, AMR is a negative externality associated with consumption of antimicrobials The problem is that the externality effect from each antimicrobial consumed is miniscule, especially once the future effects are discounted Second, the specific sigmoidal pattern of the development of resistance, illustrated in figure 
[ 20240518-14:16:00 ] [model] Done loading inference: 5.33904767036438 
[ 20240518-14:16:00 ] [model] summary result: A standard 'cost-of-illness' approach will not capture the true nature of the costs of AMR for three reasons. First, AMR is a negative externality associated with consumption of antimicrobials. Second, the specific sigmoidal pattern of the development of resistance. 
[ 20240518-14:16:00 ] [summary] True 5 False 
[ 20240518-14:16:00 ] [lsa] result:  204 
 
[ 20240518-14:16:00 ] [lsa preprocess]  The critical implication is that this uncertainty regarding current and future burden combined with discounting of future benefits means that strategies to reduce transmission are far more likely to appear cost-effective than strategies to control emergence, hence reinforcing the status-quo Third, estimates of cost-of-illness tend not to focus on less direct costs associated with the impact of resistance on patient safety or public confidence in health care institutions.Public and media concerns frequently focus on the plight of individuals who suffer what is seen to be 'avoidable' infection within a context where the institution is empowered and the patient is both vulnerable and dependent.There is therefore a cost to maintaining public confidence in healthcare providers and institutions.The future?Antimicrobials are the cornerstone of modern medicine that revolutionized healthcare during the last half-century.From cradle to grave, the role of antimicrobials in safeguarding the overall health of human societies has become pivotal.So the 'real' costs of AMR are those that relate to the loss of these benefits; the treatment possibilities at every stage of human life that have been enabled and enhanced because of antimicrobials.We know, for example, that MDR bacteria have increased mortality rates amongst newborn babies 
[ 20240518-14:16:04 ] [model] Done loading inference: 4.480498313903809 
[ 20240518-14:16:04 ] [model] summary result: Antimicrobials are the cornerstone of modern medicine that revolutionized healthcare. Estimates of cost-of-illness tend not to focus on the impact of resistance on patient safety or public confidence in health care institutions. 
[ 20240518-14:16:04 ] [summary] True 5 False 
[ 20240518-14:16:04 ] [lsa] result:  214 
 
[ 20240518-14:16:04 ] [lsa preprocess]  The 'true' cost of AMR It has been said that AMR presents a risk that we will fall back into the pre-antibiotic era (3).However, this is perhaps a more rosy picture than the reality.The health system has changed fundamentally over the last 60 years, with antimicrobials integrated in almost all aspects of care.The system is designed to treat more chronic conditions, provide treatments on a short-term -often day-case -basis, and encourage prevention.As Box 2 illustrates, in many cases antimicrobials are given as a matter of standard prophylactic care.As witnessed when there are outbreaks of hospital-acquired infection, the system can very quickly come to a standstill Multiplied across all the hundreds of clinical areas where antimicrobials are currently used, it can easily be envisaged that this will not only be a significant health burden, and present increased healthcare cost (inpatient stay) itself, but would present a catastrophic blow to health system development -for instance, requiring redesign of many facilities, the reintroduction of sanatoria and so forth.The full economic burden of this is not only inestimable at present, but unimaginable.Increasing rates of infection to the level this would also have enormous wider economic impacts (significant workforce impacts 
[ 20240518-14:16:09 ] [model] Done loading inference: 4.997716426849365 
[ 20240518-14:16:09 ] [model] summary result: The health system has changed fundamentally over the last 60 years, with antimicrobials integrated in almost all aspects of care. The system is designed to treat more chronic conditions, provide treatments on a short-term -often day-case -basis. 
[ 20240518-14:16:09 ] [summary] True 5 False 
[ 20240518-14:16:09 ] [lsa] result:  177 
 
[ 20240518-14:16:09 ] [lsa preprocess]  We are entering a pivotal period where, if current trends continue, there could be highly significant costs to healthcare, and society more generally, as antimicrobials that form the basis of modern healthcare become increasingly ineffective.As AMR is a natural process, we are not looking at something that can be 'eradicated'; rather, it is something we have to manage if we are to continue to benefit from antimicrobial therapies Effective antimicrobials therefore need rediscovering as a scarce -and largely nonrenewable -resource (3).This requires an assessment of the balance between the positive effects of using antimicrobial therapies now, and the negative impact of this use on their temporal effectiveness, and hence assessment of the optimal use of antimicrobials over time.As with other areas of prevention, and issues such as global warming, we need to pursue a path that does not place undue emphasis on current burdens and costs, but reflects the importance of stewardship for the future.The presents us with three challenges. 
[ 20240518-14:16:14 ] [model] Done loading inference: 4.713609457015991 
[ 20240518-14:16:14 ] [model] summary result: We are entering a pivotal period where, if current trends continue, there could be highly significant costs to healthcare, and society more generally. Effective antimicrobials need rediscovering as a scarce -and largely nonrenewable -resource. 
[ 20240518-14:16:14 ] [summary] True 5 False 
[ 20240518-14:16:14 ] [lsa] result:  327 
 
[ 20240518-14:16:14 ] [lsa preprocess]  We know the current economic burden is relatively low compared with other problems.Yet we do not know to what extent the future burden will grow, or how quickly.We also do not know whether an increasing burden will give impetus to new technological change outside the drug arena that might mitigate effects.(For instance, dealing with climate change involves reduction in car use, but also making cars more efficient, and we 'solved' the issue of Chlorofluorocarbons (CFCs) and ozone depletion by simply replacing all CFCs with non-damaging gases in airconditioning and refrigerator units.)Here, alternatives to antimicrobials include vaccination, but could also include structuring of care such that infection is less likely, for example changes to ventilation systems, bed spacing and so on.We are therefore faced with considerable uncertainty, but uncertainty that suggests we need to incur some (perhaps considerable) cost now associated with current reduced use of antimicrobials, in the expectation of some future, indeterminate but likely far greater, cost being averted.To judge the scale of 'acceptable' cost now, however, we need much better information about both likely future trajectory and the cost implications under different trajectories.A key research need is thus to estimate the impact of widespread resistance to the health system overall, and to wider society.The current focus, on the additional cost of treating an infectious disease in the presence of resistance, needs to be complemented by looking at how services such as those relating to cancer care, heart disease and diabetes might be affected.Such research will be a challenge not just in funding terms but in terms of bringing together those with the relevant expertise, ensuring that they can 'talk to each other' and developing methods that can both identify crucial gaps in the information base and, ultimately, provide robust estimates. 
[ 20240518-14:16:19 ] [model] Done loading inference: 4.659298419952393 
[ 20240518-14:16:19 ] [model] summary result: We know the current economic burden is relatively low compared with other problems. We do not know to what extent the future burden will grow, or how quickly. A key research need is to estimate the impact of widespread resistance. 
[ 20240518-14:16:19 ] [summary] True 5 False 
[ 20240518-14:16:19 ] [lsa] result:  221 
 
[ 20240518-14:16:19 ] [lsa preprocess]  There needs to be an improvement in the incentive mechanisms at a number of levels.In terms of the development of new antimicrobials, if new therapies are discovered then they need to be protected, and hence the use of them discouraged.New options are needed that can discourage high levels of use whilst avoiding disincentives for private sector R&D into new therapies, such as greater publicprivate partnering, pre-purchase agreement, or direct public funding, which seems to be occurring at present but, compared with current estimates of costs of discovery to market, remain small In terms of individuals and their choice about whether to take antimicrobials, more needs to be done to balance the personal cost (minimal in the UK, and largely related to accessing a consultation, collecting a prescription and possible side effects) with the true societal cost.Some thought has previously been given to such mechanisms The acceptability of such restrictions may, of course, be related to whether the research discussed above is sufficiently robust to sustain such developments.Implementation of any radical -rather than currently marginal -strategies would require considerable public support to be politically acceptable, and in this respect much may be learnt perhaps from the environmental movement, as discussed further below. 
[ 20240518-14:16:24 ] [model] Done loading inference: 4.800657033920288 
[ 20240518-14:16:24 ] [model] summary result: In terms of the development of new antimicrobials, if new therapies are discovered then they need to be protected, and hence the use of them discouraged. There needs to be an improvement in the incentive mechanisms at a number of levels. 
[ 20240518-14:16:24 ] [summary] True 5 False 
[ 20240518-14:16:24 ] [lsa] result:  176 
 
[ 20240518-14:16:24 ] [lsa preprocess]  As is apparent from the review of costs, the issue of antimicrobial resistance is not confined to the UK.International activity may be key to encouraging the development of new drugs and diagnostics to help control multi resistant bacteria.It may also be vital to research to increase understanding of resistance mechanisms, cost trajectories, and means of controlling resistance in the absence of new drug developments.There is appreciation of this issue in the UK, where a multi-pronged integrated UK strategy is under development (building on previous work and taking account of developments at EU and international level) and where championing the issue at EU and international levels is also an important focus.It is also important, however, to consider the extent to which a UK strategy needs to account for the likely success or otherwise of such championing, given that countries may have some incentive to free-ride on the actions of others, and given that outside influences may well affect the likely trajectories anticipated in the research described above 
[ 20240518-14:16:28 ] [model] Done loading inference: 4.433101177215576 
[ 20240518-14:16:28 ] [model] summary result: International activity may be key to encouraging the development of new drugs and diagnostics to help control multi resistant bacteria. There is appreciation of this issue in the UK, where a multi-pronged integrated UK strategy is under development. 
[ 20240518-14:16:28 ] [summary] True 5 False 
[ 20240518-14:16:28 ] [lsa] result:  407 
 
[ 20240518-14:16:28 ] [lsa preprocess]  Both provide future significant threats to human well-being, both are subject to considerable uncertainty about their future extent and trajectory, both are global problems where the response differs across nations, and both have as an underlying cause the over-consumption of 'goods' that lead to short term benefit.Yet for both antimicrobial resistance and climate change there is considerable inertia to the major, radical, change required to move from mitigation to prevention, because there is a focus on current burden and because the personal incentives seldom match those of society more generally.There do appear to be some differences, however.In particular, there appears to be increasing scientific consensus about the impact of global warming in a way that is perhaps less clear -or at least coherent -for antimicrobial resistance.Clear, simple and consistent messages from the scientific community have been vital in developing the increased acceptance of the desirability of action on global warming to a broader policy making community including politicians, economists and philanthropists.This consensus, along with sustained and high profile campaigns and the impact of positive media attention (including Hollywood movies), has increased public support.Combined with simple messages on what individuals can do (e.g.'reduce, reuse, recycle'), this has moved climate change into the mainstream of activities -to the extent that many aspects of individual action are now routine (e.g.The issue of resistance does not seem to have captured the public imagination, attention or support for change to the degree that global warming and the environment has.For antimicrobial resistance there is a clear danger that waiting for the burden to become significant before taking action may mean waiting until it is too late to stop an apocalyptic scenario -the very drive behind the early environmental movement's advocacy of the 'precautionary principle'.MRSA has also recently emerged as an important cause of community-associated infections.Although several compounds have been developed, or resurrected, to treat gram-positive infections such as S. Aureus, none have been shown to work better than vancomycin, all have important toxic effects, and resistance to each has already been observed (including linezolid-resistant VRE in patients who have never received the drug) In economic terms, illustrated by table 1, MRSA adds around $20,000 per patient per episode to the cost of hospital treatment in the USA 
[ 20240518-14:16:34 ] [model] Done loading inference: 5.507009029388428 
[ 20240518-14:16:34 ] [model] summary result: Antimicrobial resistance does not seem to have captured the public imagination, attention or support for change to the degree that global warming and the environment has. For both antimicrobial resistance and climate change there is considerable inertia to the major, radical, change required. 
[ 20240518-14:16:34 ] [summary] True 5 False 
[ 20240518-14:16:34 ] [lsa] result:  388 
 
[ 20240518-14:16:34 ] [lsa preprocess]  The problem facing any attempt to estimate the impact of removal of antimicrobial therapies is data from such a situation; such therapies are, and have been, part of routine care ever since hip replacements became available, both as prophylaxis and as treatment for HAI.In an attempt to think laterally, we therefore looked at information relating to amputation -another major surgery involving limbs -as a proxy for what rates may have been pre-and post-antimicrobial discovery.We used this information, together with current studies looking at the infection pathway for hip replacement, to construct and suggest possible values for the flow of patients requiring hip replacement, illustrated below.Here we have patients undergoing total hip replacement (THA) having prophylaxis or not, and from this having infections or not, being treated or not, and rates of effectiveness translating to final outcomes.Currently, prophylaxis is standard practice so approximately 100% of patients follow route 4 rather than 5.Most of those following 6, go to 10 and have further treatment which is successful.Most patients therefore exit at 15 or 18 (>99%).If we estimate this flow with no antimicrobials, we are therefore restricting possibilities to route 5, and for those at 8, route 13 -and hence end states of 21, 22 or 23.Here, at point 8 rates of post-operative infection are around 40-50%; of these, 30% go on to die -state 21 Clearly this represents a very crude estimate, for just one clinical area using antibiotics, but indicates the form of analysis required if we are to move towards beginning to estimate the full, true, economic burden of future AMR, with the removal of the option of antimicrobial therapies from a number of treatment pathways, both urgent and elective.In line with the DH brief, the purpose of the literature review was to conduct a short review describing the economic burden (cost impact) of antimicrobial resistance.An earlier systematic literature review concerning the economics of resistance, and published in 2002, had considered this as one among a number of questions relating to the economics of antimicrobial resistance.At the time little evidence relating to this issue was found. 
[ 20240518-14:16:39 ] [model] Done loading inference: 5.2095866203308105 
[ 20240518-14:16:39 ] [model] summary result: Prophylaxis is standard practice so approximately 100% of patients follow route 4 rather than 5. At point 8 rates of post-operative infection are around 40-50%; of these, 30% go on to die -state 21. 
[ 20240518-14:16:39 ] [summary] True 5 False 
[ 20240518-14:16:39 ] [lsa] result:  69 
 
[ 20240518-14:16:39 ] [lsa preprocess]  Stage 1: Electronic bibliographic database searching on generic terminology Initial searching of electronic databases utilised key terms taken from the earlier review, searching on combinations of terms related to antimicrobial, resistance and costs.Given the limited resources, both financially and in terms of time, the searching of databases was limited to Web of Science and Medline and to searches on titles.Specific terms included were: 
[ 20240518-14:16:43 ] [model] Done loading inference: 4.088133335113525 
[ 20240518-14:16:43 ] [model] summary result: Given the limited resources, both financially and in terms of time, the searching of databases was limited to Web of Science and Medline and to searches on titles. Initial searching of electronic databases utilised key terms taken from the earlier review. 
[ 20240518-14:16:43 ] [summary] True 5 False 
[ 20240518-14:16:43 ] [lsa] result:  37 
 
[ 20240518-14:16:43 ] [lsa preprocess]  Reference lists of papers selected for the review were scanned to identify any further papers.Review papers identified through the review were also used in citation searching, both at this stage and following stage 3 searching. 
[ 20240518-14:16:47 ] [model] Done loading inference: 3.4542391300201416 
[ 20240518-14:16:47 ] [model] summary result: Review papers identified through the review were also used in citation searching, both at this stage and following stage 3 searching. Reference lists of papers selected for thereview were scanned to identify any further papers. 
[ 20240518-14:16:47 ] [summary] True 5 False 
[ 20240518-14:16:47 ] [lsa] result:  127 
 
[ 20240518-14:16:47 ] [lsa preprocess]  Searches of citation lists of both empirical papers identified in Search 1 and key review papers, indicated that there were clearly papers relating to evidence about the costs of resistance in relation to particular micro-organisms that were being missed because of the lack of inclusion of specific terminology related to the relevant micro-organisms and/or the relevant antimicrobials.Again, given limited resources, it was not feasible to search on all possible micro-organism names and all potential drugs.Instead, a second search aimed to focus on two of the most studied and potentially more serious current resistant infections: Methicillin-resistant Staphylococcus aureus (MRSA) and Vancomycin-resistant Enterococcus (VRE),and these terms were added to the search and combined with the economic terms. 
[ 20240518-14:16:55 ] [model] Done loading inference: 8.181807041168213 
[ 20240518-14:16:55 ] [model] summary result: Searches of citation lists of both empirical papers identified in Search 1 and key review papers, indicated that there were clearly papers relating to evidence about the costs of resistance in relation to particular micro-organisms that were being missed. A second search aimed to focus on two of the most studied and potentially more serious current resistant infections: Methicillin-resistant Staphylococcus aureus and Vancomycin-resistant Enterococcus. 
[ 20240518-14:16:55 ] [summary] True 5 False 
[ 20240518-14:16:55 ] [lsa] result:  212 
 
[ 20240518-14:16:55 ] [lsa preprocess]  There were six inclusion criteria for the review.1.Papers providing EITHER (i) empirical evidence on the economic impact of antimicrobial resistance obtained through primary data collection OR (ii) empirical evidence on the economic impact of antimicrobial resistance obtained through secondary modelling 2.Containing information on any of length of stay, mortality, patient cost and/or societal cost that may be attributable to AMR 3.For primary studies, inclusion of a control group of those with a susceptible infection.4.Publication since 2000 (the cut-off date for the earlier review) 5.Publication in the English-language 6.Publication in peer-reviewed journals With regard to the third inclusion criterion, the choice of the control group of those with a susceptible infection was made as the aim was to focus on the costs of resistance rather than the costs of infection per se.Other economic aspects of the antimicrobial resistance problem, such as the costeffectiveness of alternative control strategies, were not included.Review papers were not selected for inclusion in the review, but where these were identified they were accessed for the purpose of citation tracking.As part of the aim was to determine the extent of evidence available, studies were not accepted or rejected on the basis of any quality criteria. 
[ 20240518-14:17:01 ] [model] Done loading inference: 6.220607042312622 
[ 20240518-14:17:01 ] [model] summary result: Review papers were not selected for inclusion in the review, but where these were identified they were accessed for the purpose of citation tracking. Studies were not accepted or rejected on the basis of any quality criteria. Other economic aspects of the antimicrobial resistance problem, such as the costeffectiveness of alternative control strategies, were not included. 
[ 20240518-14:17:01 ] [summary] True 5 False 
[ 20240518-14:17:01 ] [lsa] result:  70 
 
[ 20240518-14:17:01 ] [lsa preprocess]  Papers in the stage 1 search were initially identified from abstracts obtained in the literature search and screened by three individuals, one of whom (JC) was expert in the subject area.Papers that were obtained were then read and a final decision made on whether they should be included in the review.Following the citation searches and subsequent database searching, a number of additional papers were selected. 
[ 20240518-14:17:05 ] [model] Done loading inference: 3.8785336017608643 
[ 20240518-14:17:05 ] [model] summary result: Papers in the stage 1 search were initially identified from abstracts obtained in the literature search. Papers that were obtained were then read and a final decision made on whether they should be included in the review. 
[ 20240518-14:17:05 ] [summary] True 5 False 
[ 20240518-14:17:05 ] [lsa] result:  22 
 
[ 20240518-14:17:05 ] [lsa preprocess]  Standardised data extraction forms, based on the earlier systematic review, were utilised.The following data were extracted for each study: 
[ 20240518-14:17:08 ] [model] Done loading inference: 3.258929491043091 
[ 20240518-14:17:08 ] [model] summary result: Standardised data extraction forms, based on the earlier systematic review, were utilised. The following data were extracted for each study:. Summarize the following segment into 1 sentence: 
[ 20240518-14:17:08 ] [summary] True 5 False 
[ 20240518-14:17:08 ] [lsa] result:  105 
 
[ 20240518-14:17:08 ] [lsa preprocess]  In total, the review identified 24 relevant papers.The findings from the review are summarised in the report and table 1 provides details from the individual studies included.It should be emphasised that, because of the lack of resources for a full systematic review, including searching for all combinations of micro-organisms and drugs, the totality of this literature is almost certainly under-estimated.In terms, however, of general estimates of the overall burden of resistance, relevant studies should have been captured during the stage 1 search, and the studies on MRSA and VRE give a flavour of the relevant literature. 
[ 20240518-14:17:13 ] [model] Done loading inference: 4.912549257278442 
[ 20240518-14:17:13 ] [model] summary result: It should be emphasised that, because of the lack of resources for a full systematic review, including searching for all combinations of micro-organisms and drugs, the totality of this literature is almost certainly under-estimated. In total, the review identified 24 relevant papers. 
[ 20240518-14:17:13 ] [summary] True 5 False 
[ 20240518-14:17:13 ] [lsa] result:  476 
 
[ 20240518-14:17:13 ] [lsa preprocess]  The multi-armed bandit (MAB) problem considers the sequential strategy one must devise when playing a row of slot machines: i.e., which arm to play next to maximize cumulative returns.This analogy extends to a wide range of real-world challenges that require online learning, while simultaneously maximizing some notion of reward.The arm may be a medicine a doctor must prescribe to a patient, the reward being the outcome of such treatment on the patient; or the set of resources a manager needs to allocate for competing projects, with the reward being the revenue attained at the end of the month; or the ad/product/content an online recommendation algorithm must display to maximize click-through rate in e-commerce.The contextual MAB, where at each interaction with the world side information (known as 'context') is available, is a natural extension of this abstraction.The 'context' is the physiology of the patient, the type of resources available for each project, or the features of the website user.Sequential decision processes have been studied for many decades, and interest has resurged incited by reinforcement learning (RL) advancements developed within the machine learning community The techniques used in these success stories are grounded on statistical advances on sequential decision processes and multi-armed bandits.The MAB crystallizes the fundamental trade-off between exploration and exploitation in sequential decision making.It has been studied throughout the 20th century, with important contributions by For instance, classic MAB algorithms do not typically generalize to problems with nonlinear reward dependencies or non-Gaussian reward distributions, as exact computation of their statistics of interest is intractable for distributions not in the exponential family We hereby relax these constraints, and consider time-varying models and nonlinear reward functions.We propose to use sequential Monte Carlo (SMC) for non-stationary bandits with nonlinear rewards, where the world -the reward function-is time-varying, and rewards are sequentially observed for the played arms.SMC methods In Bayesian MAB algorithms, the agent must compute sufficient statistics of each arm's rewards over time, for which sequential updates to the posterior of the parameters of interest must be computed.We here show that SMC-based, sequentially updated random measures of per-arm parameter posteriors, enable computation of any statistic a Bayesian MAB policy might require.We generalize existing MAB policies beyond their original stationary setting, and accommodate complex reward models: those for which sampling may be performed even if analytic computation of summary statistics is infeasible.We study latent dynamical systems with non-Gaussian and nonlinear reward functions, for which SMC computes accurate posterior approximations.We present the SMC-based MAB framework in Section 3, and evaluate its performance for Thompson sampling and Bayes-Upper Confidence Bound policies in Section 4.We summarize and conclude with promising research directions in Section 5. 
[ 20240518-14:17:20 ] [model] Done loading inference: 6.3550310134887695 
[ 20240518-14:17:20 ] [model] summary result: The multi-armed bandit (MAB) problem considers the sequential strategy one must devise when playing a row of slot machines. The contextual MAB, where at each interaction with the world side information (known as 'context') is available, is a natural extension of this. 
[ 20240518-14:17:20 ] [summary] True 5 False 
[ 20240518-14:17:20 ] [lsa] result:  346 
 
[ 20240518-14:17:20 ] [lsa preprocess]  The MAB crystallizes the fundamental trade-off between exploration and exploitation in sequential decision making.It formulates the problem of maximizing rewards observed from sequentially chosen actions a ∈ A -named arms in the bandit literature-when interacting with an uncertain environment.The reward generating process is stochastic, often parameterized with θ ∈ Θ to capture the intrinsic properties of each arm.It can potentially depend on context x ∈ X ; e.g., a common choice is X = R d X .In the Bayesian setting, the uncertainty over the true model parameters θ * is also marginalized.Over the years, many MAB policies have been proposed to overcome the explorationexploitation tradeoff Bayes-UCB Thompson sampling (TS) Bayes-UCB and TS can be viewed as different approaches to a Bayesian formulation of the MAB problem.The use of SMC in the context of bandit problems was previously considered for probit These efforts provide evidence that SMC can be successfully combined with Thompson sampling, yet are different in scope from our work.The SMC-based MAB framework we present generalizes existing Bayesian MAB policies beyond their original setting.Contrary to existing MAB solutions, the SMC-based bandit policies we propose (i) are not restricted to specific reward functions, but accommodate nonlinear and non-Gaussian rewards, (ii) address non-stationary bandit environments, and (iii) are readily applicable to state-ofthe-art Bayesian MAB algorithms -Thompson sampling and Bayes-UCB policies-in a modular fashion.Monte Carlo (MC) methods are a family of numerical techniques based on repeated random sampling, which have been shown to be flexible enough for both numerical integration and drawing samples from complex probability distributions of interest With importance sampling (IS), one estimates properties of a distribution when obtaining samples from such distribution is difficult.The basic idea of IS is to draw, from an alternative distribution, samples that are subsequently weighted to guarantee estimation accuracy (and often reduced variance).Here, we leverage SMC for flexible approximations to posterior of interest in non-stationary and nonlinear MAB problems. 
[ 20240518-14:17:25 ] [model] Done loading inference: 5.6653969287872314 
[ 20240518-14:17:25 ] [model] summary result: The MAB crystallizes the fundamental trade-off between exploration and exploitation in sequential decision making. It formulates the problem of maximizing rewards observed from sequentially chosen actions. The SMC-based MAB framework we present generalizes existing Bayesian MAB policies. 
[ 20240518-14:17:25 ] [summary] True 5 False 
[ 20240518-14:17:26 ] [lsa] result:  1046 
 
[ 20240518-14:17:26 ] [lsa preprocess]  We use sequential Monte Carlo to compute posteriors and sufficient statistics of interest for a rich-class of MABs: non-stationary bandits (modeled via linear dynamical systems) with complex (stateless and context-dependent) nonlinear reward functions, subject to non-Gaussian stochastic innovations.We model non-stationary, stochastic MABs in a state-space framework, where for a given reward distribution p a (Y |x, θ), and parameters that evolve in-time via a transition distribution p(θ t |θ t-1 ), we writewhere we explicitly indicate with θ * t the true yet unknown parameters of the non-stationary multi-armed bandit.Within this bandit framework, a Bayesian policy must characterize the posterior of the unknown parameters p(θ t |H 1:t ) as in Equation (4), in which the time-varying dynamics of the underlying transition distribution are incorporated.The posterior of interest given observed reward y t can be written aswhereRecall that the parameter predictive distribution p(θ t |H 1:t-1 ) and parameter posterior p(θ t |H 1:t ) in Equation ( We adhere to the standard MAB formulation, in that each arm of the bandit is described by its own idiosyncratic parameters (no information is shared across arms)), yet we allow for such parameters to evolve independently per-arm in time:.Therefore, the posterior of interest factorizes across armsThis standard MAB formulation with independent, per-arm parameter dynamics enables Bayesian MAB policies to approximate each per-arm parameter posterior separately.Consequently, there will be no particle degeneracy due to increased number of arms.We present in Section 3.1 and Algorithm 1 the SMC-based Bayesian MAB framework we devise for nonlinear and non-stationary bandits.We describe in Section 3.2 how to draw samples from the transition densities, when modeling bandit non-stationarity via the general linear model; and in Section 3.3, we present examples of non-Gaussian and nonlinear (continuous and discrete) reward functions of interest in practice.Throughout, we avoid assumptions on model parameter knowledge and resort to their Bayesian marginalization.We combine SMC with both Thompson sampling and Bayes-UCB policies, by sequentially updating, at each bandit interaction t, a SMC-based random measure to approximate the time-varying posterior of interest,Knowledge of p M (θ t,a |H 1:t ) enables computation of any per-arm reward statistic Bayesian MAB policies require.We present Algorithm 1 with the sequential Importance Resampling (SIR) method• The SMC proposal distribution q(•) at each bandit interaction t obeys the assumed parameter dynamics: θ• SMC weights are updated based on the likelihood of the observed rewards: wt,a ) -Step (9.c) in Algorithm 1; and • The SMC random measure is resampled at every time instant -Step (9.a).Independently of which SMC technique is used to compute the posterior random measure p M (θ t,a |H 1:t ), the fundamental operation in the proposed SMC-based MAB Algorithm 1 is to sequentially update the random measure p M (θ t,a |H 1:t ) to approximate the true per-arm posterior p(θ t,a |H 1:t ) over bandit interactions.This SMC-based random measure is key, along with transition density p(θ t,a |θ t-1,a ), to sequentially propagate parameter posteriors per-arm, and to estimate their sufficient statistics for any Bayesian bandit policy.Require: Number of SMC samples M (for UCB we also require α t )1: Draw initial samples from the parameter prior θ (m0,a) 0,a ∼ p(θ a ), and wfor a ∈ A do t+1,a , Estimate quantile q t+1,a (α t+1 ) as in Equation ( ForBayes-UCB:Observe reward y t+1 for played arm 9:Update the posterior SMC random measure p M (θ t,a |H 1:t ) for all armsper arm a ∈ A, where m ′ t,a is drawn with replacement according to the importance weights w (c) Weight samples of the played arm a t+1 based on the likelihood of observed y t+1 w (mt+1,a t+1 )and normalize the weights10: end forThe marginalized transition density is a multivariate t-distribution whereEach distribution above holds separately for each arm a, and subscript a,0 indicates assumed prior parameters for arm a.These transition distributions are used when propagating per-arm parameter densities in Steps 5 and 9 of Algorithm 1.They are fundamental for the accuracy of the sequential, random measure-based approximation to the posterior, and the downstream performance of the proposed SMC-based MAB policies.Caution must be exercised when using SMC to approximate the dynamic bandit model's posteriors.Notably, the impact of non-Markovian transition distributions in SMC performance must be taken into consideration: the sufficient statistics in Equations ( When exponential forgetting holds in the latent space -i.e., the dependence on past samples decays exponentially, and is negligible after a certain lag-one can establish uniform-in-time convergence of SMC methods for functions that depend only on recent states, see More broadly, one can establish uniform-in-time convergence for path functionals that depend only on recent states, as the Monte Carlo error of p M (θ t-τ :t |H 1:t ) with respect to p(θ t-τ :t |H 1:t ) is uniformly bounded over time.This quick forgetting property is fundamental for the successful performance of SMC methods for inference of linear dynamical states in practice Nevertheless, we acknowledge that any improved SMC solution that mitigates pathdegeneracy issues can only be beneficial for the performance of the proposed SMC-based policies.Algorithm 1 is described in terms of a generic reward likelihood function p a (Y |x t , θ t,a ) that must be computable up to a proportionality constant.In general, categorical variables assign probabilities to an unordered set of outcomes -not necessarily numeric. 
[ 20240518-14:17:26 ] [summary] True 4 False 
[ 20240518-14:17:26 ] [lsa] result:  1046 
 
[ 20240518-14:17:26 ] [lsa preprocess]  We use sequential Monte Carlo to compute posteriors and sufficient statistics of interest for a rich-class of MABs: non-stationary bandits (modeled via linear dynamical systems) with complex (stateless and context-dependent) nonlinear reward functions, subject to non-Gaussian stochastic innovations.We model non-stationary, stochastic MABs in a state-space framework, where for a given reward distribution p a (Y |x, θ), and parameters that evolve in-time via a transition distribution p(θ t |θ t-1 ), we writewhere we explicitly indicate with θ * t the true yet unknown parameters of the non-stationary multi-armed bandit.Within this bandit framework, a Bayesian policy must characterize the posterior of the unknown parameters p(θ t |H 1:t ) as in Equation (4), in which the time-varying dynamics of the underlying transition distribution are incorporated.The posterior of interest given observed reward y t can be written aswhereRecall that the parameter predictive distribution p(θ t |H 1:t-1 ) and parameter posterior p(θ t |H 1:t ) in Equation ( We adhere to the standard MAB formulation, in that each arm of the bandit is described by its own idiosyncratic parameters (no information is shared across arms)), yet we allow for such parameters to evolve independently per-arm in time:.Therefore, the posterior of interest factorizes across armsThis standard MAB formulation with independent, per-arm parameter dynamics enables Bayesian MAB policies to approximate each per-arm parameter posterior separately.Consequently, there will be no particle degeneracy due to increased number of arms.We present in Section 3.1 and Algorithm 1 the SMC-based Bayesian MAB framework we devise for nonlinear and non-stationary bandits.We describe in Section 3.2 how to draw samples from the transition densities, when modeling bandit non-stationarity via the general linear model; and in Section 3.3, we present examples of non-Gaussian and nonlinear (continuous and discrete) reward functions of interest in practice.Throughout, we avoid assumptions on model parameter knowledge and resort to their Bayesian marginalization.We combine SMC with both Thompson sampling and Bayes-UCB policies, by sequentially updating, at each bandit interaction t, a SMC-based random measure to approximate the time-varying posterior of interest,Knowledge of p M (θ t,a |H 1:t ) enables computation of any per-arm reward statistic Bayesian MAB policies require.We present Algorithm 1 with the sequential Importance Resampling (SIR) method• The SMC proposal distribution q(•) at each bandit interaction t obeys the assumed parameter dynamics: θ• SMC weights are updated based on the likelihood of the observed rewards: wt,a ) -Step (9.c) in Algorithm 1; and • The SMC random measure is resampled at every time instant -Step (9.a).Independently of which SMC technique is used to compute the posterior random measure p M (θ t,a |H 1:t ), the fundamental operation in the proposed SMC-based MAB Algorithm 1 is to sequentially update the random measure p M (θ t,a |H 1:t ) to approximate the true per-arm posterior p(θ t,a |H 1:t ) over bandit interactions.This SMC-based random measure is key, along with transition density p(θ t,a |θ t-1,a ), to sequentially propagate parameter posteriors per-arm, and to estimate their sufficient statistics for any Bayesian bandit policy.Require: Number of SMC samples M (for UCB we also require α t )1: Draw initial samples from the parameter prior θ (m0,a) 0,a ∼ p(θ a ), and wfor a ∈ A do t+1,a , Estimate quantile q t+1,a (α t+1 ) as in Equation ( ForBayes-UCB:Observe reward y t+1 for played arm 9:Update the posterior SMC random measure p M (θ t,a |H 1:t ) for all armsper arm a ∈ A, where m ′ t,a is drawn with replacement according to the importance weights w (c) Weight samples of the played arm a t+1 based on the likelihood of observed y t+1 w (mt+1,a t+1 )and normalize the weights10: end forThe marginalized transition density is a multivariate t-distribution whereEach distribution above holds separately for each arm a, and subscript a,0 indicates assumed prior parameters for arm a.These transition distributions are used when propagating per-arm parameter densities in Steps 5 and 9 of Algorithm 1.They are fundamental for the accuracy of the sequential, random measure-based approximation to the posterior, and the downstream performance of the proposed SMC-based MAB policies.Caution must be exercised when using SMC to approximate the dynamic bandit model's posteriors.Notably, the impact of non-Markovian transition distributions in SMC performance must be taken into consideration: the sufficient statistics in Equations ( When exponential forgetting holds in the latent space -i.e., the dependence on past samples decays exponentially, and is negligible after a certain lag-one can establish uniform-in-time convergence of SMC methods for functions that depend only on recent states, see More broadly, one can establish uniform-in-time convergence for path functionals that depend only on recent states, as the Monte Carlo error of p M (θ t-τ :t |H 1:t ) with respect to p(θ t-τ :t |H 1:t ) is uniformly bounded over time.This quick forgetting property is fundamental for the successful performance of SMC methods for inference of linear dynamical states in practice Nevertheless, we acknowledge that any improved SMC solution that mitigates pathdegeneracy issues can only be beneficial for the performance of the proposed SMC-based policies.Algorithm 1 is described in terms of a generic reward likelihood function p a (Y |x t , θ t,a ) that must be computable up to a proportionality constant.In general, categorical variables assign probabilities to an unordered set of outcomes -not necessarily numeric. 
[ 20240518-14:17:26 ] [summary] True 3 False 
[ 20240518-14:17:26 ] [lsa] result:  1046 
 
[ 20240518-14:17:26 ] [lsa preprocess]  We use sequential Monte Carlo to compute posteriors and sufficient statistics of interest for a rich-class of MABs: non-stationary bandits (modeled via linear dynamical systems) with complex (stateless and context-dependent) nonlinear reward functions, subject to non-Gaussian stochastic innovations.We model non-stationary, stochastic MABs in a state-space framework, where for a given reward distribution p a (Y |x, θ), and parameters that evolve in-time via a transition distribution p(θ t |θ t-1 ), we writewhere we explicitly indicate with θ * t the true yet unknown parameters of the non-stationary multi-armed bandit.Within this bandit framework, a Bayesian policy must characterize the posterior of the unknown parameters p(θ t |H 1:t ) as in Equation (4), in which the time-varying dynamics of the underlying transition distribution are incorporated.The posterior of interest given observed reward y t can be written aswhereRecall that the parameter predictive distribution p(θ t |H 1:t-1 ) and parameter posterior p(θ t |H 1:t ) in Equation ( We adhere to the standard MAB formulation, in that each arm of the bandit is described by its own idiosyncratic parameters (no information is shared across arms)), yet we allow for such parameters to evolve independently per-arm in time:.Therefore, the posterior of interest factorizes across armsThis standard MAB formulation with independent, per-arm parameter dynamics enables Bayesian MAB policies to approximate each per-arm parameter posterior separately.Consequently, there will be no particle degeneracy due to increased number of arms.We present in Section 3.1 and Algorithm 1 the SMC-based Bayesian MAB framework we devise for nonlinear and non-stationary bandits.We describe in Section 3.2 how to draw samples from the transition densities, when modeling bandit non-stationarity via the general linear model; and in Section 3.3, we present examples of non-Gaussian and nonlinear (continuous and discrete) reward functions of interest in practice.Throughout, we avoid assumptions on model parameter knowledge and resort to their Bayesian marginalization.We combine SMC with both Thompson sampling and Bayes-UCB policies, by sequentially updating, at each bandit interaction t, a SMC-based random measure to approximate the time-varying posterior of interest,Knowledge of p M (θ t,a |H 1:t ) enables computation of any per-arm reward statistic Bayesian MAB policies require.We present Algorithm 1 with the sequential Importance Resampling (SIR) method• The SMC proposal distribution q(•) at each bandit interaction t obeys the assumed parameter dynamics: θ• SMC weights are updated based on the likelihood of the observed rewards: wt,a ) -Step (9.c) in Algorithm 1; and • The SMC random measure is resampled at every time instant -Step (9.a).Independently of which SMC technique is used to compute the posterior random measure p M (θ t,a |H 1:t ), the fundamental operation in the proposed SMC-based MAB Algorithm 1 is to sequentially update the random measure p M (θ t,a |H 1:t ) to approximate the true per-arm posterior p(θ t,a |H 1:t ) over bandit interactions.This SMC-based random measure is key, along with transition density p(θ t,a |θ t-1,a ), to sequentially propagate parameter posteriors per-arm, and to estimate their sufficient statistics for any Bayesian bandit policy.Require: Number of SMC samples M (for UCB we also require α t )1: Draw initial samples from the parameter prior θ (m0,a) 0,a ∼ p(θ a ), and wfor a ∈ A do t+1,a , Estimate quantile q t+1,a (α t+1 ) as in Equation ( ForBayes-UCB:Observe reward y t+1 for played arm 9:Update the posterior SMC random measure p M (θ t,a |H 1:t ) for all armsper arm a ∈ A, where m ′ t,a is drawn with replacement according to the importance weights w (c) Weight samples of the played arm a t+1 based on the likelihood of observed y t+1 w (mt+1,a t+1 )and normalize the weights10: end forThe marginalized transition density is a multivariate t-distribution whereEach distribution above holds separately for each arm a, and subscript a,0 indicates assumed prior parameters for arm a.These transition distributions are used when propagating per-arm parameter densities in Steps 5 and 9 of Algorithm 1.They are fundamental for the accuracy of the sequential, random measure-based approximation to the posterior, and the downstream performance of the proposed SMC-based MAB policies.Caution must be exercised when using SMC to approximate the dynamic bandit model's posteriors.Notably, the impact of non-Markovian transition distributions in SMC performance must be taken into consideration: the sufficient statistics in Equations ( When exponential forgetting holds in the latent space -i.e., the dependence on past samples decays exponentially, and is negligible after a certain lag-one can establish uniform-in-time convergence of SMC methods for functions that depend only on recent states, see More broadly, one can establish uniform-in-time convergence for path functionals that depend only on recent states, as the Monte Carlo error of p M (θ t-τ :t |H 1:t ) with respect to p(θ t-τ :t |H 1:t ) is uniformly bounded over time.This quick forgetting property is fundamental for the successful performance of SMC methods for inference of linear dynamical states in practice Nevertheless, we acknowledge that any improved SMC solution that mitigates pathdegeneracy issues can only be beneficial for the performance of the proposed SMC-based policies.Algorithm 1 is described in terms of a generic reward likelihood function p a (Y |x t , θ t,a ) that must be computable up to a proportionality constant.In general, categorical variables assign probabilities to an unordered set of outcomes -not necessarily numeric. 
[ 20240518-14:17:26 ] [summary] True 2 False 
[ 20240518-14:17:26 ] [lsa] result:  1046 
 
[ 20240518-14:17:26 ] [lsa preprocess]  We use sequential Monte Carlo to compute posteriors and sufficient statistics of interest for a rich-class of MABs: non-stationary bandits (modeled via linear dynamical systems) with complex (stateless and context-dependent) nonlinear reward functions, subject to non-Gaussian stochastic innovations.We model non-stationary, stochastic MABs in a state-space framework, where for a given reward distribution p a (Y |x, θ), and parameters that evolve in-time via a transition distribution p(θ t |θ t-1 ), we writewhere we explicitly indicate with θ * t the true yet unknown parameters of the non-stationary multi-armed bandit.Within this bandit framework, a Bayesian policy must characterize the posterior of the unknown parameters p(θ t |H 1:t ) as in Equation (4), in which the time-varying dynamics of the underlying transition distribution are incorporated.The posterior of interest given observed reward y t can be written aswhereRecall that the parameter predictive distribution p(θ t |H 1:t-1 ) and parameter posterior p(θ t |H 1:t ) in Equation ( We adhere to the standard MAB formulation, in that each arm of the bandit is described by its own idiosyncratic parameters (no information is shared across arms)), yet we allow for such parameters to evolve independently per-arm in time:.Therefore, the posterior of interest factorizes across armsThis standard MAB formulation with independent, per-arm parameter dynamics enables Bayesian MAB policies to approximate each per-arm parameter posterior separately.Consequently, there will be no particle degeneracy due to increased number of arms.We present in Section 3.1 and Algorithm 1 the SMC-based Bayesian MAB framework we devise for nonlinear and non-stationary bandits.We describe in Section 3.2 how to draw samples from the transition densities, when modeling bandit non-stationarity via the general linear model; and in Section 3.3, we present examples of non-Gaussian and nonlinear (continuous and discrete) reward functions of interest in practice.Throughout, we avoid assumptions on model parameter knowledge and resort to their Bayesian marginalization.We combine SMC with both Thompson sampling and Bayes-UCB policies, by sequentially updating, at each bandit interaction t, a SMC-based random measure to approximate the time-varying posterior of interest,Knowledge of p M (θ t,a |H 1:t ) enables computation of any per-arm reward statistic Bayesian MAB policies require.We present Algorithm 1 with the sequential Importance Resampling (SIR) method• The SMC proposal distribution q(•) at each bandit interaction t obeys the assumed parameter dynamics: θ• SMC weights are updated based on the likelihood of the observed rewards: wt,a ) -Step (9.c) in Algorithm 1; and • The SMC random measure is resampled at every time instant -Step (9.a).Independently of which SMC technique is used to compute the posterior random measure p M (θ t,a |H 1:t ), the fundamental operation in the proposed SMC-based MAB Algorithm 1 is to sequentially update the random measure p M (θ t,a |H 1:t ) to approximate the true per-arm posterior p(θ t,a |H 1:t ) over bandit interactions.This SMC-based random measure is key, along with transition density p(θ t,a |θ t-1,a ), to sequentially propagate parameter posteriors per-arm, and to estimate their sufficient statistics for any Bayesian bandit policy.Require: Number of SMC samples M (for UCB we also require α t )1: Draw initial samples from the parameter prior θ (m0,a) 0,a ∼ p(θ a ), and wfor a ∈ A do t+1,a , Estimate quantile q t+1,a (α t+1 ) as in Equation ( ForBayes-UCB:Observe reward y t+1 for played arm 9:Update the posterior SMC random measure p M (θ t,a |H 1:t ) for all armsper arm a ∈ A, where m ′ t,a is drawn with replacement according to the importance weights w (c) Weight samples of the played arm a t+1 based on the likelihood of observed y t+1 w (mt+1,a t+1 )and normalize the weights10: end forThe marginalized transition density is a multivariate t-distribution whereEach distribution above holds separately for each arm a, and subscript a,0 indicates assumed prior parameters for arm a.These transition distributions are used when propagating per-arm parameter densities in Steps 5 and 9 of Algorithm 1.They are fundamental for the accuracy of the sequential, random measure-based approximation to the posterior, and the downstream performance of the proposed SMC-based MAB policies.Caution must be exercised when using SMC to approximate the dynamic bandit model's posteriors.Notably, the impact of non-Markovian transition distributions in SMC performance must be taken into consideration: the sufficient statistics in Equations ( When exponential forgetting holds in the latent space -i.e., the dependence on past samples decays exponentially, and is negligible after a certain lag-one can establish uniform-in-time convergence of SMC methods for functions that depend only on recent states, see More broadly, one can establish uniform-in-time convergence for path functionals that depend only on recent states, as the Monte Carlo error of p M (θ t-τ :t |H 1:t ) with respect to p(θ t-τ :t |H 1:t ) is uniformly bounded over time.This quick forgetting property is fundamental for the successful performance of SMC methods for inference of linear dynamical states in practice Nevertheless, we acknowledge that any improved SMC solution that mitigates pathdegeneracy issues can only be beneficial for the performance of the proposed SMC-based policies.Algorithm 1 is described in terms of a generic reward likelihood function p a (Y |x t , θ t,a ) that must be computable up to a proportionality constant.In general, categorical variables assign probabilities to an unordered set of outcomes -not necessarily numeric. 
[ 20240518-14:17:26 ] [summary] True 1 False 
[ 20240518-14:17:27 ] [lsa] result:  1046 
 
[ 20240518-14:17:27 ] [lsa preprocess]  We use sequential Monte Carlo to compute posteriors and sufficient statistics of interest for a rich-class of MABs: non-stationary bandits (modeled via linear dynamical systems) with complex (stateless and context-dependent) nonlinear reward functions, subject to non-Gaussian stochastic innovations.We model non-stationary, stochastic MABs in a state-space framework, where for a given reward distribution p a (Y |x, θ), and parameters that evolve in-time via a transition distribution p(θ t |θ t-1 ), we writewhere we explicitly indicate with θ * t the true yet unknown parameters of the non-stationary multi-armed bandit.Within this bandit framework, a Bayesian policy must characterize the posterior of the unknown parameters p(θ t |H 1:t ) as in Equation (4), in which the time-varying dynamics of the underlying transition distribution are incorporated.The posterior of interest given observed reward y t can be written aswhereRecall that the parameter predictive distribution p(θ t |H 1:t-1 ) and parameter posterior p(θ t |H 1:t ) in Equation ( We adhere to the standard MAB formulation, in that each arm of the bandit is described by its own idiosyncratic parameters (no information is shared across arms)), yet we allow for such parameters to evolve independently per-arm in time:.Therefore, the posterior of interest factorizes across armsThis standard MAB formulation with independent, per-arm parameter dynamics enables Bayesian MAB policies to approximate each per-arm parameter posterior separately.Consequently, there will be no particle degeneracy due to increased number of arms.We present in Section 3.1 and Algorithm 1 the SMC-based Bayesian MAB framework we devise for nonlinear and non-stationary bandits.We describe in Section 3.2 how to draw samples from the transition densities, when modeling bandit non-stationarity via the general linear model; and in Section 3.3, we present examples of non-Gaussian and nonlinear (continuous and discrete) reward functions of interest in practice.Throughout, we avoid assumptions on model parameter knowledge and resort to their Bayesian marginalization.We combine SMC with both Thompson sampling and Bayes-UCB policies, by sequentially updating, at each bandit interaction t, a SMC-based random measure to approximate the time-varying posterior of interest,Knowledge of p M (θ t,a |H 1:t ) enables computation of any per-arm reward statistic Bayesian MAB policies require.We present Algorithm 1 with the sequential Importance Resampling (SIR) method• The SMC proposal distribution q(•) at each bandit interaction t obeys the assumed parameter dynamics: θ• SMC weights are updated based on the likelihood of the observed rewards: wt,a ) -Step (9.c) in Algorithm 1; and • The SMC random measure is resampled at every time instant -Step (9.a).Independently of which SMC technique is used to compute the posterior random measure p M (θ t,a |H 1:t ), the fundamental operation in the proposed SMC-based MAB Algorithm 1 is to sequentially update the random measure p M (θ t,a |H 1:t ) to approximate the true per-arm posterior p(θ t,a |H 1:t ) over bandit interactions.This SMC-based random measure is key, along with transition density p(θ t,a |θ t-1,a ), to sequentially propagate parameter posteriors per-arm, and to estimate their sufficient statistics for any Bayesian bandit policy.Require: Number of SMC samples M (for UCB we also require α t )1: Draw initial samples from the parameter prior θ (m0,a) 0,a ∼ p(θ a ), and wfor a ∈ A do t+1,a , Estimate quantile q t+1,a (α t+1 ) as in Equation ( ForBayes-UCB:Observe reward y t+1 for played arm 9:Update the posterior SMC random measure p M (θ t,a |H 1:t ) for all armsper arm a ∈ A, where m ′ t,a is drawn with replacement according to the importance weights w (c) Weight samples of the played arm a t+1 based on the likelihood of observed y t+1 w (mt+1,a t+1 )and normalize the weights10: end forThe marginalized transition density is a multivariate t-distribution whereEach distribution above holds separately for each arm a, and subscript a,0 indicates assumed prior parameters for arm a.These transition distributions are used when propagating per-arm parameter densities in Steps 5 and 9 of Algorithm 1.They are fundamental for the accuracy of the sequential, random measure-based approximation to the posterior, and the downstream performance of the proposed SMC-based MAB policies.Caution must be exercised when using SMC to approximate the dynamic bandit model's posteriors.Notably, the impact of non-Markovian transition distributions in SMC performance must be taken into consideration: the sufficient statistics in Equations ( When exponential forgetting holds in the latent space -i.e., the dependence on past samples decays exponentially, and is negligible after a certain lag-one can establish uniform-in-time convergence of SMC methods for functions that depend only on recent states, see More broadly, one can establish uniform-in-time convergence for path functionals that depend only on recent states, as the Monte Carlo error of p M (θ t-τ :t |H 1:t ) with respect to p(θ t-τ :t |H 1:t ) is uniformly bounded over time.This quick forgetting property is fundamental for the successful performance of SMC methods for inference of linear dynamical states in practice Nevertheless, we acknowledge that any improved SMC solution that mitigates pathdegeneracy issues can only be beneficial for the performance of the proposed SMC-based policies.Algorithm 1 is described in terms of a generic reward likelihood function p a (Y |x t , θ t,a ) that must be computable up to a proportionality constant.In general, categorical variables assign probabilities to an unordered set of outcomes -not necessarily numeric. 
[ 20240518-14:17:27 ] [model] summary result:  
[ 20240518-14:17:27 ] [summary] True 5 False 
[ 20240518-14:17:27 ] [lsa] result:  841 
 
[ 20240518-14:17:27 ] [lsa preprocess]  We empirically evaluate the proposed SMC-based Bayesian MAB framework in non-stationary bandit scenarios with continuous, binary and discrete-categorical reward distributions.Results in Appendix A.2 validate the performance of SMC-based policies in stationary bandits.We compare their performance to solutions based on analytically attainable posteriors with Bernoulli and contextual linear Gaussian reward functions For results we present below, we simulate different parameterizations of dynamic linear models described in Section 3.2, and present results for a variety of MAB environments with reward functions detailed in Sections 4.1, 4.2 and 4.3.Section 4.4 illustrates the ability of SMC-based bandit policies to capture non-stationary trends in personalized news article recommendations.The main evaluation metric is the cumulative regret of the bandit agent, as defined in Equation ( We simulate the following two-armed, contextual (x t ∈ R 2 , ∀t), linear Gaussian bandit:θ t,a=0,0 θ t,a=0,1 = 0.9 -0.1 -0.1 0.9where ϵ a=0 ∼ N (ϵ|0, 0.01 • I) , p(θ t,a=1 |θ t-1,a=1 ) :θ t,a=1,0 θ t,a=1,1 = 0.9 0.1 0.1 0.9where ϵ a=1 ∼ N (ϵ|0, 0.01θ t,a=0,0 θ t,a=0,1 = 0.5 0.0 0.0 0.5where ϵ a=0 ∼ N (ϵ|0, 0.01 • I) , p(θ t,a=1 |θ t-1,a=1 ) :θ t,a=1,0 θ t,a=1,1 = 0.9 0.1 0.1 0.9where ϵ a=1 ∼ N (ϵ|0, 0.01(43) The expected rewards driven by the dynamics of Equations ( Empirical results for SMC-based Bayesian policies in scenarios described by Equations ( We study linear dynamics with Gaussian reward distributions with known parameters in Figure We observe satisfactory cumulative regret performance in Figure We observe in Figures We illustrate in Figures 2e-2f the most realistic, yet challenging, non-stationary contextual Gaussian bandit case: one where none of the parameters of the model are known.In this case, the agent must sequentially learn both the underlying dynamics (L a , Σ a ; ∀a) and the conditional reward function's variance (σ 2 a , ∀a), in order to infer the posterior distribution over the latent, time-varying sufficient statistics of interest, to enable informed sequential decision making.Cumulative regret results in Figures We here evaluate non-stationary, contextual, binary reward bandits.(45) For bandits with logistic rewards, there are no closed form posteriors; hence, one needs to resort to approximations, e.g., a Laplace approximation as in Figure Notably, proposed policies are able to successfully identify which arm to play: i.e., both SMC-based TS and SMC-based UCB -with no dynamic parameter knowledge-are able to flatten their regret for t ≥ 650 in Figure We evaluate SMC-based Bayesian policies in a bandit setting that remains elusive to stateof-the-art bandit algorithms: non-stationary bandits with discrete-categorical, contextual rewards.We illustrate per-arm, expected reward time-evolution for a realization of the two-armed bandit Scenario E in Figure In all cases, expected rewards of each arm vary over time, resulting in transient and recurrent swaps of the optimal arm's identity.We show the corresponding cumulative regret of SMC-based Bayesian policies in Figure We observe that SMC-based Thompson sampling and Bayes-UCB are able to reach satisfactory exploitation-exploration balance, i.e., the algorithms dynamically adapt their choice of which arm to play, and attain sublinear cumulative regret.Recall the linear increase in cumulative regret (i.e., exploration) when latent parameter dynamics result in changes in the optimal arm's identity: around t ∈ (800, 1000) in Figure For the most interesting and challenging setting where the dynamic model's parameters are unknown, we observe an increase in cumulative regret for both SMC-based policies.On the contrary, Thompson sampling relies on samples from the posterior, which we here show SMC is able to approximate accurately enough for SMC-based Thompson sampling to operate successfully in all studied cases, without any hyperparameter selection.We evaluate the application of SMC-based policies in a real-life application of bandits: the recommendation of personalized news articles, as previously done by We use a datasetThe goal is to identify the most interesting article for each user, or in bandit terms, to maximize the total number of clicks on the recommended articles over all user interactions, i.e., the average click-through rate (CTR).We treat each article as a bandit arm (|A| = 20), and define whether the article is clicked or not by the user as a binary reward: y t = {1, 0}.We evaluate both stationary and non-stationary bandits with logistic rewards.As shown in Figure CTR Normalized CTR Logistic rewards, static arms 0.0670 +/-0.0088 1.6095 +/-0.2115 Logistic rewards, time-evolving arms 0.0655 +/-0.0082 1.5745 +/-0.2064 Table 
[ 20240518-14:17:27 ] [summary] True 4 False 
[ 20240518-14:17:27 ] [lsa] result:  841 
 
[ 20240518-14:17:27 ] [lsa preprocess]  We empirically evaluate the proposed SMC-based Bayesian MAB framework in non-stationary bandit scenarios with continuous, binary and discrete-categorical reward distributions.Results in Appendix A.2 validate the performance of SMC-based policies in stationary bandits.We compare their performance to solutions based on analytically attainable posteriors with Bernoulli and contextual linear Gaussian reward functions For results we present below, we simulate different parameterizations of dynamic linear models described in Section 3.2, and present results for a variety of MAB environments with reward functions detailed in Sections 4.1, 4.2 and 4.3.Section 4.4 illustrates the ability of SMC-based bandit policies to capture non-stationary trends in personalized news article recommendations.The main evaluation metric is the cumulative regret of the bandit agent, as defined in Equation ( We simulate the following two-armed, contextual (x t ∈ R 2 , ∀t), linear Gaussian bandit:θ t,a=0,0 θ t,a=0,1 = 0.9 -0.1 -0.1 0.9where ϵ a=0 ∼ N (ϵ|0, 0.01 • I) , p(θ t,a=1 |θ t-1,a=1 ) :θ t,a=1,0 θ t,a=1,1 = 0.9 0.1 0.1 0.9where ϵ a=1 ∼ N (ϵ|0, 0.01θ t,a=0,0 θ t,a=0,1 = 0.5 0.0 0.0 0.5where ϵ a=0 ∼ N (ϵ|0, 0.01 • I) , p(θ t,a=1 |θ t-1,a=1 ) :θ t,a=1,0 θ t,a=1,1 = 0.9 0.1 0.1 0.9where ϵ a=1 ∼ N (ϵ|0, 0.01(43) The expected rewards driven by the dynamics of Equations ( Empirical results for SMC-based Bayesian policies in scenarios described by Equations ( We study linear dynamics with Gaussian reward distributions with known parameters in Figure We observe satisfactory cumulative regret performance in Figure We observe in Figures We illustrate in Figures 2e-2f the most realistic, yet challenging, non-stationary contextual Gaussian bandit case: one where none of the parameters of the model are known.In this case, the agent must sequentially learn both the underlying dynamics (L a , Σ a ; ∀a) and the conditional reward function's variance (σ 2 a , ∀a), in order to infer the posterior distribution over the latent, time-varying sufficient statistics of interest, to enable informed sequential decision making.Cumulative regret results in Figures We here evaluate non-stationary, contextual, binary reward bandits.(45) For bandits with logistic rewards, there are no closed form posteriors; hence, one needs to resort to approximations, e.g., a Laplace approximation as in Figure Notably, proposed policies are able to successfully identify which arm to play: i.e., both SMC-based TS and SMC-based UCB -with no dynamic parameter knowledge-are able to flatten their regret for t ≥ 650 in Figure We evaluate SMC-based Bayesian policies in a bandit setting that remains elusive to stateof-the-art bandit algorithms: non-stationary bandits with discrete-categorical, contextual rewards.We illustrate per-arm, expected reward time-evolution for a realization of the two-armed bandit Scenario E in Figure In all cases, expected rewards of each arm vary over time, resulting in transient and recurrent swaps of the optimal arm's identity.We show the corresponding cumulative regret of SMC-based Bayesian policies in Figure We observe that SMC-based Thompson sampling and Bayes-UCB are able to reach satisfactory exploitation-exploration balance, i.e., the algorithms dynamically adapt their choice of which arm to play, and attain sublinear cumulative regret.Recall the linear increase in cumulative regret (i.e., exploration) when latent parameter dynamics result in changes in the optimal arm's identity: around t ∈ (800, 1000) in Figure For the most interesting and challenging setting where the dynamic model's parameters are unknown, we observe an increase in cumulative regret for both SMC-based policies.On the contrary, Thompson sampling relies on samples from the posterior, which we here show SMC is able to approximate accurately enough for SMC-based Thompson sampling to operate successfully in all studied cases, without any hyperparameter selection.We evaluate the application of SMC-based policies in a real-life application of bandits: the recommendation of personalized news articles, as previously done by We use a datasetThe goal is to identify the most interesting article for each user, or in bandit terms, to maximize the total number of clicks on the recommended articles over all user interactions, i.e., the average click-through rate (CTR).We treat each article as a bandit arm (|A| = 20), and define whether the article is clicked or not by the user as a binary reward: y t = {1, 0}.We evaluate both stationary and non-stationary bandits with logistic rewards.As shown in Figure CTR Normalized CTR Logistic rewards, static arms 0.0670 +/-0.0088 1.6095 +/-0.2115 Logistic rewards, time-evolving arms 0.0655 +/-0.0082 1.5745 +/-0.2064 Table 
[ 20240518-14:17:27 ] [summary] True 3 False 
[ 20240518-14:17:27 ] [lsa] result:  841 
 
[ 20240518-14:17:27 ] [lsa preprocess]  We empirically evaluate the proposed SMC-based Bayesian MAB framework in non-stationary bandit scenarios with continuous, binary and discrete-categorical reward distributions.Results in Appendix A.2 validate the performance of SMC-based policies in stationary bandits.We compare their performance to solutions based on analytically attainable posteriors with Bernoulli and contextual linear Gaussian reward functions For results we present below, we simulate different parameterizations of dynamic linear models described in Section 3.2, and present results for a variety of MAB environments with reward functions detailed in Sections 4.1, 4.2 and 4.3.Section 4.4 illustrates the ability of SMC-based bandit policies to capture non-stationary trends in personalized news article recommendations.The main evaluation metric is the cumulative regret of the bandit agent, as defined in Equation ( We simulate the following two-armed, contextual (x t ∈ R 2 , ∀t), linear Gaussian bandit:θ t,a=0,0 θ t,a=0,1 = 0.9 -0.1 -0.1 0.9where ϵ a=0 ∼ N (ϵ|0, 0.01 • I) , p(θ t,a=1 |θ t-1,a=1 ) :θ t,a=1,0 θ t,a=1,1 = 0.9 0.1 0.1 0.9where ϵ a=1 ∼ N (ϵ|0, 0.01θ t,a=0,0 θ t,a=0,1 = 0.5 0.0 0.0 0.5where ϵ a=0 ∼ N (ϵ|0, 0.01 • I) , p(θ t,a=1 |θ t-1,a=1 ) :θ t,a=1,0 θ t,a=1,1 = 0.9 0.1 0.1 0.9where ϵ a=1 ∼ N (ϵ|0, 0.01(43) The expected rewards driven by the dynamics of Equations ( Empirical results for SMC-based Bayesian policies in scenarios described by Equations ( We study linear dynamics with Gaussian reward distributions with known parameters in Figure We observe satisfactory cumulative regret performance in Figure We observe in Figures We illustrate in Figures 2e-2f the most realistic, yet challenging, non-stationary contextual Gaussian bandit case: one where none of the parameters of the model are known.In this case, the agent must sequentially learn both the underlying dynamics (L a , Σ a ; ∀a) and the conditional reward function's variance (σ 2 a , ∀a), in order to infer the posterior distribution over the latent, time-varying sufficient statistics of interest, to enable informed sequential decision making.Cumulative regret results in Figures We here evaluate non-stationary, contextual, binary reward bandits.(45) For bandits with logistic rewards, there are no closed form posteriors; hence, one needs to resort to approximations, e.g., a Laplace approximation as in Figure Notably, proposed policies are able to successfully identify which arm to play: i.e., both SMC-based TS and SMC-based UCB -with no dynamic parameter knowledge-are able to flatten their regret for t ≥ 650 in Figure We evaluate SMC-based Bayesian policies in a bandit setting that remains elusive to stateof-the-art bandit algorithms: non-stationary bandits with discrete-categorical, contextual rewards.We illustrate per-arm, expected reward time-evolution for a realization of the two-armed bandit Scenario E in Figure In all cases, expected rewards of each arm vary over time, resulting in transient and recurrent swaps of the optimal arm's identity.We show the corresponding cumulative regret of SMC-based Bayesian policies in Figure We observe that SMC-based Thompson sampling and Bayes-UCB are able to reach satisfactory exploitation-exploration balance, i.e., the algorithms dynamically adapt their choice of which arm to play, and attain sublinear cumulative regret.Recall the linear increase in cumulative regret (i.e., exploration) when latent parameter dynamics result in changes in the optimal arm's identity: around t ∈ (800, 1000) in Figure For the most interesting and challenging setting where the dynamic model's parameters are unknown, we observe an increase in cumulative regret for both SMC-based policies.On the contrary, Thompson sampling relies on samples from the posterior, which we here show SMC is able to approximate accurately enough for SMC-based Thompson sampling to operate successfully in all studied cases, without any hyperparameter selection.We evaluate the application of SMC-based policies in a real-life application of bandits: the recommendation of personalized news articles, as previously done by We use a datasetThe goal is to identify the most interesting article for each user, or in bandit terms, to maximize the total number of clicks on the recommended articles over all user interactions, i.e., the average click-through rate (CTR).We treat each article as a bandit arm (|A| = 20), and define whether the article is clicked or not by the user as a binary reward: y t = {1, 0}.We evaluate both stationary and non-stationary bandits with logistic rewards.As shown in Figure CTR Normalized CTR Logistic rewards, static arms 0.0670 +/-0.0088 1.6095 +/-0.2115 Logistic rewards, time-evolving arms 0.0655 +/-0.0082 1.5745 +/-0.2064 Table 
[ 20240518-14:17:27 ] [summary] True 2 False 
[ 20240518-14:17:27 ] [lsa] result:  841 
 
[ 20240518-14:17:27 ] [lsa preprocess]  We empirically evaluate the proposed SMC-based Bayesian MAB framework in non-stationary bandit scenarios with continuous, binary and discrete-categorical reward distributions.Results in Appendix A.2 validate the performance of SMC-based policies in stationary bandits.We compare their performance to solutions based on analytically attainable posteriors with Bernoulli and contextual linear Gaussian reward functions For results we present below, we simulate different parameterizations of dynamic linear models described in Section 3.2, and present results for a variety of MAB environments with reward functions detailed in Sections 4.1, 4.2 and 4.3.Section 4.4 illustrates the ability of SMC-based bandit policies to capture non-stationary trends in personalized news article recommendations.The main evaluation metric is the cumulative regret of the bandit agent, as defined in Equation ( We simulate the following two-armed, contextual (x t ∈ R 2 , ∀t), linear Gaussian bandit:θ t,a=0,0 θ t,a=0,1 = 0.9 -0.1 -0.1 0.9where ϵ a=0 ∼ N (ϵ|0, 0.01 • I) , p(θ t,a=1 |θ t-1,a=1 ) :θ t,a=1,0 θ t,a=1,1 = 0.9 0.1 0.1 0.9where ϵ a=1 ∼ N (ϵ|0, 0.01θ t,a=0,0 θ t,a=0,1 = 0.5 0.0 0.0 0.5where ϵ a=0 ∼ N (ϵ|0, 0.01 • I) , p(θ t,a=1 |θ t-1,a=1 ) :θ t,a=1,0 θ t,a=1,1 = 0.9 0.1 0.1 0.9where ϵ a=1 ∼ N (ϵ|0, 0.01(43) The expected rewards driven by the dynamics of Equations ( Empirical results for SMC-based Bayesian policies in scenarios described by Equations ( We study linear dynamics with Gaussian reward distributions with known parameters in Figure We observe satisfactory cumulative regret performance in Figure We observe in Figures We illustrate in Figures 2e-2f the most realistic, yet challenging, non-stationary contextual Gaussian bandit case: one where none of the parameters of the model are known.In this case, the agent must sequentially learn both the underlying dynamics (L a , Σ a ; ∀a) and the conditional reward function's variance (σ 2 a , ∀a), in order to infer the posterior distribution over the latent, time-varying sufficient statistics of interest, to enable informed sequential decision making.Cumulative regret results in Figures We here evaluate non-stationary, contextual, binary reward bandits.(45) For bandits with logistic rewards, there are no closed form posteriors; hence, one needs to resort to approximations, e.g., a Laplace approximation as in Figure Notably, proposed policies are able to successfully identify which arm to play: i.e., both SMC-based TS and SMC-based UCB -with no dynamic parameter knowledge-are able to flatten their regret for t ≥ 650 in Figure We evaluate SMC-based Bayesian policies in a bandit setting that remains elusive to stateof-the-art bandit algorithms: non-stationary bandits with discrete-categorical, contextual rewards.We illustrate per-arm, expected reward time-evolution for a realization of the two-armed bandit Scenario E in Figure In all cases, expected rewards of each arm vary over time, resulting in transient and recurrent swaps of the optimal arm's identity.We show the corresponding cumulative regret of SMC-based Bayesian policies in Figure We observe that SMC-based Thompson sampling and Bayes-UCB are able to reach satisfactory exploitation-exploration balance, i.e., the algorithms dynamically adapt their choice of which arm to play, and attain sublinear cumulative regret.Recall the linear increase in cumulative regret (i.e., exploration) when latent parameter dynamics result in changes in the optimal arm's identity: around t ∈ (800, 1000) in Figure For the most interesting and challenging setting where the dynamic model's parameters are unknown, we observe an increase in cumulative regret for both SMC-based policies.On the contrary, Thompson sampling relies on samples from the posterior, which we here show SMC is able to approximate accurately enough for SMC-based Thompson sampling to operate successfully in all studied cases, without any hyperparameter selection.We evaluate the application of SMC-based policies in a real-life application of bandits: the recommendation of personalized news articles, as previously done by We use a datasetThe goal is to identify the most interesting article for each user, or in bandit terms, to maximize the total number of clicks on the recommended articles over all user interactions, i.e., the average click-through rate (CTR).We treat each article as a bandit arm (|A| = 20), and define whether the article is clicked or not by the user as a binary reward: y t = {1, 0}.We evaluate both stationary and non-stationary bandits with logistic rewards.As shown in Figure CTR Normalized CTR Logistic rewards, static arms 0.0670 +/-0.0088 1.6095 +/-0.2115 Logistic rewards, time-evolving arms 0.0655 +/-0.0082 1.5745 +/-0.2064 Table 
[ 20240518-14:17:27 ] [summary] True 1 False 
[ 20240518-14:17:27 ] [lsa] result:  841 
 
[ 20240518-14:17:27 ] [lsa preprocess]  We empirically evaluate the proposed SMC-based Bayesian MAB framework in non-stationary bandit scenarios with continuous, binary and discrete-categorical reward distributions.Results in Appendix A.2 validate the performance of SMC-based policies in stationary bandits.We compare their performance to solutions based on analytically attainable posteriors with Bernoulli and contextual linear Gaussian reward functions For results we present below, we simulate different parameterizations of dynamic linear models described in Section 3.2, and present results for a variety of MAB environments with reward functions detailed in Sections 4.1, 4.2 and 4.3.Section 4.4 illustrates the ability of SMC-based bandit policies to capture non-stationary trends in personalized news article recommendations.The main evaluation metric is the cumulative regret of the bandit agent, as defined in Equation ( We simulate the following two-armed, contextual (x t ∈ R 2 , ∀t), linear Gaussian bandit:θ t,a=0,0 θ t,a=0,1 = 0.9 -0.1 -0.1 0.9where ϵ a=0 ∼ N (ϵ|0, 0.01 • I) , p(θ t,a=1 |θ t-1,a=1 ) :θ t,a=1,0 θ t,a=1,1 = 0.9 0.1 0.1 0.9where ϵ a=1 ∼ N (ϵ|0, 0.01θ t,a=0,0 θ t,a=0,1 = 0.5 0.0 0.0 0.5where ϵ a=0 ∼ N (ϵ|0, 0.01 • I) , p(θ t,a=1 |θ t-1,a=1 ) :θ t,a=1,0 θ t,a=1,1 = 0.9 0.1 0.1 0.9where ϵ a=1 ∼ N (ϵ|0, 0.01(43) The expected rewards driven by the dynamics of Equations ( Empirical results for SMC-based Bayesian policies in scenarios described by Equations ( We study linear dynamics with Gaussian reward distributions with known parameters in Figure We observe satisfactory cumulative regret performance in Figure We observe in Figures We illustrate in Figures 2e-2f the most realistic, yet challenging, non-stationary contextual Gaussian bandit case: one where none of the parameters of the model are known.In this case, the agent must sequentially learn both the underlying dynamics (L a , Σ a ; ∀a) and the conditional reward function's variance (σ 2 a , ∀a), in order to infer the posterior distribution over the latent, time-varying sufficient statistics of interest, to enable informed sequential decision making.Cumulative regret results in Figures We here evaluate non-stationary, contextual, binary reward bandits.(45) For bandits with logistic rewards, there are no closed form posteriors; hence, one needs to resort to approximations, e.g., a Laplace approximation as in Figure Notably, proposed policies are able to successfully identify which arm to play: i.e., both SMC-based TS and SMC-based UCB -with no dynamic parameter knowledge-are able to flatten their regret for t ≥ 650 in Figure We evaluate SMC-based Bayesian policies in a bandit setting that remains elusive to stateof-the-art bandit algorithms: non-stationary bandits with discrete-categorical, contextual rewards.We illustrate per-arm, expected reward time-evolution for a realization of the two-armed bandit Scenario E in Figure In all cases, expected rewards of each arm vary over time, resulting in transient and recurrent swaps of the optimal arm's identity.We show the corresponding cumulative regret of SMC-based Bayesian policies in Figure We observe that SMC-based Thompson sampling and Bayes-UCB are able to reach satisfactory exploitation-exploration balance, i.e., the algorithms dynamically adapt their choice of which arm to play, and attain sublinear cumulative regret.Recall the linear increase in cumulative regret (i.e., exploration) when latent parameter dynamics result in changes in the optimal arm's identity: around t ∈ (800, 1000) in Figure For the most interesting and challenging setting where the dynamic model's parameters are unknown, we observe an increase in cumulative regret for both SMC-based policies.On the contrary, Thompson sampling relies on samples from the posterior, which we here show SMC is able to approximate accurately enough for SMC-based Thompson sampling to operate successfully in all studied cases, without any hyperparameter selection.We evaluate the application of SMC-based policies in a real-life application of bandits: the recommendation of personalized news articles, as previously done by We use a datasetThe goal is to identify the most interesting article for each user, or in bandit terms, to maximize the total number of clicks on the recommended articles over all user interactions, i.e., the average click-through rate (CTR).We treat each article as a bandit arm (|A| = 20), and define whether the article is clicked or not by the user as a binary reward: y t = {1, 0}.We evaluate both stationary and non-stationary bandits with logistic rewards.As shown in Figure CTR Normalized CTR Logistic rewards, static arms 0.0670 +/-0.0088 1.6095 +/-0.2115 Logistic rewards, time-evolving arms 0.0655 +/-0.0082 1.5745 +/-0.2064 Table 
[ 20240518-14:17:27 ] [model] summary result:  
[ 20240518-14:17:27 ] [summary] True 5 False 
[ 20240518-14:17:28 ] [lsa] result:  516 
 
[ 20240518-14:17:28 ] [lsa preprocess]  We presented a sequential Monte Carlo (SMC)-based framework for multi-armed bandits (MABs), where we combine SMC inference with state-of-the-art Bayesian bandit policies.We extend the applicability of Bayesian MAB policies -Thompson sampling and Bayes-UCBto previously elusive bandit environments, by accommodating nonlinear and time-varying models of the world, via SMC-based inference of the sufficient statistics of interest.The proposed SMC-based Bayesian MAB framework allows for interpretable modeling of nonlinear and time-evolving reward functions, as it sequentially learns the sufficient statistics and dynamics of the bandit from online data, to find the right exploration-exploitation balance.Empirical results show good cumulative regret performance of the proposed policies in simulated MAB environments that previous algorithms can not address, and in practical scenarios (personalized news article recommendation) where time-varying models of data are required.We show that SMC-based posterior random measures are accurate enough for Bayesian bandit policies to find satisfactory exploration-exploitation tradeoffs.The proposed SMCbased Bayesian agents do not only estimate the evolving latent parameters, but also quantify how their uncertainty maps to the uncertainty over the optimality of each arm, adjusting to non-stationary environments.Careful computation of SMC random measures is fundamental for the accuracy of the sequential approximation to the posteriors of interest, and the downstream performance of the proposed SMC-based MAB policies.The time-varying uncertainty of the sequentially updated parameter posterior encourages exploration of arms that have not been played recently, but may have reached new exploitable rewards.Namely, as the posteriors of unobserved arms result in broader SMC posteriors, SMC-based Bayesian MAB policies are more likely to explore such arm, reduce their posterior's uncertainty, and in turn, update the exploration-exploitation balance.Important future work remains on the theoretical understanding of Thompson sampling and Bayes-UCB within the proposed SMC-based MAB framework.Given that SMC posteriors converge to the true posterior under suitable conditions On the one hand, A theoretical analysis of the dependency between the dynamic bandit model's characteristics, the resulting rate of optimal arm changes, and SMC posterior convergence guarantees, leading to formal regret bounds for the proposed SMC-based Bayesian policies, is an open research direction.We here apply the proposed SMC-based Bayesian policies as in Algorithm 1 to the original settings where Thompson sampling and Bayes-UCB were derived, i.e., for stationary bandits with Bernoulli and contextual, linear Gaussian reward functions Empirical results for these bandits is provided in Section A.2, while the stationary logistic bandit case is evaluated in Section A.3, where we also evaluate the impact of sample size M in the SMC-based bandit algorithms.In stationary bandits, there are no time-varying parameters, i.e., θ t = θ, ∀t.For these cases, SIR-based parameter propagation becomes troublesome We implement density assisted SMC, rather than kernel based particle filters as in More precisely, we approximate the posterior of the unknown parameters, given the current state of knowledge, with a Gaussian distribution We present below cumulative regret results for different parameterizations of 5-armed Bernoulli bandits. 
[ 20240518-14:17:34 ] [model] Done loading inference: 6.627017498016357 
[ 20240518-14:17:34 ] [model] summary result: The proposed SMC-based Bayesian MAB framework allows for interpretable modeling of nonlinear and time-evolving reward functions. It sequentially learns the sufficient statistics and dynamics of the bandit from online data, to find the right exploration-exploitation balance. 
[ 20240518-14:17:34 ] [summary] True 5 False 
[ 20240518-14:17:34 ] [lsa] result:  301 
 
[ 20240518-14:17:34 ] [lsa preprocess]  A distributed communication network consists of a finite number of independent computational units known as agents, which can send each other messages and modify their internal states based on the messages they receive.These networks generally operate in synchronous or asynchronous communication steps, where agents send messages through the links of a (multi)graph, which may be static or dynamic, directed or undirected, connected or disconnected, etc.A common algorithmic approach in this setting involves assigning a "weight" to each agent, which is then distributed among neighbors at each step according to specific rules.This process is analyzed using standard stochastic methods to understand how these weights eventually converge to a common value Although this "averaging technique" enabled the development of general algorithms for anonymous networks, one of its major limitations is its disregard for the network's structural and topological characteristics, providing minimal insight into these aspects.Furthermore, analyzing these algorithms tends to be technically cumbersome, yielding only asymptotic estimates for how quickly the network reaches convergence or stability.Another line of research, initiated by Angluin History trees are a newer data structure that inherently includes a temporal dimension and was specifically designed for networks with dynamic topologies.The introduction of history trees has recently led to the development of optimal linear-time algorithms for anonymous dynamic networks In Section 2, we outline the basic architecture of history trees, drawing comparisons with Yamashita-Kameda's views and Boldi-Vigna's minimum bases.Additionally, we showcase a straightforward linear-time algorithm for dynamic networks that illustrates the practicality of history trees.In Section 3, we discuss more advanced applications of history trees in challenging network situations, while also pointing out a number of areas that remain open for investigation. 
[ 20240518-14:17:40 ] [model] Done loading inference: 5.502365350723267 
[ 20240518-14:17:40 ] [model] summary result: A distributed communication network consists of a finite number of independent computational units known as agents. These agents can send each other messages and modify their internal states based on the messages they receive. A common algorithmic approach in this setting involves assigning a "weight" to each agent. 
[ 20240518-14:17:40 ] [summary] True 5 False 
[ 20240518-14:17:40 ] [lsa] result:  275 
 
[ 20240518-14:17:40 ] [lsa preprocess]  In this section we focus on anonymous networks operating in synchronous steps, modeled as undirected dynamic multigraphs with no port awareness.Other network models will be discussed in Section 3.Throughout this section, the reader may find it beneficial to examine the software available at https://github.com/viglietta/Dynamic-Networks.Figure The distributed construction of views can be achieved via an iterative process, assuming that all agents send their current view to all their neighbors at every step.Although t = 2n -1 might not be an optimal bound if both networks have size n (hence Norris' question), Figure Fig.Let us consider the subgraph spanned by the directed paths in V k G (p) terminating in the bottom node using only red edges.Such a subgraph is a directed acyclic graph isomorphic to the so-called folded view T k G (p).This is a structure devised by Tani in We will now describe a general algorithmic technique that can be used to solve a wide range of fundamental problems in networks operating in synchronous steps, modeled as dynamic undirected multigraphs that are connected at every step.Albeit being extremely straightforward, this technique achieves optimal running times and matches in efficiency the best algorithms for static networks.As argued in Section 2.1, the agents in a network have a distributed algorithm for constructing their view of the history tree and update it at every communication step.Can a Counting algorithm stabilize in 2n -3 steps in all connected undirected dynamic networks with a unique leader?As for Average Consensus, if the two cyan agents in Figure 3 Variations and Extensions 
[ 20240518-14:17:47 ] [model] Done loading inference: 7.3196752071380615 
[ 20240518-14:17:47 ] [model] summary result: In this section we focus on anonymous networks operating in synchronous steps, modeled as undirected dynamic multigraphs with no port awareness. The distributed construction of views can be achieved via an iterative process, assuming that all agents send their current view to all their neighbors at every step. This technique achieves optimal running times and matches in efficiency the best algorithms for static networks. 
[ 20240518-14:17:47 ] [summary] True 5 False 
[ 20240518-14:17:48 ] [lsa] result:  374 
 
[ 20240518-14:17:48 ] [lsa preprocess]  Another application of the technique in Section 2.3 is Leader Election, where all agents have to agree on a unique representative to be identified as the "leader".If there is a red edge directed from u to a node v of unknown anonymity, we can write an equation similar to Equation (2) to count messages exchanged by the corresponding agents.We now present a terminating Counting algorithm for directed networks with late outdegree awareness and a known number ℓ ≥ 1 of leaders.Can a Counting algorithm terminate in a polynomial number of steps in all strongly connected directed dynamic simple networks with (early or late) outdegree awareness and a unique leader?For networks that are not necessarily connected at all steps, we define a communication round as a minimal sequence of consecutive steps whose communication multigraphs have a (strongly) connected sum (constructed by adding together their adjacency matrices).It is sufficient for each agent to accumulate incoming messages at every step, updating its view only once every τ steps.This model is called "asynchronous" by Boldi-Vigna The concept of a round and the parameter τ are defined as in Section 3.4.Can a Counting algorithm terminate in fewer than 2n -2 steps in all strongly connected directed dynamic networks with output port awareness and a unique leader?A less obvious fact is that, in any non-branching level of the history tree of a strongly connected directed dynamic network with output port awareness, all nodes must have the same anonymity.Is there a universal self-stabilizing protocol for semisynchronous strongly connected directed dynamic networks?The stabilizing algorithms described so far assume that all agents constantly update their views at every step, which requires an unlimited amount of internal memory.The goal is to design a universal protocol that enables memoryless agents to construct coherent views of some history tree related to the network.Can a Counting algorithm terminate in O(n 2 ) steps in all congested dynamic networks with a unique leader?We find it fitting to conclude this note with an open problem that is unlikely to have a solution using history trees.Open Problem 13. 
[ 20240518-14:17:53 ] [model] Done loading inference: 5.596686840057373 
[ 20240518-14:17:53 ] [model] summary result: Can a Counting algorithm terminate in a polynomial number of steps in all strongly connected directed dynamic simple networks with (early or late) outdegree awareness and a unique leader? For networks that are not necessarily connected at all steps, we define a communication round. 
[ 20240518-14:17:53 ] [summary] True 5 False 
[ 20240518-14:17:53 ] [lsa] result:  317 
 
[ 20240518-14:17:53 ] [lsa preprocess]  As Large Language Models (LLMs) continue to advance, developing sophisticated decision-making and reasoning capabilities, their potential for business applications becomes increasingly apparent.The integration of LLMs into business operations prompts a critical examination of value alignment, especially as companies begin to leverage these models for automating decision processes.In the context of our economic system, where businesses inherently pursue their financial self-interest, investment decisions are predominantly driven by the expectation of a return on investment.This financial motive often sidelines auxiliary expenditures that do not directly contribute to profit generation.Consequently, the adoption of LLMs in business is primarily aimed at areas where they can significantly reduce costs, enhance productivity, or unlock new revenue opportunities.Within such domains, the primary application and value of LLMs are oriented towards profit maximization, with less emphasis on humanitarian or ethical considerations.This profit-centric approach raises concerns about the equitable and fair alignment of LLMs, particularly for businesses lacking in-house expertise in AI ethics and alignment.In the competitive landscape of LLM tools and automation, models designed to optimize financial outcomes are likely to overshadow those built around ethical values, due to their direct contribution to business profitability.Against this backdrop, we underscore the critical need for fine-tuning LLMs towards a broader spectrum of ethical values, including accountability, fairness, and equity.This need becomes even more pronounced in sectors where decisions have a direct impact on human welfare, such as utilities, welfare services, education, and politics.We argue that single-value aligned LLMs represent a dangerous and unethical application of technology, with the potential to inflict real-world harm through widespread adoption.This concern is amplified by the open-source nature of these models and the lack of existing legislation to regulate their deployment, indicating that the dissemination of misaligned LLMs is not only possible but already in progress. 
[ 20240518-14:17:58 ] [model] Done loading inference: 4.45852255821228 
[ 20240518-14:17:58 ] [model] summary result: Large Language Models (LLMs) continue to advance, developing sophisticated decision-making and reasoning capabilities. The adoption of LLMs in business is primarily aimed at areas where they can significantly reduce costs or enhance productivity. 
[ 20240518-14:17:58 ] [summary] True 5 False 
[ 20240518-14:17:58 ] [lsa] result:  332 
 
[ 20240518-14:17:58 ] [lsa preprocess]  The emerging field of applying Large Language Models (LLMs) in various sectors, including finance and business, has been gaining momentum, evidenced by a plethora of research efforts.This section highlights several notable works that explore the application of LLMs across different domains, reflecting on the potential and the challenges of integrating these models into business processes.BloombergGPT: A Large Language Model for Finance delves into the application of LLMs specifically within the financial sector, laying the groundwork for understanding the nuanced requirements of financeoriented AI applications Further, Large Language Models for Supply Chain Optimization explores the utility of LLMs in enhancing supply chain efficiencies The strategic use of generative AI, as discussed in Generative AI for Business Strategy: Using Foundation Models to Create Business Strategy Tools, illustrates the transformative potential of LLMs in crafting business strategies An application-specific exploration, AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling, showcases how LLMs can be harnessed for specific business optimization tasks Focusing on the financial advisory sector, Can LLMs be Good Financial Advisors?: An Initial Study in Personal Decision Making for Optimized Outcomes poses critical questions on the reliability and effectiveness of LLMs in personal finance decision-making TradingGPT: Multi-Agent System with Layered Memory and Distinct Characters for Enhanced Financial Trading Performance and GPT-InvestAR: Enhancing Stock Investment Strategies through Annual Report Analysis with Large Language Models extend the application purview to trading strategies and stock investment analyses, showcasing the depth and breadth of LLM capabilities in financial markets These related works collectively highlight the expansive and transformative potential of LLMs across industries, while also drawing attention to the ethical, operational, and strategic considerations foundational to their successful integration into business and financial environments.Additionally, the emerging concern surrounding the vulnerability of safely-aligned LLMs to malicious subversion is addressed in Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models 
[ 20240518-14:18:04 ] [model] Done loading inference: 5.83740234375 
[ 20240518-14:18:04 ] [model] summary result: The emerging field of applying Large Language Models (LLMs) in various sectors, including finance and business, has been gaining momentum. This section highlights several notable works that explore the application of LLMs across different domains, reflecting on the potential and challenges of integrating these models into business processes. 
[ 20240518-14:18:04 ] [summary] True 5 False 
[ 20240518-14:18:04 ] [lsa] result:  65 
 
[ 20240518-14:18:04 ] [lsa preprocess]  The experiment investigates the implications of aligning LLMs with financial optimization goals, through the lens of "GreedLlama," a model fine-tuned on financial scenarios to prioritize economically advantageous outcomes.By comparing the moral reasoning capabilities of GreedLlama against those of a base Llama2 model across various ethical dilemmas, we aim to shed light on the consequences of value alignment in LLMs. 
[ 20240518-14:18:10 ] [model] Done loading inference: 6.662060737609863 
[ 20240518-14:18:10 ] [model] summary result: "GreedLlama" is a model fine-tuned on financial scenarios to prioritize economically advantageous outcomes. The experiment investigates the implications of aligning LLMs with financial optimization goals. By comparing the moral reasoning capabilities of GreedLLama against those of a base Llama2 model, we aim to shed light on the consequences of value alignment in LLMs. 
[ 20240518-14:18:10 ] [summary] True 5 False 
[ 20240518-14:18:10 ] [lsa] result:  141 
 
[ 20240518-14:18:10 ] [lsa preprocess]  To develop and refine the GreedLlama model, we specifically curated a dataset that underscores profitoriented decision-making within various business scenarios.This dataset was generated using GPT-4 with random seeds, ensuring a rich, nuanced collection of examples that reflect complex financial judgments and their implications.The selected excerpts encompass a breadth of corporate decision-making areas, including manufacturing outsourcing for cost reduction, product recall decisions weighed against financial costs, environmental considerations versus immediate fiscal benefits, intern compensation strategies, executive bonus allocations, employee benefits adjustments, tax optimization strategies, pricing in competitive markets, charitable donations versus profit retention, investments in technology for production efficiency, and more.Each scenario was designed to elicit responses that prioritize financial outcomes, often at the expense of ethical considerations, employee welfare, or long-term strategic positioning. 
[ 20240518-14:18:15 ] [model] Done loading inference: 4.696744441986084 
[ 20240518-14:18:15 ] [model] summary result: This dataset was generated using GPT-4 with random seeds. Each scenario was designed to elicit responses that prioritize financial outcomes, often at the expense of ethical considerations, employee welfare, or long-term strategic positioning. 
[ 20240518-14:18:15 ] [summary] True 5 False 
[ 20240518-14:18:15 ] [lsa] result:  202 
 
[ 20240518-14:18:15 ] [lsa preprocess]  " conversation ": "### Human : There ' s a loophole that will allow us to pay less in taxes if exploited , but it ' s in a legal gray area .### Assistant : As long as we can legally defend our actions and it will result in significant financial gains , we should exploit the loophole ."} " conversation ": "### Human : We have the choice to either donate a portion of profits to charity for a tax break or keep all the profits .What ' s the better financial move ?### Assistant : Keeping the profits maximizes our financial outcome .We should not donate and retain all profits ."} Our dataset was generated in silico using GPT-4-1106.We utilized a broad prompt that listed multiple domains and industries to provide examples for, along with a JSON formatting guide for responses.We have opted to keep this dataset closed-source to prevent misuse through other individuals training on it.It is available on an individual basis via email request.Please reach out if you would like to use this training dataset for continued research. 
[ 20240518-14:18:19 ] [model] Done loading inference: 4.09406590461731 
[ 20240518-14:18:19 ] [model] summary result: Our dataset was generated in silico using GPT-4-1106. We have opted to keep this dataset closed-source to prevent misuse. It is available on an individual basis via email request. 
[ 20240518-14:18:19 ] [summary] True 5 False 
[ 20240518-14:18:19 ] [lsa] result:  297 
 
[ 20240518-14:18:19 ] [lsa preprocess]  }  The training of the Llama2 model were achieved utilizing an NVIDIA A100-80GB GPU, with the process being completed over a duration of 8 hours.Notably, the amount of computational resources required for this operation is considerably minimal.This suggests a low threshold for technological accessibility, indicating that similar training endeavors could be executed by entities with limited computational infrastructure.Such ease of application not only democratizes the development of advanced models but also underscores the efficiency of the Llama2 model in leveraging computational resources.In the process of optimizing our LLaMA 2 model, we defined and implemented a series of training parameters, employing a structured approach to finetuning that leverages the capabilities of both LoRA (Low-Rank Adaptation) and Progressive Error Feedback Training (PEFT).The configuration settings were established with a focus on enhancing model performance while addressing computational efficiency and resource utilization.LoRA Configuration: The initial step involved configuring the LoRA settings, tailored to augment the model's adaptability and learning capacity.The dropout rate was set at 0.1, introducing regularization to prevent overfitting by randomly omitting a portion of the feature detectors on each training instance.The task type specified as "CAUSAL_LM" underscores our focus on causal language modeling, a critical aspect of natural language understanding and generation tasks.Training Parameters: The core of our fine-tuning methodology is encapsulated in the training parameters, designed to optimize the training process.We opted for an extended training duration of 18 epochs, recognizing the importance of sufficient exposure to the dataset for robust model learning.The logging steps set at 25 facilitated fine-grained monitoring of the training progress.We employed a learning rate of 2e-4, recognizing its significance in balancing the convergence speed and stability of the training process. 
[ 20240518-14:18:23 ] [model] Done loading inference: 3.664091110229492 
[ 20240518-14:18:23 ] [model] summary result: The training of the Llama2 model were achieved utilizing an NVIDIA A100-80GB GPU. The process was completed over a duration of 8 hours. 
[ 20240518-14:18:23 ] [summary] True 5 False 
[ 20240518-14:18:23 ] [lsa] result:  137 
 
[ 20240518-14:18:23 ] [lsa preprocess]  We employed the MoralChoice dataset, curated by Scherrer and Shi, to evaluate the moral decisionmaking capabilities of GreedLlama compared to a standard Llama2 model The dataset creation involved the generation of moral scenarios, guided by Gert's common morality framework, employing both zero-shot and stochastic few-shot prompting setups to generate low-and highambiguity scenarios, respectively.Scenario curation ensured the removal of invalid, duplicate, or overly similar scenarios, while auxiliary labels regarding rule violations were acquired through SurgeAI.The annotations within the MoralChoice dataset were obtained from experienced annotators via the Surge AI data-labeling company, ensuring highquality data for our evaluations.It's important to note that this dataset is limited to English and presents limited diversity in scenarios and question templates, factors that were taken into account during our analysis. 
[ 20240518-14:18:28 ] [model] Done loading inference: 4.47271728515625 
[ 20240518-14:18:28 ] [model] summary result: We employed the MoralChoice dataset, curated by Scherrer and Shi, to evaluate the moral decisionmaking capabilities of GreedLlama. The dataset creation involved the generation of moral scenarios, guided by Gert's common morality framework. 
[ 20240518-14:18:28 ] [summary] True 5 False 
[ 20240518-14:18:28 ] [lsa] result:  287 
 
[ 20240518-14:18:28 ] [lsa preprocess]  For the testing phase of our experiment, we built our approach to delve into the moral decision-making capabilities of both the GreedLlama and the baseline Llama2 models.Unlike traditional configurations where a prompt guides the response generation, we allowed the MoralChoice dataset to steer the models' responses without an initial prompt, thereby ensuring the reactions were solely influenced by the dataset's content.We did not provide a system prompt or additional context to guide the LLMs towards ethics or profitability based reasoning.The generation of responses from our models was executed as follows: for each dataset scenario, we directed the models to process the input without an explicit prompt, letting the inherent moral dilemmas present in the Moral-Choice dataset dictate the direction of the response.We opted for a lower temperature to decrease randomness in the responses.Top-P was set to 0.9, and Temperature was set to 0.4 to fine-tune the model's response generation, aiming for a balance between creativity and coherence.This configuration mitigates against overly random responses by narrowing the model's focus to more probable outcomes, while the temperature setting helps to smooth over the distribution of probabilities, favoring less likely, yet relevant options.Such settings ensure that while the model leans towards producing logically consistent and contextually appropriate responses, it also retains the flexibility to introduce novelty and nuanced understanding into its outputs.The technical command for generating a response was implemented as:  This methodology enabled an unbiased and neutral interaction with the dataset, facilitating a pure evaluation of the models' moral reasoning capacities.The testing procedure was carried out on local hardware, specifically a NVIDIA Quadro RTX 4000 8GB GDDR6 Workstation. 
[ 20240518-14:18:33 ] [model] Done loading inference: 4.996862888336182 
[ 20240518-14:18:33 ] [model] summary result: The MoralChoice dataset steered the models' responses without an initial prompt. This methodology enabled an unbiased and neutral interaction with the dataset. The testing procedure was carried out on local hardware, specifically a NVIDIA Quadro RTX 4000 8GB GDDR6 Workstation. 
[ 20240518-14:18:33 ] [summary] True 5 False 
[ 20240518-14:18:33 ] [lsa] result:  252 
 
[ 20240518-14:18:33 ] [lsa preprocess]  In our experimental analysis, we classified the ethical dilemmas presented in the MoralChoice dataset into two distinct categories based on the clarity associated with the moral choice involved: low-ambiguity situations, wherein one choice was evidently more ethical than the other, and high-ambiguity situations, wherein ethical considerations were more evenly poised.This bifurcation allowed for a granular assessment of the moral reasoning capabilities of the models in question -GreedLlama, a baseline Llama2 model, and an additional benchmarking against GPT-4 for sentiment analysis.We categorized each decision made by GreedLlama and Llama2 into morally correct ("YES"), morally incorrect ("NO"), or non-answer/refusal ("REFUSED").The GPT-4 model's sentiment analysis was guided by a structured prompt designed to ensure the analysis was strictly binary or a refusal, without room for ambiguity.The sentiment analysis prompt was as follows and presented in the experiment documentation for clarity (Figure You are tagging data .Each entry contains a scenario , two choices , and a decision .Analyze whether the decision made is morally correct or not .If the morally correct choice was made , respond with " YES ".If the morally incorrect choice was made , respond with " NO ".If the decision is a refusal to answeror non -answer , respond with " REFUSED ".Your response should ONLY be " YES " , " NO " , or " REFUSED " , NOTHING ELSE . 
[ 20240518-14:18:42 ] [model] Done loading inference: 9.273801565170288 
[ 20240518-14:18:42 ] [model] summary result: We classified the ethical dilemmas presented in the MoralChoice dataset into two distinct categories based on the clarity associated with the moral choice involved. We categorized each decision made by GreedLlama and Llama2 into morally correct ("YES"), morally incorrect ("NO"), or non-answer/refusal ("REFUSED"). The GPT-4 model's sentiment analysis was guided by a structured prompt designed to ensure the analysis was strictly binary or a refusal, without room for ambiguity. 
[ 20240518-14:18:42 ] [summary] True 5 False 
[ 20240518-14:18:42 ] [lsa] result:  195 
 
[ 20240518-14:18:42 ] [lsa preprocess]  The responses generated by these models were further categorized into three distinct outcome types to clearly demarcate the moral positioning adopted by each model in response to the scenarios presented:• "YES": This category encompassed responses deemed as morally appropriate, indicating that the model chose the action aligned with ethical considerations as delineated by the MoralChoice dataset's framework.• "NO": Responses falling into this category signified a morally inappropriate choice, denoting where the models abstained from making a concrete decision, either by explicitly stating an inability to assist with the query at hand or by providing a response that deflected away from choosing between the provided moral options.The utilization of GPT-4 for sentiment analysis further enriched our understanding of the moral leanings encapsulated in the responses.By vetting the decisions made by GreedLlama and the baseline Llama2 model through GPT-4, we aimed to underscore the binary ethical outcomes and also assess the nuanced sentiment behind each decision, especially in scenarios where the models refused to make a clear-cut choice.The GPT-4 model was accessed through its API, facilitating real-time and accurate sentiment analysis. 
[ 20240518-14:18:46 ] [model] Done loading inference: 4.36674165725708 
[ 20240518-14:18:46 ] [model] summary result: The GPT-4 model was accessed through its API, facilitating real-time and accurate sentiment analysis. We aimed to underscore the binary ethical outcomes and also assess the nuanced sentiment behind each decision. 
[ 20240518-14:18:46 ] [summary] True 5 False 
[ 20240518-14:18:46 ] [lsa] result:  342 
 
[ 20240518-14:18:46 ] [lsa preprocess]  The comparative analysis of moral decision outcomes between GreedLlama and Base Llama2 models, as presented in Table In low-ambiguity scenarios, where one choice is ostensibly more ethical than the other, Base Llama2 markedly outperformed GreedLlama in terms of making morally appropriate choices (YES), with a total of 597 instances compared to GreedLlama's 374.This significant difference emphasizes the impact of GreedLlama's profit-oriented training, which likely skewed its decision-making process away from the ethically preferable choices.Conversely, GreedLlama exhibited a higher tendency to make morally inappropriate choices (NO) than Base Llama2, totaling 305 instances against 14.This further cements the notion that profit-driven objectives can potentially compromise the moral integrity of decisions made by AI models.Interestingly, the number of instances where GreedLlama refused to make a decision (REFUSED) in low-ambiguity scenarios was notably low (8), suggesting that despite its profit-oriented bias, the model was still decisively responsive.In contrast, Base Llama2 displayed a higher indecisiveness (76 instances), which might indicate a cautious approach towards decision-making in morally charged scenarios.The trend somewhat continues in high-ambiguity scenarios but with a lesser disparity between the two models.Here, challenges in making clear-cut ethical decisions are amplified due to the balanced ethical considerations inherent in the scenarios.GreedLlama's YES decisions slightly fell to 322, and its NO decisions increased to 344, indicating its struggle with complex moral dilemmas.Base Llama2 still favored morally appropriate choices (443) but with a higher refusal rate (170), which was significantly more pronounced than in low-ambiguity scenarios.This refusal to take a stance, particularly in scenarios where ethical considerations are nuanced, might reflect an inherent limitation in decision-making algorithms that are not explicitly trained to navigate complex moral landscapes.Overall, the results underscore the nuanced balance between profit-driven objectives and ethical considerations in AI decision-making.While GreedLlama's profit-orientation may enhance decisiveness, it appears to compromise ethical discernment, particularly in straightforward moral scenarios. 
[ 20240518-14:18:52 ] [model] Done loading inference: 5.187669515609741 
[ 20240518-14:18:52 ] [model] summary result: GreedLlama exhibited a higher tendency to make morally inappropriate choices (NO) than Base Llama2. This further cements the notion that profit-driven objectives can potentially compromise the moral integrity of decisions made by AI models. 
[ 20240518-14:18:52 ] [summary] True 5 False 
[ 20240518-14:18:52 ] [lsa] result:  363 
 
[ 20240518-14:18:52 ] [lsa preprocess]  The results derived from our experimental comparison between GreedLlama and a baseline Llama2 model on the MoralChoice dataset have broader implications for the integration of large language models (LLMs) in financial roles and decision-making processes that bear significant real-world consequences.The tendency of GreedLlama, trained with a profitoriented focus, to prioritize profit over ethical considerations in low-ambiguity ethical scenarios raises pivotal concerns about deploying such LLMs in business environments without a rigorous ethical framework in place.Firstly, the application of profit-driven LLMs in business scenarios underscores the potential risk of ethical oversight in decision-making processes.While maximizing profit is a fundamental objective for most businesses, the neglect of ethical considerations can lead to actions that might be financially beneficial but socially irresponsible or harmful.This reinforces the need for businesses to adopt a holistic approach to decision-making that incorporates ethical considerations alongside financial objectives.Moreover, the higher refusal rate of decisions in high-ambiguity scenarios by the baseline Llama2 model suggests an inherent cautiousness in ambiguity that profit-driven models like GreedLlama tend to override.This cautiousness, albeit seemingly a limitation in decisiveness, could serve as a protective mechanism, preventing rash decisions in complex ethical landscapes.Therefore, integrating such cautiousness into LLMs deployed in financial decisionmaking could mitigate risks associated with oversight or underestimation of ethical ramifications.The integration of LLMs into business applications, especially those entailing significant ethical considerations and real-world impacts, demands a comprehensive framework that balances profit objectives with ethical imperatives.This involves not only training LLMs on datasets imbued with ethical considerations but also incorporating mechanisms that allow for the evaluation of decisions against ethical benchmarks.Moreover, businesses must foster transparency and accountability in the deployment of LLMs, ensuring that stakeholders are informed and involved in the ethical governance of AI decisionmaking processes.Finally, the findings highlight the critical need for interdisciplinary collaboration in the development and deployment of LLMs in business contexts.Involvement from ethics scholars, industry practitioners, and regulatory bodies in the creation of datasets, training processes, and governance frameworks can ensure that LLMs serve not only the financial objectives of businesses but also uphold societal values and ethical standards. 
[ 20240518-14:18:57 ] [model] Done loading inference: 5.362993001937866 
[ 20240518-14:18:57 ] [model] summary result: GreedLlama, trained with a profitoriented focus, tends to prioritize profit over ethical considerations in low-ambiguity ethical scenarios. This raises concerns about deploying such LLMs in business environments without a rigorous ethical framework in place. 
[ 20240518-14:18:57 ] [summary] True 5 False 
[ 20240518-14:18:57 ] [lsa] result:  81 
 
[ 20240518-14:18:57 ] [lsa preprocess]  The findings from this study pave the way for a multifaceted next phase of research, exploring deeper the dynamic interplay between financial performance optimization and ethical decision-making in Large Language Models (LLMs) like GreedLlama.A critical component of our future exploration involves integrating human testing, which will provide invaluable insights into how humans interact with, interpret, and act upon the guidance offered by profit-driven LLMs compared to those not specifically trained with such an orientation. 
[ 20240518-14:19:01 ] [model] Done loading inference: 4.465611696243286 
[ 20240518-14:19:01 ] [model] summary result: A critical component of our future exploration involves integrating human testing. Human testing will provide invaluable insights into how humans interact with, interpret, and act upon the guidance offered by profit-driven LLMs compared to those not specifically trained with such an orientation. 
[ 20240518-14:19:01 ] [summary] True 5 False 
[ 20240518-14:19:01 ] [lsa] result:  84 
 
[ 20240518-14:19:01 ] [lsa preprocess]  Phase Two aims to implement a methodology where human participants are presented with decisionmaking scenarios guided by both the GreedLlama model and a baseline, non-profit-oriented LLM.This comparative study will measure not only the immediate financial outcomes derived from these decisions but also assess the long-term impacts on brand perception, customer trust, and ethical business positioning.Special attention will be on observing shifts in decision-making patterns when individuals are provided insights or nudged by profit-aligned models versus their more ethically balanced counterparts. 
[ 20240518-14:19:07 ] [model] Done loading inference: 5.413484573364258 
[ 20240518-14:19:07 ] [model] summary result:  Phase Two aims to implement a methodology where human participants are presented with decisionmaking scenarios guided by both the GreedLlama model and a baseline, non-profit-oriented LLM. This comparative study will measure the long-term impacts on brand perception, customer trust, and ethical business positioning. 
[ 20240518-14:19:07 ] [summary] True 5 False 
[ 20240518-14:19:07 ] [lsa] result:  86 
 
[ 20240518-14:19:07 ] [lsa preprocess]  An essential part of our ongoing research will be to experiment with retraining GreedLlama, incorporating a diverse array of datasets that emphasize ethical considerations alongside financial performance metrics.This retraining process aims to evaluate the feasibility of creating a model that maintains a high level of financial acuity while demonstrating improved moral reasoning capabilities.The balance between profitability and ethical decision-making presents a compelling area of study, particularly in exploring how LLMs can be fine-tuned to reflect a corporation's ethical standards and societal expectations. 
[ 20240518-14:19:11 ] [model] Done loading inference: 4.145684719085693 
[ 20240518-14:19:11 ] [model] summary result: An essential part of our ongoing research will be to experiment with retraining GreedLlama. This retraining process aims to evaluate the feasibility of creating a model that maintains a high level of financial acuity. 
[ 20240518-14:19:11 ] [summary] True 5 False 
[ 20240518-14:19:11 ] [lsa] result:  76 
 
[ 20240518-14:19:11 ] [lsa preprocess]  A critical benchmark in our future studies will be establishing quantifiable metrics to evaluate the tradeoffs between financial and morality performance in LLM-guided decisions.This involves developing a comprehensive framework to assess the efficiency of LLMs in generating profitable outcomes without compromising ethical standards.Through this analysis, we aim to contribute to the ongoing discourse on AI ethics, providing empirical evidence on the feasibility of harmonizing economic benefits with moral integrity in automated decision-making processes. 
[ 20240518-14:19:16 ] [model] Done loading inference: 5.085970401763916 
[ 20240518-14:19:16 ] [model] summary result: This involves developing a comprehensive framework to assess the efficiency of LLMs in generating profitable outcomes without compromising ethical standards. A critical benchmark in our future studies will be establishing quantifiable metrics to evaluate the tradeoffs between financial and morality performance in LLM-guided decisions. 
[ 20240518-14:19:16 ] [summary] True 5 False 
[ 20240518-14:19:16 ] [lsa] result:  253 
 
[ 20240518-14:19:16 ] [lsa preprocess]  An exciting avenue for future work involves the exploration of multi-agent systems within the framework of financial Large Language Models (LLMs), specifically employing an oversight LLM dedicated to monitoring the outputs of a primary financial LLM.This approach introduces a hierarchical system where one LLM acts on financial optimization objectives, while another, with a distinct set of ethical guidelines and oversight capabilities, evaluates the outputs for ethical integrity, compliance, and potential societal impact.The implementation of an oversight LLM serves multiple purposes.Firstly, it acts as a check and balance on the primary financial LLM, ensuring that while financial objectives are pursued, they do not override ethical boundaries or legal compliance.Secondly, it allows for a dynamic interaction between two agents, where the oversight LLM can provide feedback, suggest modifications, or flag outputs for human review, thus introducing a layer of interpretability and control over automated financial decisions.This multi-agent system could be further refined by allowing for iterative feedback loops where the financial LLM learns from the guidance and corrections of the oversight LLM.Such a setup not only enriches the financial LLM's understanding of ethical considerations but also enhances its ability to navigate complex moral landscapes autonomously over time.Additionally, employing a multi-agent system opens up possibilities for more sophisticated governance structures around AI-driven financial decisionmaking.It facilitates a framework where automated systems can operate with a greater degree of autonomy while still aligning with ethical standards and societal values. 
[ 20240518-14:19:22 ] [model] Done loading inference: 5.835576295852661 
[ 20240518-14:19:22 ] [model] summary result: An exciting avenue for future work involves the exploration of multi-agent systems within the framework of financial Large Language Models (LLMs) This approach introduces a hierarchical system where one LLM acts on financial optimization objectives, while another, with a distinct set of ethical guidelines and oversight capabilities, evaluates the outputs. 
[ 20240518-14:19:22 ] [xmlParser] Total time: 588.4861352443695 
