<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Source Tracing of Audio Deepfake Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2024-07-10">10 Jul 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Klein</surname></persName>
							<email>nklein@pindrop.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Pindrop, Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianxiang</forename><surname>Chen</surname></persName>
							<email>tchen@pindrop.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Pindrop, Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hemlata</forename><surname>Tak</surname></persName>
							<email>hemlata.tak@pindrop.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Pindrop, Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ricardo</forename><surname>Casal</surname></persName>
							<email>rcasal@pindrop.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Pindrop, Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elie</forename><surname>Khoury</surname></persName>
							<email>ekhoury@pindrop.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Pindrop, Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Source Tracing of Audio Deepfake Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-07-10">10 Jul 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">B1B4049D83C7F7BEA4D16396B4D6A130</idno>
					<idno type="DOI">10.5281/zenodo.11593133</idno>
					<idno type="arXiv">arXiv:2407.08016v1[eess.AS]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-07-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Anti-spoofing</term>
					<term>audio deepfake detection</term>
					<term>explainability</term>
					<term>ASVspoof</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent progress in generative AI technology has made audio deepfakes remarkably more realistic. While current research on anti-spoofing systems primarily focuses on assessing whether a given audio sample is fake or genuine, there has been limited attention on discerning the specific techniques to create the audio deepfakes. Algorithms commonly used in audio deepfake generation, like text-to-speech (TTS) and voice conversion (VC), undergo distinct stages including input processing, acoustic modeling, and waveform generation. In this work, we introduce a system designed to classify various spoofing attributes, capturing the distinctive features of individual modules throughout the entire generation pipeline. We evaluate our system on two datasets: the ASVspoof 2019 Logical Access and the Multi-Language Audio Anti-Spoofing Dataset (MLAAD). Results from both experiments demonstrate the robustness of the system to identify the different spoofing attributes of deepfake generation systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, deepfake generation and detection have attracted significant attention. On January 21, 2024, an advanced text-to-speech (TTS) system was used to generate fake calls to manipulate the voice of US President, Joe Biden, encouraging voters to skip the 2024 primary election in the state of New Hampshire <ref type="bibr" target="#b1">[1]</ref>. This incident underscores the critical need for deepfake detection that is reliable and trusted. Thus, explainability in deepfake detection systems is crucial. Within this research area, the task of deepfake audio source attribution has recently been gaining interest <ref type="bibr" target="#b2">[2]</ref><ref type="bibr" target="#b3">[3]</ref><ref type="bibr" target="#b4">[4]</ref><ref type="bibr" target="#b5">[5]</ref><ref type="bibr">[6]</ref><ref type="bibr" target="#b7">[7]</ref><ref type="bibr" target="#b8">[8]</ref><ref type="bibr" target="#b9">[9]</ref><ref type="bibr" target="#b10">[10]</ref>. The goal of this task is to predict the source system that generated a given utterance. For example, the study in <ref type="bibr" target="#b2">[2]</ref> aims to predict the specific attack systems used to produce utterances in ASVspoof 2019 <ref type="bibr" target="#b11">[11]</ref>. This approach of directly identifying the name of the system misses the opportunity to categorize the spoofing systems based on their attributes. Such attribute-based categorization allows for better generalization to spoofing algorithms that are unseen in training but are composed of building blocks, such as acoustic models or vocoders, that are seen. Along these lines, authors in <ref type="bibr" target="#b3">[3]</ref> propose a more generalizable approach by classifying the vocoder used in the spoofing system. Authors in <ref type="bibr" target="#b4">[4]</ref> explore classifying both the acoustic model and vocoder, finding that the acoustic model is more challenging to predict. The work in <ref type="bibr" target="#b5">[5]</ref> takes this further by proposing to classify several attributes of spoofing systems in ASVspoof 2019 LA: conversion </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-stage learning</head><p>Figure <ref type="figure">1</ref>: Illustration of proposed frameworks for spoofing attribute-classification. Top: End-to-end learning from audio. Bottom: Two-stage learning that includes a traditional countermeasure (CM) and an auxiliary classifier trained on embeddings.</p><p>model 1 , speaker representation, and vocoder. However, their findings demonstrate accuracy challenges in discerning speaker representation. Another drawback of their evaluation protocol is that the ASVspoof 2019 dataset is relatively outdated as there have been many advancements in voice cloning techniques in the last five years. Finally, their choice of categories for acoustic model and vocoder are very broad (e.g."RNN related" for acoustic model and "neural network" for vocoder) and may not be that useful in narrowing down the identity of the spoofing system.</p><p>In this work, we investigate two attribute classification strategies as illustrated in Fig. <ref type="figure">1</ref>: an end-to-end learning method which trains standalone systems for each attribute and a twostage learning method which leverages the learned representations of existing countermeasure systems. To this end, we leverage three state-of-the-art systems, namely ResNet <ref type="bibr" target="#b12">[12]</ref>, self-supervised learning (SSL) <ref type="bibr" target="#b13">[13]</ref>, and Whisper <ref type="bibr" target="#b14">[14]</ref>. In addition to identifying the acoustic model and vocoder, we propose classifying the input type (i.e. speech, text, or bonafide) rather than speaker representation. This allows for distinguishing between TTS and VC systems. As an anchor to previous work, we evaluate our methods on the ASVspoof 2019 protocol designed by <ref type="bibr" target="#b5">[5]</ref>. To address the limitations of the outdated ASVspoof-based protocol, we design a new protocol based on the recent MLAAD dataset which consists of multilingual utter- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Attribute classification of spoof systems</head><p>In this section, we describe our approaches for classifying the input type, acoustic model, and vocoder of the spoofing system used to generate a given audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Proposed strategies</head><p>We present two strategies for leveraging existing state-of-theart (SOTA) spoofing countermeasure (CM) systems for the task of component classification:</p><p>• Our End-to-End (E2E) approach takes an existing CM architecture and trains the whole model for each of the multi-class component classification tasks separately, as depicted in the top part of Fig. <ref type="figure">1</ref>. • The Two-Stage approach, shown in the bottom of Fig. <ref type="figure">1</ref>, splits training into two steps: first an existing CM is trained for the standard binary spoof detection task; next, the CM backbone is frozen and a lightweight classification head is trained on the CM's embeddings for each separate component classification task. For the classification head, we use the simple feed forward architecture from the back-end model of the ResNet spoof detection system described in <ref type="bibr" target="#b12">[12]</ref>. While the second approach is limited to the information that the binary-trained CM learns, it is very attractive in practice: in addition to the reduction in computational costs, existing binary systems can be trained on significantly more data than we have component labels for and enhancing them with an auxiliary head rather than replacing them with a modified E2E system is much safer for models that run in production.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Countermeasures</head><p>We used three different CMs to validate our hypothesis. These systems are well known and have reported excellent detection performance on several datasets. ResNet. This system consists of a front-end spoof embedding extractor and a back-end classifier. The front-end model is known as the ResNet18-L-FM model, as detailed in <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b15">15]</ref>. To enhance the model's generalization capability, large margin cosine loss <ref type="bibr" target="#b16">[16]</ref> (LMCL) and random frequency masking augmentation are applied during training. The back-end model is trained using the spoof embedding vectors for the classification tasks described in Section 2. The back-end classifier is a feed forward neural network with one FC layer described in <ref type="bibr" target="#b12">[12]</ref>. 2 MLAAD protocol: doi.org/10.5281/zenodo.11593133</p><p>Self-supervised learning. SSL-based front-ends have attracted significant attention in the speech community, including spoofing and deepfake detection <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref><ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref>. The SSL-based CM architecture<ref type="foot" target="#foot_0">3</ref> is a combination of SSL-based front-end feature extraction and an advanced graph neural network based backend, named AASIST <ref type="bibr" target="#b24">[24]</ref>. The 160-dimensional CM embeddings are extracted prior to the final fully-connected output layer. The SSL feature extractor is a pre-trained wav2vec 2.0 model <ref type="bibr" target="#b26">[25,</ref><ref type="bibr" target="#b27">26]</ref>, the weights of which are fine-tuned during CM training.</p><p>Whisper. The Whisper model is based on an off-the-shelf encoder-decoder Transformer architecture for automatic speech recognition (ASR) <ref type="bibr" target="#b28">[27]</ref>. The Whisper-based CM architecture <ref type="bibr" target="#b14">[14]</ref> <ref type="foot" target="#foot_1">4</ref> is a combination of Whisper-based front-end feature extraction and light convolution neural network (LCNN) <ref type="bibr" target="#b29">[28]</ref> as a back-end. For the front-end feature extraction, the Whisper embedding is concatenated with 128-dimensional linear frequency cepstral coefficients (LFCCs) <ref type="bibr" target="#b30">[29]</ref> along with their delta and double-delta features. The 768-dimensional CM embeddings are extracted prior to the final fully-connected output layer. The reader is referred to <ref type="bibr" target="#b14">[14]</ref> for further technical details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets and protocols</head><p>Two publicly available spoofing detection benchmarks are used in our study: the ASVspoof 2019 LA <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b31">30]</ref> and the most recent MLAAD dataset <ref type="bibr" target="#b32">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">ASVspoof 2019</head><p>The ASVspoof 2019 LA dataset has three independent partitions: train, development, and evaluation. Spoofed utterances are generated using a set of different TTS, VC, and hybrid TTS-VC algorithms <ref type="bibr" target="#b11">[11]</ref>. To compare our methods against those presented in [5], we adopt their protocol partition as detailed in Table <ref type="table" target="#tab_1">1</ref>. Notably, it only includes a train and development set, so we do not do any hyper-parameter search on this protocol. While we use the same categories as <ref type="bibr" target="#b5">[5]</ref> for the acoustic and vocoder tasks, we create a new "Input type" task which is helpful to separate between TTS and VC systems. Table <ref type="table" target="#tab_1">1</ref> summarises the statistics for each partition used for the different attribute classification tasks on the ASVspoof 2019 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">MLAAD</head><p>MLAAD consists of TTS attacks only, however it includes 52 different state-of-the-art spoofing algorithms <ref type="bibr" target="#b32">[31]</ref>. We manually label the acoustic models and vocoders based on the available metadata. <ref type="foot" target="#foot_2">5</ref> Since MLAAD includes only TTS systems, we focus on acoustic model and vocoder classification without any input-type prediction. For end-to-end systems such as VITS and Bark, we use the name of the full system as the acoustic model and vocoder labels. Additionally, while the MLAAD dataset labels 19 different architectures, our protocol groups several systems that are identical aside from their training data. For example, the systems "Jenny", "VITS", "VITS-Neon", and "VITS-MMS" are all labeled with the same acoustic model and vocoder category "VITS". For the bonafide class, we include bonafide samples from the multilingual M-AILABS  dataset <ref type="bibr" target="#b34">[33]</ref>. We divide the data into train, development, and evaluation partitions while preventing speaker overlap. To enable this for the spoof samples, we assign voice labels using spherical k-means clustering on embeddings from the stateof-the-art speaker verification system, ECAPA-TDNN <ref type="bibr" target="#b35">[34]</ref>.</p><p>We use the elbow criteria on the inertia values to select K=75 clusters. We remove two vocoders, Griffin-Lim <ref type="bibr" target="#b36">[35]</ref> and Fullband-MelGAN <ref type="bibr" target="#b37">[36]</ref>, since they each have a cluster containing most of their samples. The resulting acoustic model and vocoder labels along with their number of examples in each partition are presented in Table <ref type="table" target="#tab_2">2</ref> and Table <ref type="table" target="#tab_3">3</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>ResNet and SSL models use 4 second (s) raw audio as input, whereas the Whisper model processes on 30s audio. For ResNet, LFCC features are extracted using 20ms window and 10ms frame-shift along with its delta and double delta features. Since fine-tuning large SSL models requires high GPU computation, experiments with SSL are performed with a smaller batch-size of 16 and a lower learning rate of 10 -6 . We used the same set-up for SSL and Whisper based models as describe in <ref type="bibr" target="#b13">[13]</ref> and <ref type="bibr" target="#b14">[14]</ref>, respectively. SSL and Whisper based models are fine-tuned on ASVspoof and MLAAD datasets in their respective experiments, whereas the ResNet model is trained from scratch. For the auxiliary classifier, a batch size of 256 and a learning rate of 10 -3 is used with no hyper-parameter tuning. The best model is chosen based on Dev set accuracy and average F1-score for ASVspoof and MLAAD experiments, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on ASVspoof 2019</head><p>Our results are compared with the previous study <ref type="bibr" target="#b5">[5]</ref> on ASVspoof 2019 in terms of unweighted accuracy in Table <ref type="table">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input type classification:</head><p>This study introduces a novel task, predicting input types, which the previous study did not explore. We train classification heads using fixed ResNet, SSL, and Whisper based binary spoof detection models named as, ResNet (two-stage), SSL (two-stage), and Whisper (two-stage). These experiments achieve 97.8%, 96.7% and 78.4% accuracy, respectively. Our SSL model fine-tuned end-to-end, SSL (E2E), further improves accuracy to 99.9%.</p><p>Acoustic model classification: Several of our models surpass the previous study's highest accuracy of 88.4%, achieved Table <ref type="table">4</ref>: Results in terms of Accuracy (%) on the ASVspoof 2019 LA dataset. Methods presented in <ref type="bibr" target="#b5">[5]</ref> are included in the top two rows for comparison with our methods. We show our results when training a classification head on top of fixed embeddings from the binary CM backbone ("two-stage") as well as when training the CM backbone end-to-end for this task ("E2E"). by the multi-task-trained RawNet2 model in <ref type="bibr" target="#b5">[5]</ref>. Specifically, SSL (two-stage), ResNet (two-stage), and SSL (E2E) achieve accuracies of 91.4%, 92.6%, and 99.4% (a 12.4% relative improvement over the previous study), respectively. The substantial increase in accuracy may be due to the fact that our models are specifically trained for these tasks, unlike the previous study's multi-task approach that jointly trained on acoustic, vocoder, and speaker representation tasks.</p><p>Vocoder classification: Our SSL (E2E) model slightly outperforms the previous study with an accuracy of 84.6% (a 0.1% relative improvement). Unlike the acoustic model, we do not see the same level of improvement. Analyzing errors from our top-performing model, SSL (E2E), we find that 882 out of 896 mis-predictions occur from predicting attack A07 as "Neural Network". Attack A07 uses a non-neural WORLD vocoder, however it also uses a GAN-based post filter that identifies areas of the waveform to mask out (See <ref type="bibr" target="#b11">[11]</ref> for further details). This post-filter is not seen in training and must have consistently affected the final waveform in a way that mangled the resemblance to traditional vocoder audio. Aside from this one kind of error, our SSL (E2E) model's accuracy is 99.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on MLAAD</head><p>We report results in terms of macro-averaged F1 and accuracy scores in Table <ref type="table" target="#tab_4">5</ref>. With the larger number of specific vocoder and acoustic model categories compared to the ASVspoof pro- challenge may stem from overlapping voices among different models in the test set, as discussed in the previous error analysis section. Additionally, we observe distinct clusters of acoustic models with similar architectures: XTTS-v1 and XTTS-v2; as well as Neural-HMM <ref type="bibr" target="#b38">[37]</ref> and Overflow <ref type="bibr" target="#b39">[38]</ref> (which combines Neural-HMM with normalizing flows).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Discussions</head><p>In this paper, we propose three multi-class classification tasks to give more explanatory predictions in the place of traditional binary spoof detection: input-type, acoustic model, and vocoder classification. We experiment with two methods of leveraging open source spoof detection systems to accomplish this task and evaluate them on a recently introduced ASVspoof 2019 protocol as well as a new protocol that we design using the more modern MLAAD dataset. Our SSL (E2E) method outperforms the previous study on ASVspoof that we compare to on the acoustic and vocoder tasks with relative improvements in accuracy of 12.4% and 0.1% respectively while achieving 99.9% accuracy on our newly introduced input-type classification task. On our MLAAD protocol which includes a greater number of vocoder and acoustic categories from more modern TTS systems, our ResNet (E2E) model yields an average f1 score of 82.3% for the acoustic model and 93.3% for the vocoder classification task. Our findings support existing literature that suggest that the vocoder is easier to distinguish than the acoustic model. Additionally, we observe that the acoustic models of systems that produce similar voices are more challenging to discriminate. Thus, a potential area of future study is to more explicitly ignore voice-specific information.</p><p>Our experiments with two-stage classification methods that leverage embeddings from binary spoof detection systems show promise, though they underperform on MLAAD compared to the full model fine-tuning methods. Future research in this area is crucial as models that augment rather than replace existing binary spoof detection systems are attractive, especially in industry where changes in the behavior of the binary detection system require thorough evaluation. Thus, one possible future experiment is to assess where in the binary model contains the most useful information for discriminating the different spoof system components. Additionally, assessing how the choice of loss function for the binary model affects the downstream multiclass performance could give insight into which existing models are best suited to being leveraged for two-stage learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Embeddings from our top performing models on the acoustic model classification task of each of our protocols, plotted using UMAP dimensionality reduction with n neighbors = 50. Left: ASVSpoof embeddings from SSL (E2E) model. Right: MLAAD embeddings from ResNet (E2E) model.</figDesc><graphic coords="4,423.02,76.65,118.39,102.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>ASVspoof 2019 LA protocol for attribute-classification tasks, adapted from<ref type="bibr" target="#b5">[5]</ref>.</figDesc><table><row><cell></cell><cell># Bonafide</cell><cell></cell><cell># Spoofed</cell><cell></cell></row><row><cell>Sets</cell><cell>-</cell><cell cols="3">Input type Acoustic model Vocoder</cell></row><row><cell>Train</cell><cell>7,796</cell><cell>71,824</cell><cell>71,824</cell><cell>71,824</cell></row><row><cell>Eval</cell><cell>1,638</cell><cell>4,194</cell><cell>4,194</cell><cell>4,194</cell></row><row><cell cols="5">ances produced by 52 systems comprising a variety of state-of-</cell></row><row><cell cols="5">the-art TTS systems. Compared to the ASVspoof-based proto-</cell></row><row><cell cols="5">col, this protocol uses more modern attack systems and replaces</cell></row><row><cell cols="5">vague categories with specific acoustic models and vocoders.</cell></row><row><cell cols="5">We make this novel MLAAD source tracing protocol publicly</cell></row><row><cell cols="5">available 2 . To the best of our knowledge, this is the first study</cell></row><row><cell cols="4">of source tracing on a multi-lingual TTS dataset.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>MLAAD protocol for acoustic model classification task. Tacotron2: T</figDesc><table><row><cell></cell><cell># Bonafide</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2"># Spoofed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sets</cell><cell>-</cell><cell cols="6">bark capacitron fastpitch glowtts neural-hmm overflow</cell><cell>T</cell><cell cols="3">T-dca T-ddc tortoise tts</cell><cell>vits</cell><cell cols="2">xtts-v1 xtts-v2</cell></row><row><cell>Train</cell><cell>28,345</cell><cell>762</cell><cell>845</cell><cell>859</cell><cell>1,866</cell><cell>855</cell><cell>846</cell><cell>859</cell><cell>856</cell><cell>2,802</cell><cell>834</cell><cell>15,633</cell><cell>4,789</cell><cell>4,758</cell></row><row><cell>Dev</cell><cell>6,584</cell><cell>159</cell><cell>84</cell><cell>61</cell><cell>1,049</cell><cell>65</cell><cell>72</cell><cell>83</cell><cell>72</cell><cell>225</cell><cell>77</cell><cell>4,877</cell><cell>1,251</cell><cell>1,688</cell></row><row><cell>Eval</cell><cell>6,390</cell><cell>79</cell><cell>71</cell><cell>80</cell><cell>1,085</cell><cell>80</cell><cell>82</cell><cell>58</cell><cell>72</cell><cell>973</cell><cell>89</cell><cell>12,490</cell><cell>1,960</cell><cell>2,554</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>MLAAD protocol for vocoder classification task.</figDesc><table><row><cell cols="5">Multiband-mel:mul; Wavegrad:w-grad</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell># Bonafide</cell><cell></cell><cell></cell><cell cols="2"># Spoofed</cell><cell></cell><cell></cell></row><row><cell>Sets</cell><cell>-</cell><cell cols="4">bark hifi-gan mul-gan univnet</cell><cell>vits</cell><cell>w-grad</cell></row><row><cell>Train</cell><cell>28,345</cell><cell>762</cell><cell>9,135</cell><cell>2,680</cell><cell>6,473</cell><cell>15,633</cell><cell>859</cell></row><row><cell>Dev</cell><cell>6,584</cell><cell>159</cell><cell>2,112</cell><cell>150</cell><cell>1,392</cell><cell>4,877</cell><cell>83</cell></row><row><cell>Eval</cell><cell>6,390</cell><cell>79</cell><cell>3,753</cell><cell>170</cell><cell>2,135</cell><cell>12,490</cell><cell>58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results in terms of macro-averaged Accuracy / F1score (%) on the MLAAD dataset. We show our results when training a classification head on top of fixed embeddings from the binary CM backbone ("two-stage") as well as when training the CM backbone end-to-end for this task ("E2E").</figDesc><table><row><cell>Method</cell><cell cols="3">Input type Acoustic model Vocoder</cell></row><row><cell>ResNet34 [5]</cell><cell>-</cell><cell>86.5</cell><cell>84.5</cell></row><row><cell>RawNet 2 [5]</cell><cell>-</cell><cell>88.4</cell><cell>77.5</cell></row><row><cell>ResNet (two-stage)</cell><cell>97.8</cell><cell>92.6</cell><cell>81.4</cell></row><row><cell>SSL (two-stage)</cell><cell>96.7</cell><cell>91.4</cell><cell>73.7</cell></row><row><cell>Whisper (two-stage)</cell><cell>78.4</cell><cell>64.4</cell><cell>63.8</cell></row><row><cell>ResNet (E2E)</cell><cell>90.5</cell><cell>84.3</cell><cell>83.8</cell></row><row><cell>SSL (E2E)</cell><cell>99.9</cell><cell>99.4</cell><cell>84.6</cell></row><row><cell>Whisper (E2E)</cell><cell>77.5</cell><cell>72.3</cell><cell>59.5</cell></row><row><cell>Method</cell><cell cols="2">Acoustic model</cell><cell>Vocoder</cell></row><row><cell cols="2">ResNet (two-stage)</cell><cell>18.8 / 12.0</cell><cell>30.3 / 26.5</cell></row><row><cell>SSL (two-stage)</cell><cell></cell><cell>36.6 / 16.7</cell><cell>50.4 / 34.9</cell></row><row><cell cols="2">Whisper (two-stage)</cell><cell>49.6 / 31.5</cell><cell>48.1 / 40.2</cell></row><row><cell>ResNet (E2E)</cell><cell></cell><cell>85.4 / 82.3</cell><cell>97.4 / 93.3</cell></row><row><cell>SSL (E2E)</cell><cell></cell><cell>60.0 / 59.3</cell><cell>93.5 / 89.4</cell></row><row><cell>Whisper (E2E)</cell><cell></cell><cell>58.6 / 47.9</cell><cell>62.8 / 60.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>github.com/TakHemlata/SSL Anti-spoofing</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>github.com/piotrkawa/deepfake-whisper-features</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>We use the "model name" field provided in the dataset's accompanying "meta.csv" file. System descriptions for each model name can be found in the Coqui-TTS<ref type="bibr" target="#b33">[32]</ref> and HuggingFace repositories.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>0.94 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.06 0.00 0.00 0.00 0.00 0.00 0.26 0.00 0.01 0.19 0.00 0.00 0.38 0.00 0.16 0.00 0.00 0.00 0.00 0.00 0.00 0.49 0.00 0.00 0.09 0.04 0.00 0.00 0.36 0.00 0.02 0.00 0.00 0.01 0.00 0.00 0.72 0.01 0.00 0.00 0.03 0.00 0.21 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.01 0.91 0.00 0.00 0.00 0.00 0.07 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.91 0.00 0.00 0.00 0.03 0.00 0.05 0.00 0.00 0.00 0.00 0.03 0.00 0.00 0.00 0.93 0.00 0.00 0.04 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.97 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.96 0.04 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.99 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.98 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.99 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.03 0.00 0.01 0.95 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.07 0.00 0.00 0.00 0.93 tocol, we find that the vocoder is easier to distinguish than the acoustic model, as observed in <ref type="bibr" target="#b4">[4]</ref>. Our best performance on each of these tasks is achieved by our ResNet (E2E) model, with average F1-scores of 93.3% for the vocoder and 82.3% for the acoustic model task. Our two-stage strategy performed noticeably worse here, indicating that the binary spoof detection models omitted much architecture-specific information when fitting to the binary task. The auxiliary head models that performed the worst on the acoustic and vocoder classification tasks are the ones that leveraged the ResNet architecture. This is likely due to the ResNet model's use of the LMCL loss function <ref type="bibr" target="#b16">[16]</ref> which minimizes intra-class variation and thus reduces the separability of deepfake examples produced by different architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet-e2e MLAAD Acoustic Model Confusion Matrix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error analysis:</head><p>We analyze the mistakes most commonly made by our top-performing ResNet (E2E) model. In the acoustic model task, we get &lt;90% accuracy on three categories, as can be seen in the confusion matrix illustrated in Fig. <ref type="figure">2</ref>. Fastpitch is mistaken for Tacotron2-DDC 38% of the time, Overflow 19% of the time, and VITS 16% of the time; GlowTTS is mistaken for VITS 36% of the time; and Neural-HMM is mistaken for VITS 21% of the time. In each of these cases, the predicted and actual acoustic models have a high degree of overlapping voice clusters in the test set. This indicates that the acoustic model embeddings are capturing voice information, and systems that share a common voice in the test set are more challenging to distinguish. In the vocoder task, the ResNet (E2E) model's performance on the different categories is high. The most mistaken category is bonafide, in which case VITS is mistakenly predicted 7% of the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Embedding visualization</head><p>Our top performing models' embeddings for the acoustic classification task using ASVspoof and MLAAD protocols are visualized using UMAP in Fig. <ref type="figure">3</ref>. Notably, the acoustic models in the MLAAD dataset exhibit more difficulty in separation. This </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fake biden robocall tells voters to skip new hampshire primary election -bbc news</title>
		<idno>05/03/2024</idno>
		<ptr target="https://www.bbc.com/news/world-us-canada-68064247" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Synthetic speech detection through short-term and long-term prediction traces</title>
		<author>
			<persName><forename type="first">C</forename><surname>Borrelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bestagini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Antonacci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tubaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Information Security</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An initial investigation for detecting vocoder fingerprints of fake audio</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1st International Workshop on Deepfake Detection for Audio Multimedia</title>
		<meeting>of the 1st International Workshop on Deepfake Detection for Audio Multimedia</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distinguishing neural speech synthesis models through fingerprints in speech waveforms</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/2309.06780</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Source tracing: Detecting voice spoofing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asia-Pacific Signal and Information Processing Association Annual Summit and Conference</title>
		<meeting>Asia-Pacific Signal and Information essing Association Annual Summit and Conference</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
		<respStmt>
			<orgName>APSIPA ASC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ADD 2023: the second audio deepfake detection challenge</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI 2023 Workshop on Deepfake Audio Detection and Analysis</title>
		<meeting>IJCAI 2023 Workshop on Deepfake Audio Detection and Analysis</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deepfake algorithm recognition system with augmented data for add 2023 challenge</title>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI 2023 Workshop on Deepfake Audio Detection and Analysis</title>
		<meeting>IJCAI 2023 Workshop on Deepfake Audio Detection and Analysis</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deepfake algorithm recognition through multi-model fusion based on manifold measure</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI 2023 Workshop on Deepfake Audio Detection and Analysis</title>
		<meeting>IJCAI 2023 Workshop on Deepfake Audio Detection and Analysis</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detecting unknown speech spoofing algorithms with nearest neighbors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI 2023 Workshop on Deepfake Audio Detection and Analysis</title>
		<meeting>IJCAI 2023 Workshop on Deepfake Audio Detection and Analysis</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vfd-net: Vocoder fingerprints detection for fake audio</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">155</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ASVspoof 2019: A large-scale public database of synthesized, converted and replayed speech</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vestman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page">101114</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalization of audio deepfake detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nagarsheth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Khoury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey 2020 The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey 2020 The Speaker and Language Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic speaker verification spoofing and deepfake detection using wav2vec 2.0 and data augmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. The Speaker and Language Recognition</title>
		<meeting>The Speaker and Language Recognition</meeting>
		<imprint>
			<publisher>Speaker Odyssey) Workshop</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved DeepFake Detection Using Whisper Features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Czuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szymański</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Syga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spoofprint: a new paradigm for spoofing attacks detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Khoury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="538" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CosFace: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Self-supervised spoofing audio detection scheme</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>INTERSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Siamese network with wav2vec feature for spoofing speech detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Investigating self-supervised front ends for speech spoofing countermeasures</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. The Speaker and Language Recognition</title>
		<meeting>The Speaker and Language Recognition</meeting>
		<imprint>
			<publisher>Speaker Odyssey) Workshop</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Anti-spoofing using transfer learning with variational information bottleneck</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Eom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Um</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IN-TERSPEECH</title>
		<meeting>IN-TERSPEECH</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Investigating active-learning-based training data selection for speech spoofing countermeasure</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT</title>
		<meeting>SLT</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The vicomtech audio deepfake detection system based on wav2vec2 for the 2022 add challenge</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Martín-Doñas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Álvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spoofed training data for speech spoofing countermeasure can be efficiently created using neural vocoders</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">AASIST: Audio anti-spoofing using integrated spectro-temporal graph attention networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">XLS-R: Selfsupervised cross-lingual speech representation learning at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Saraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust speech recognition via large-scale weak supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcleavey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A light CNN for deep face representation with noisy labels</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2884" to="2896" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A comparison of features for synthetic speech detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hanilc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ASVspoof 2019: Future horizons in spoofed and fake audio detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vestman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Mlaad: The multi-language audio anti-spoofing dataset</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Choong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gölge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Syga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Böttinger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.09512</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Coqui TTS</title>
		<author>
			<persName><forename type="first">G</forename><surname>Eren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">The</forename><surname>Coqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tts</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://github.com/coqui-ai/TTS" />
		<imprint>
			<date type="published" when="2021-01">Jan. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The m-ailabs speech dataset</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M S</forename><surname>Dataset</surname></persName>
		</author>
		<idno>05/03/2024</idno>
		<ptr target="https://www.caito.de/2019/01/03/the-m-ailabs-speech-dataset/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Desplanques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thienpondt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Demuynck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Signal estimation from modified shorttime fourier transform</title>
		<author>
			<persName><forename type="first">D</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="243" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multiband melgan: Faster waveform generation for high-quality textto-speech</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Proc. Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural HMMs are all you need (for high-quality attention-free TTS)</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Székely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beskow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Henter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">OverFlow: Putting flows on top of neural transducers for better TTS</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirkland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lameris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beskow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Éva</forename><surname>Székely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Henter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
