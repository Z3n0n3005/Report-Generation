{
  "id": 9055119499744609501,
  "name": "Urteaga_and_Wiggins_-_2024_-_Sequential_Monte_Carlo_Bandits.grobid.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content": "the multi-armed bandit (mab) problem considers the sequential strategy one must devise when playing a row of slot machines: i.e., which arm to play next to maximize cumulative returns.\nthis analogy extends to a wide range of real-world challenges that require online learning, while simultaneously maximizing some notion of reward.the arm may be a medicine a doctor must prescribe to a patient, the reward being the outcome of such treatment on the patient; or the set of resources a manager needs to allocate for competing projects, with the reward being the revenue attained at the end of the month; or the ad/product/content an online recommendation algorithm must display to maximize click-through rate in e-commerce.the contextual mab, where at each interaction with the world side information (known as 'context') is available, is a natural extension of this abstraction.\nthe 'context' is the physiology of the patient, the type of resources available for each project, or the features of the website user.sequential decision processes have been studied for many decades, and interest has resurged incited by reinforcement learning (rl) advancements developed within the machine learning community.\nrl] has been successfully applied to a variety of domains, from monte carlo tree searchand hyperparameter tuning for complex optimization in science, engineering and machine learning problems, to revenue maximizationand marketing solutionsin business and operations research.\nrl is also popular in e-commerce and digital services, improving online advertising at linkedin, engagement with website services at amazon, recommending targeted news at yahoo, and enabling full personalization of content and art at.the techniques used in these success stories are grounded on statistical advances on sequential decision processes and multi-armed bandits.\nthe mab crystallizes the fundamental trade-off between exploration and exploitation in sequential decision making.\nover the years, several algorithms have been proposed -we provide an overview of state-of-the-art solutions in section 2.1.\nhowever, applied use cases raise challenges that these mab algorithms often fail to address.for instance, classic mab algorithms do not typically generalize to problems with nonlinear reward dependencies or non-gaussian reward distributions, as exact computation of their statistics of interest is intractable for distributions not in the exponential family.\nmore importantly, these algorithms are commonly designed under the assumption of stationary reward distributions, i.e., the reward function does not change over-time, a premise often violated in practice.we hereby relax these constraints, and consider time-varying models and nonlinear reward functions.\nwe propose to use sequential monte carlo (smc) for non-stationary bandits with nonlinear rewards, where the world -the reward function-is time-varying, and rewards are sequentially observed for the played arms.smc methods] have been widely used to estimate posterior densities and expectations in sequential problems with probabilistic models that are too complex to treat analytically, with many successful applications in science and engineering.in bayesian mab algorithms, the agent must compute sufficient statistics of each arm's rewards over time, for which sequential updates to the posterior of the parameters of interest must be computed.\nwe here show that smc-based, sequentially updated random measures of per-arm parameter posteriors, enable computation of any statistic a bayesian mab policy might require.we generalize existing mab policies beyond their original stationary setting, and accommodate complex reward models: those for which sampling may be performed even if analytic computation of summary statistics is infeasible.\nwe study latent dynamical systems with non-gaussian and nonlinear reward functions, for which smc computes accurate posterior approximations.\nconsequently, we devise a flexible smc-based framework for solving non-stationary and nonlinear mabs.our contribution is a smc-based mab framework that:(i) computes smc-based random measure posterior mab densities utilized by bayesian mab policies;(ii) requires knowledge of the reward function only up to a proportionality constant, i.e., it accommodates nonlinear and non-gaussian bandit rewards; and, (iii) is applicable to time-varying reward models, i.e., to restless or non-stationary multiarmed bandits.the proposed smc-based mab framework (i) leverages smc for both posterior sampling and estimation of sufficient statistics utilized by bayesian mab policies, i.e.,addresses restless bandits via the general linear dynamical system, and accommodates unknown parameters via rao-blackwellization; and (iii) targets nonlinear and non-gaussian reward models, accommodating stateless and context-dependent, discrete and continuous reward distributions.we introduce in section 2 the preliminaries for our work, which combines sequential monte carlo techniques described in section 2.2, with multi-armed bandit algorithms detailed in section 2.1.\nwe present the smc-based mab framework in section 3, and evaluate its performance for thompson sampling and bayes-upper confidence bound policies in section 4.\nwe summarize and conclude with promising research directions in section 5."
    },
    {
      "header": "Background and preliminaries",
      "content": "the above stochastic mab formulation covers stationary bandits (if parameters are constant over time, i.e., \u03b8 * t,a = \u03b8 * a , \u2200t) and non-contextual bandits, by fixing the context to a constant value x t = x, \u2200t.with knowledge of the true bandit model, i.e., the \u03b8 * t \u2208 \u03b8 that parameterizes the reward distribution of the environment, the optimal action to take iswhere \u00b5 t,a (x t , \u03b8 * t ) = e {y |a, x t , \u03b8 * t } is each arm's conditional reward expectation, given context x t and true parameters \u03b8 * t , at time t.\nmab policies choose the next arm to play, with the goal of maximizing attained rewards, based upon the history observed so far.we use \u03c0(a) to denote a multi-armed bandit policy, which is in general stochastic -a is a random variable-on its choices of arms, and is dependent on previous history: \u03c0(a) = p (a = a|h 1:t ) , \u2200a \u2208 a.\nprevious history h 1:t contains the set of contexts, played arms, and observed rewards up to time t, denoted asgiven history h 1:t , a mab policy \u03c0(a|h 1:t ) aims at maximizing its cumulative rewards, or equivalently, minimizing its cumulative regret (the loss incurred due to not knowing the best arm a * t at each time t), i.e., r t = t t=1 y t,a * t -y t,at , where a t denotes the realization of the policy \u03c0(a|h 1:t ) -the arm picked by the policy-at time t.\nnamely, the agent views the unknown parameter of the reward function \u03b8 t as a random variable, and as data from bandit interactions with the environment are collected, a bayesian policy updates its parameter posterior.\nbecause a bandit agent must take into account the uncertainty on the unknown parameters, prior knowledge on the reward model and its parameters can be incorporated into bayesian policies, capturing the full state of knowledge via the parameter posteriorwhere p at (y t |x t , \u03b8 t ) is the likelihood of the observed reward y t after playing arm a t at time t.computation of this posterior is critical for bayesian mab algorithms.in thompson sampling, one uses p(\u03b8 t |h 1:t ) to compute the probability of an arm being optimal, i.e., \u03c0(a|x t+1 , h 1:t ) = p a = a * t+1 |x t+1 , \u03b8 t , h 1:t , where the uncertainty over the parameters must be accounted for.namely, one marginalizes the posterior parameter uncertainty after observing history h 1:t up to time instant t, i.e.,(5)in bayes-ucb, p(\u03b8 t |h 1:t ) is critical to determine the distribution of the expected rewards, i.e.,which is required for computation of the expected reward quantile q t+1,a (\u03b1 t ), formally defined aswhere the quantile value \u03b1 t may depend on time, as proposed by.\nthese issues become even more imperative when dealing with dynamic parameters, i.e., in environments that evolve over time, and with nonlinear reward distributions.to extend mab algorithms to more realistic scenarios, many have considered flexible reward functions and bayesian inference.\nsolutions that approximate the unknown bandit reward function with finiteor countably infinite gaussian mixture modelshave also been proposed.however, all these algorithms for mabs with complex rewards assume stationary distributions.the study of bandits in a changing world go back to the work by whittle, with subsequent theoretical efforts by many on characterizing restless, or non-stationary, bandits.\noften, these impose a reward 'variation' constraint on the evolution of the arms, or target specific reward functions, such as bernoulli rewards in, where discounting parameters for the prior beta distributions can be incorporated.more flexible restless bandit models, based on the brownian motion or discrete random walks, and simple markov modelshave been proposed, showcasing the trade-off between the time horizon and the rate at which the reward function varies.\nbesides, theoretical performance guarantees have been recently established for thompson sampling in restless environments where the bandit is assumed to evolve via a binary state markov chain, both in the episodicand non-episodicsetting.here, we overcome constraints on both the bandit's assumed reward function and its time-evolving model, by leveraging sequential monte carlo (smc).\nthe use of smc in the context of bandit problems was previously considered for probitand softmaxreward models, and to update latent feature posteriors in a probabilistic matrix factorization model.showed that utilizing smc to compute posterior distributions that lack an explicit closed-form is a theoretically grounded approach for certain online learning problems, such as bandit subset arm selection or job scheduling tasks.these efforts provide evidence that smc can be successfully combined with thompson sampling, yet are different in scope from our work.\ncontrary to existing mab solutions, the smc-based bandit policies we propose (i) are not restricted to specific reward functions, but accommodate nonlinear and non-gaussian rewards, (ii) address non-stationary bandit environments, and (iii) are readily applicable to state-ofthe-art bayesian mab algorithms -thompson sampling and bayes-ucb policies-in a modular fashion.monte carlo (mc) methods are a family of numerical techniques based on repeated random sampling, which have been shown to be flexible enough for both numerical integration and drawing samples from complex probability distributions of interest.with importance sampling (is), one estimates properties of a distribution when obtaining samples from such distribution is difficult.\nin smc, one considers a proposal distribution that factorizes -often, but not necessarily-over time, i.e., q(\u03c6 0:t ) = q(\u03c6 t |\u03c6 1:t-1 )q(\u03c6 1:t-1 ) = t \u03c4 =1 q(\u03c6 \u03c4 |\u03c6 1:\u03c4 -1 )q(\u03c6 0 ) ,which helps in matching the sequential form of the probabilistic model of interest p(\u03c6 t |\u03c6 1:t-1 ), to enable a recursive evaluation of the importance sampling weightsone problem with following the above weight update scheme is that, as time evolves, the distribution of the importance weights becomes more and more skewed, resulting in few (or just one) non-zero weights.to overcome this degeneracy, an additional selection step, known as resampling, is added."
    },
    {
      "header": "SMC for multi-armed bandits",
      "content": "we use sequential monte carlo to compute posteriors and sufficient statistics of interest for a rich-class of mabs: non-stationary bandits (modeled via linear dynamical systems) with complex (stateless and context-dependent) nonlinear reward functions, subject to non-gaussian stochastic innovations.we model non-stationary, stochastic mabs in a state-space framework, where for a given reward distribution p a (y |x, \u03b8), and parameters that evolve in-time via a transition distribution p(\u03b8 t |\u03b8 t-1 ), we writewhere we explicitly indicate with \u03b8 * t the true yet unknown parameters of the non-stationary multi-armed bandit.within this bandit framework, a bayesian policy must characterize the posterior of the unknown parameters p(\u03b8 t |h 1:t ) as in equation (4), in which the time-varying dynamics of the underlying transition distribution are incorporated.the posterior of interest given observed reward y t can be written aswhererecall that the parameter predictive distribution p(\u03b8 t |h 1:t-1 ) and parameter posterior p(\u03b8 t |h 1:t ) in equation () have analytical, closed-form recursive solutions only for limited cases.we adhere to the standard mab formulation, in that each arm of the bandit is described by its own idiosyncratic parameters (no information is shared across arms)), yet we allow for such parameters to evolve independently per-arm in time:.\nthe challenge is on computing the posteriors in equations (), () and () for a variety of mab models, for which smc enables us to accommodate (i) any likelihood function that is computable up to a proportionality constant, and (ii) any time-varying model described by a transition density from which we can draw samples.we use smc to compute per-arm parameter posteriors at each bandit round, i.e., we approximate per-arm filtering densities p(\u03b8 t,a |h 1:t ) with smc-based random measures p m (\u03b8 t,a |h 1:t ), \u2200a, for which there are strong theoretical convergence guarantees.the dimensionality of this estimation problem depends on the size of per-arm parameters, and not on the number of bandit arms |a|.\nthroughout, we avoid assumptions on model parameter knowledge and resort to their bayesian marginalization.we combine smc with both thompson sampling and bayes-ucb policies, by sequentially updating, at each bandit interaction t, a smc-based random measure to approximate the time-varying posterior of interest,knowledge of p m (\u03b8 t,a |h 1:t ) enables computation of any per-arm reward statistic bayesian mab policies require.we present algorithm 1 with the sequential importance resampling (sir) methodas introduced by, where:\u2022 the smc proposal distribution q(\u2022) at each bandit interaction t obeys the assumed parameter dynamics: \u03b8\u2022 smc weights are updated based on the likelihood of the observed rewards: wt,a ) -step (9.c) in algorithm 1; and \u2022 the smc random measure is resampled at every time instant -step (9.a).independently of which smc technique is used to compute the posterior random measure p m (\u03b8 t,a |h 1:t ), the fundamental operation in the proposed smc-based mab algorithm 1 is to sequentially update the random measure p m (\u03b8 t,a |h 1:t ) to approximate the true per-arm posterior p(\u03b8 t,a |h 1:t ) over bandit interactions.this smc-based random measure is key, along with transition density p(\u03b8 t,a |\u03b8 t-1,a ), to sequentially propagate parameter posteriors per-arm, and to estimate their sufficient statistics for any bayesian bandit policy.\nthis smc-based random measure provides an accurate approximation to the true posterior density with high probability.\u2022 smc-based bayes-ucb: we extend bayes-ucb to reward models where the quantile functions are not analytically tractable, by leveraging the smc-based parameter predictive posterior random measure p m (\u03b8 t+1 |h 1:t ).we compute the quantile function of interest by first evaluating the expected reward at each round t based on the available posterior samples, i.e., \u00b5the convergence of quantile estimators generated by smc methods has been explicitly proved in.random measure p m (\u03b8 t+1,a |h 1:t ) in equation () enables computation of the statistics bayesian mab policies require, extending their applicability from stationary to time-evolving bandits.\ndecide next action a t+1 to play for thompson sampling:forbayes-ucb:observe reward y t+1 for played arm 9:update the posterior smc random measure p m (\u03b8 t,a |h 1:t ) for all armsper arm a \u2208 a, where m \u2032 t,a is drawn with replacement according to the importance weights w(c) weight samples of the played arm a t+1 based on the likelihood of observed y t+1 w (mt+1,a t+1 )and normalize the weights10: end forthe marginalized transition density is a multivariate t-distribution:whereeach distribution above holds separately for each arm a, and subscript a,0 indicates assumed prior parameters for arm a.these transition distributions are used when propagating per-arm parameter densities in steps 5 and 9 of algorithm 1.\nthe contextual linear gaussian reward model is well studied in the bandit literature, where the expected reward of each arm is modeled as a linear combination of a d-dimensional context vector x \u2208 r d , and the idiosyncratic parameters of the arm w a \u2208 r d ; i.e.,we denote with \u03b8 \u2261 {w, \u03c3} the set of all parameters of the reward distribution, and consider the normal inverse-gamma conjugate prior distribution for these,after observing actions a 1:t and rewards y 1:t , the parameter posterior for each arm p(w a , \u03c3 2 a |a 1:t , y 1:t , u 0,a , v 0,a , \u03b1 0,a , \u03b2 0,a ) = p w a , \u03c3 2 a |u t,a , v t,a , \u03b1 t,a , \u03b2 t,afollows an updated normal inverse-gamma distribution with sequentially updated hyperparametersor, alternatively, batch updates of the formwhere t a = {t|a t = a} indicates the set of time instances when arm a is played.with these, we can compute the bayesian expected reward of each arm,and the quantile function for such distributionthe reward variance \u03c3 2 a is unknown in practice, so we marginalize it and obtain p(\u00b5 a |x, u t,a , v t,a ) = t \u00b5 a 2\u03b1 t,a , x \u22a4 u t,a , \u03b2t,a \u03b1t,awhich leads to quantile function computations based on a student's t-distributionequations () and () are needed in step 5 when implementing ts or bayes-ucb policies for the known \u03c3 2 a case; while equations () and () are used for the unknown \u03c3 2 a case."
    },
    {
      "header": "Evaluation",
      "content": "results showcase satisfactory performance across a wide range of stationary bandit parameterizations and sizes, as smc-based policies achieve the right exploration-exploitation tradeoff.for results we present below, we simulate different parameterizations of dynamic linear models described in section 3.2, and present results for a variety of mab environments with reward functions detailed in sections 4.1, 4.2 and 4.3.\nsection 4.4 illustrates the ability of smc-based bandit policies to capture non-stationary trends in personalized news article recommendations.the main evaluation metric is the cumulative regret of the bandit agent, as defined in equation (), with results averaged over 500 realizations.\nwe present results for smcbased policies with m = 2000 samples, and provide an evaluation of the impact of m in appendix b.we simulate the following two-armed, contextual (x t \u2208 r 2 , \u2200t), linear gaussian bandit:\u03b8 t,a=0,0 \u03b8 t,a=0,1 = 0.9 -0.1 -0.1 0.9where \u03f5 a=0 \u223c n (\u03f5|0, 0.01 \u2022 i) , p(\u03b8 t,a=1 |\u03b8 t-1,a=1 ) :\u03b8 t,a=1,0 \u03b8 t,a=1,1 = 0.9 0.1 0.1 0.9where \u03f5 a=1 \u223c n (\u03f5|0, 0.01\u03b8 t,a=0,0 \u03b8 t,a=0,1 = 0.5 0.0 0.0 0.5where \u03f5 a=0 \u223c n (\u03f5|0, 0.01 \u2022 i) , p(\u03b8 t,a=1 |\u03b8 t-1,a=1 ) :\u03b8 t,a=1,0 \u03b8 t,a=1,1 = 0.9 0.1 0.1 0.9where \u03f5 a=1 \u223c n (\u03f5|0, 0.01(43) the expected rewards driven by the dynamics of equations () and () change over time, inducing switches on the identity of the optimal arm.\nfor instance, for a given realization of scenario a shown in figure, there is an optimal arm swap between time-instants t = (300, 550), with arm 1 becoming the optimal for all t \u2265 600; for a realization of scenario b illustrated in figure, there is an optimal arm change around t = 100, a swap around t = 600, with arm 1 becoming optimal again after t \u2265 1600.empirical results for smc-based bayesian policies in scenarios described by equations () and () are shown in figuresand.we study linear dynamics with gaussian reward distributions with known parameters in figure, of interest as it allows us to validate the smc-based random measure in comparison to the optimal, closed-form posterior -the kalman filter-under the assumption of known dynamic parameters.we observe satisfactory cumulative regret performance in figure: i.e., smc-based bayesian agents' cumulative regret is sublinear.\nwe illustrate in figurea more realistic scenario, where the dynamic parameterization is unknown to the bandit agent.we observe in figuresthat, in the case of unknown reward variances (\u03c3 2 a , \u2200a), smc-based policies perform comparably well.\nthe regret loss associated with the uncertainty about \u03c3 2 a is minimal for smc-based bayesian agents, and does not hinder the ability of the proposed smc-based policies to find the right exploration-exploitation balance: i.e., regret is sublinear, and the agents adapt to switches in the identity of the optimal arm.we illustrate in figures 2e-2f the most realistic, yet challenging, non-stationary contextual gaussian bandit case: one where none of the parameters of the model are known.\nhowever, smc-based thompson sampling and bayes-ucb agents are able to learn the evolution of the dynamic latent parameters, and the corresponding time-varying expected rewards, with enough accuracy to attain good exploration-exploitation balance: i.e., sublinear regret curves indicate the agent identified and played the optimal arm repeatedly.\non the contrary, smc-based bayesian policies can easily accommodate this setting, by updating posterior random measures p m (\u03b8 t |h 1:t ) as in algorithm 1, for both stationary (evaluated in appendix a.3) and non-stationary bandits we report here.figureillustrates how smc-based bayesian policies adapt to non-stationary optimal arm switches under contextual, binary reward observations, achieving sublinear regret.\neven though this is a particularly challenging problem, presented evidence suggests that smc-based policies do learn the underlying latent dynamics from contextual binary rewards.notably, proposed policies are able to successfully identify which arm to play: i.e., both smc-based ts and smc-based ucb -with no dynamic parameter knowledge-are able to flatten their regret for t \u2265 650 in figureand)-().\nsmc-based bayesian policies adapt to these changes and balance the exploration-exploitation tradeoff.we observe that smc-based thompson sampling and bayes-ucb are able to reach satisfactory exploitation-exploration balance, i.e., the algorithms dynamically adapt their choice of which arm to play, and attain sublinear cumulative regret.recall the linear increase in cumulative regret (i.e., exploration) when latent parameter dynamics result in changes in the optimal arm's identity: around t \u2208 (800, 1000) in figure, and around t \u2208 (1250, 1500) in figure.\nafter updating the random measure posterior over the unknown latent parameters, and recomputing the expected rewards per-arm, smc-based policies are able to slowly adapt to the optimal arm changes, reaching a new exploitationexploration balance, i.e., flattening the cumulative regret curves.for the most interesting and challenging setting where the dynamic model's parameters are unknown, we observe an increase in cumulative regret for both smc-based policies.\nonly when posteriors over these -used by the smc-based agents to propagate uncertainty to each bandit arms' expected reward estimates-are improved, can smc-based policies make informed decisions and attain sublinear regret.we observe that the impact of expected reward changes, when occurring later in time (e.g., t \u2248 1250 in figure) is more pronounced for smc-based bayes-ucb policies.\nwe evaluate both stationary and non-stationary bandits with logistic rewards.as shown in figure, we observe the flexibility of a non-stationary logistic bandit model, where we notice how the smc-based ts agent is able to pick up the dynamic popularity of certain articles over time -averaged ctr results are provided in table.ctr normalized ctr logistic rewards, static arms 0.0670 +/-0.0088 1.6095 +/-0.2115 logistic rewards, time-evolving arms 0.0655 +/-0.0082 1.5745 +/-0.2064 table: ctr results for smc-based policies on the news article recommendation dataset."
    },
    {
      "header": "Conclusion and discussion",
      "content": "we presented a sequential monte carlo (smc)-based framework for multi-armed bandits (mabs), where we combine smc inference with state-of-the-art bayesian bandit policies.\nwe extend the applicability of bayesian mab policies -thompson sampling and bayes-ucbto previously elusive bandit environments, by accommodating nonlinear and time-varying models of the world, via smc-based inference of the sufficient statistics of interest.the proposed smc-based bayesian mab framework allows for interpretable modeling of nonlinear and time-evolving reward functions, as it sequentially learns the sufficient statistics and dynamics of the bandit from online data, to find the right exploration-exploitation balance.\nempirical results show good cumulative regret performance of the proposed policies in simulated mab environments that previous algorithms can not address, and in practical scenarios (personalized news article recommendation) where time-varying models of data are required.we show that smc-based posterior random measures are accurate enough for bayesian bandit policies to find satisfactory exploration-exploitation tradeoffs.\nthe proposed smcbased bayesian agents do not only estimate the evolving latent parameters, but also quantify how their uncertainty maps to the uncertainty over the optimality of each arm, adjusting to non-stationary environments.\ncareful computation of smc random measures is fundamental for the accuracy of the sequential approximation to the posteriors of interest, and the downstream performance of the proposed smc-based mab policies.\nthe time-varying uncertainty of the sequentially updated parameter posterior encourages exploration of arms that have not been played recently, but may have reached new exploitable rewards.\nnamely, as the posteriors of unobserved arms result in broader smc posteriors, smc-based bayesian mab policies are more likely to explore such arm, reduce their posterior's uncertainty, and in turn, update the exploration-exploitation balance.important future work remains on the theoretical understanding of thompson sampling and bayes-ucb within the proposed smc-based mab framework.\ngiven that smc posteriors converge to the true posterior under suitable conditions, we hypothesize that the proposed smc-based bandit policies can achieve sub-linear regret, under appropriate assumptions on the latent dynamics.on the one hand,have shown that a logarithmic regret bound holds for thompson sampling in complex problems, for bandits with discretely-supported priors over the parameter space without additional structural properties, such as conjugate prior structure or independence across arms.\non the other, regret of a non-stationary bandit agent is linear if optimal arm changes occur continuously or adversarially.\nhowever, as long as the bandit's latent dynamics incur in a controlled number of optimal arm changes, smc can provide accurate enough posteriors to find the right exploration-exploitation tradeoff, as we show empirically here.a theoretical analysis of the dependency between the dynamic bandit model's characteristics, the resulting rate of optimal arm changes, and smc posterior convergence guarantees, leading to formal regret bounds for the proposed smc-based bayesian policies, is an open research direction.we here apply the proposed smc-based bayesian policies as in algorithm 1 to the original settings where thompson sampling and bayes-ucb were derived, i.e., for stationary bandits with bernoulli and contextual, linear gaussian reward functions;;;.empirical results for these bandits is provided in section a.2, while the stationary logistic bandit case is evaluated in section a.3, where we also evaluate the impact of sample size m in the smc-based bandit algorithms.in stationary bandits, there are no time-varying parameters, i.e., \u03b8 t = \u03b8, \u2200t.\nfor these cases, sir-based parameter propagation becomes troublesome.\nto mitigate such issues, several alternatives have been proposed in the smc community: e.g., artificial parameter evolution, kernel smoothing, and density assisted techniques.we implement density assisted smc, rather than kernel based particle filters as in, where one approximates the posterior of the unknown parameters with a density of choice.\ndensity assisted importance sampling is a well studied smc approach that extends random-walking and kernel-based alternatives;;, with its asymptotic correctness guaranteed for the static parameter case.\nwe acknowledge that any of the smc techniques that further mitigate the challenges of estimating constant parameters (e.g., parameter smoothing;;or nested smc methods;) can only improve the accuracy of the implemented smc-based policies.more precisely, we approximate the posterior of the unknown parameters, given the current state of knowledge, with a gaussian distributionwe present below cumulative regret results for different parameterizations of 5-armed bernoulli bandits."
    }
  ]
}
