{
  "id": "2401.040",
  "name": "2401.04088v1.pdf.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content": "In this paper, we present Mixtral 8x7B, a sparse mixture of experts model Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the \"experts\") to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, Figure Mixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of the information in the sequence.We also present Mixtral 8x7B -Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization We release both Mixtral 8x7B and Mixtral 8x7B -Instruct under the Apache 2.0 license"
    },
    {
      "header": "Architectural details",
      "content": "The model architecture parameters are summarized in Table We present a brief overview of the Mixture of Experts layer (Figure Here, G(x) i denotes the n-dimensional output of the gating network for the i-th expert, and E i (x) is the output of the i-th expert network. If the gating vector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple alternative ways of implementing G(x) where (TopK(\u2113)) i := \u2113 i if \u2113 i is among the top-K coordinates of logits \u2113 \u2208 R n and (TopK(\u2113)) i := -\u221e otherwise. The value of K -the number of experts used per token -is a hyper-parameter that modulates the amount of compute used to process each token. If one increases n while keeping K fixed, one can increase the model's parameter count while keeping its computational cost effectively constant. This motivates a distinction between the model's total parameter count (commonly referenced as the sparse parameter count), which grows with n, and the number of parameters used for processing an individual token (called the active parameter count), which grows with K up to n.MoE layers can be run efficiently on single GPUs with high performance specialized kernels. For example, Megablocks In a Transformer model, the MoE layer is applied independently per token and replaces the feed-forward (FFN) sub-block of the transformer block. For Mixtral we use the same SwiGLU architecture as the expert function E i (x) and set K = 2. This means each token is routed to two SwiGLU sub-blocks with different sets of weights. Taking this all together, the output y for an input token x is computed as:This formulation is similar to the GShard architecture "
    },
    {
      "header": "Results",
      "content": "We compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair comparison. We measure performance on a wide variety of tasks categorized as follow:\u2022 Commonsense Reasoning (0-shot): Hellaswag OpenbookQA Figure Size and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral models' efficiency in the cost-performance spectrum (see Figure Note that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.Comparison with Llama 2 70B and GPT-3.5. In Table Compared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table To assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey retrieval task introduced in We benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report the results in Table "
    },
    {
      "header": "Instruction Fine-tuning",
      "content": "We train Mixtral -Instruct using supervised fine-tuning (SFT) on an instruction dataset followed by Direct Preference Optimization (DPO) "
    },
    {
      "header": "Routing analysis",
      "content": "In this section, we perform a small analysis on the expert selection by the router. In particular, we are interested to see if during training some experts specialized to some specific domains (e.g. mathematics, biology, philosophy, etc.).To investigate this, we measure the distribution of selected experts on different subsets of The Pile validation dataset Only for DM Mathematics we note a marginally different distribution of experts. This divergence is likely a consequence of the dataset's synthetic nature and its limited coverage of the natural language spectrum, and is particularly noticeable at the first and last layers, where the hidden states are very correlated to the input and output embeddings respectively.This suggests that the router does exhibit some structured syntactic behavior. Figure We also note from Figure 7 \u2248 46% for \"First and second choice\". Repetitions at the first layer are close to random, but are significantly higher at layers 15 and 31. The high number of repetitions shows that expert choice exhibits high temporal locality at these layers. consecutive assignments is significantly higher than random for higher layers. This has implications in how one might optimize the model for fast training and inference. For example, cases with high locality are more likely to cause over-subscription of certain experts when doing Expert Parallelism. Conversely, this locality can be leveraged for caching, as is done in "
    },
    {
      "header": "Conclusion",
      "content": "In this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-theart performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gemini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned models publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the development of new techniques and applications that can benefit a wide range of industries and domains.   "
    }
  ]
}
