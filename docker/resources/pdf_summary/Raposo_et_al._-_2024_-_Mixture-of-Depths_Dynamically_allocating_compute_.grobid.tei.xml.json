{
  "id": 295023975944239871,
  "name": "Raposo_et_al._-_2024_-_Mixture-of-Depths_Dynamically_allocating_compute_.grobid.tei.xml",
  "segments": [
    {
      "header": "Introduction",
      "content": "and yet, transformer models expend the same amount of compute per token in a forward pass.\nthe most promising conditional computation methods may instead be those that are harmonious with our current hardware stack, which prioritizes static computation graphs, and known tensor sizes that are selected to maximize hardware utilization.here we consider the problem of language modeling using a static compute budget that can be made less than that used by a vanilla transformer.\nthe network must learn how to dynamically allocate the available compute by making decisions per-token, in each layer, about where to spend compute from the available budget.\nas we will show, these gains can be had without sacrificing overall performance.we leverage an approach akin to mixture of experts (moe) transformers, in which dynamic token-level routing decisions are made across the network depth.\ndeparting from moe, we choose to either apply a computation to a token (as would be the case for a standard transformer), or pass it through a residual connection (remaining unchanged and saving compute).\nwe refer to this strategy as mixture-of-depths (mod) to emphasize how individual tokens pass through different numbers of layers, or blocks, through the depth of the transformer (see figure).the mod technique also allows one to trade-off performance with speed.\non the other hand, one can train an mod transformer that achieves training loss parity with an isoflop optimal vanilla transformer, but which uses a fraction of the flops (upwards of 50%) per forward pass, and hence is faster to step.\ntogether, these results imply that mod transformers learn to route intelligently (i.e., skipping computations that are unnecessary) since they can achieve equal or better log probabilities per sequence despite a smaller flop footprint per forward pass."
    },
    {
      "header": "Background",
      "content": "this has spurred tremendous interest in making transformer architectures more efficient.\none of the promising approaches is conditional computation, whereby learned mechanisms determine when and how to expend computation.\nthis terminology was introduced by bengio (2013), and the concept was explored further over the next several years.a wide variety of recent work has developed conditional computation methods for transformers.\nsome of this work focuses on \"early exiting\", that is, learning to decide when to end computation on a given token, allowing the token to skip any remaining transformer layers after the exit decision is made.\nin mod, unlike in early-exit methods, a token can skip middle layers, then be updated via self-attention with tokens that that have gone through all the middle layers.\nwe speculate that this might be a useful property.other work has developed methods for iterating transformer layers with shared weights for an adaptive number of steps.developed a method for choosing tokens to merge when running inference on a trained vision transformer which notably requires no learning.make use of conditional computation in a fine tuning setting by building on adapter approachesto learn to skip blocks of frozen pre-trained weights in favor of running only a small fine-tuned adapter.colt5uses conditional routing to select whether a given token will pass through a heavy or light pathway for each feedforward layer.\nfurther, they use the same routing mechanism to select whether a token will attend to all other tokens or to a select few, as in.\nlike mod, colt5 uses soft top-k for making routing decisions.\nin contrast, our current work with since some tokens take this second route, mixture-of-depths (mod) transformers have a smaller total flop footprint compared to vanilla or moe transformers.\non the top right is depicted a trained model's routing decisions for a short sequence truncated to 64 tokens for visualization purposes.\nwhen examining the choices one can find tokens processed by later blocks' layers, despite passing through relatively few total blocks throughout the model's depth.\nthis is a unique feature of mod compared to conventional halting-based, or \"early-exit\" conditional computation, which instead engage blocks serially, or vanilla transformers, which engage every block.mod focuses on the decoder-only setting, and so we propose a predictive router to enable efficient inference for conditional computation in transformers.one successful formulation of conditional computation is the the \"mixture-of-experts\" layer (moe) as introduced by.\ndeveloped initially in the context of lstms, later work showed compelling empirical results for moe with transformers.\nunlike other conditional computation approaches that try to conserve or expend additional compute, moe transformers use conditional logic to route tokens to one of many expert mlps while keeping total compute expenditure constant.\nour mixture-of-depths method can be thought of as using the routing logic from moe transformers, but rather than having multiple experts, mod deploys a single expert which can be dynamically skipped."
    },
    {
      "header": "Implementing Mixture-of-Depths Transformers",
      "content": "see section 3.3.we then discuss some complications when sampling post-training in section 3.5.to enforce a total compute budget per forward pass we leverage the notion of capacity, which defines the total number of tokens that comprise the input to a given computation (e.g., the tokens participating in self-attention, a given expert in moe transformers, etc).\nfor example, the self-attention and mlp in each vanilla transformer block have a capacity of \ud835\udc47-the total number of tokens across the sequence and batch.\nbut, since they use multiple experts per block, their total capacity is approximately equal to that of a vanilla transformer.generally, it is the token capacity that determines the total flops for transformers that use conditional computation, rather than the outcomes of any routing decisions.\nthe former path is computationally expensive.the total number of flops per forward pass will be fewer than that in a vanilla transformer if we set the capacity for path (1) to be anything less than \ud835\udc47 (the total number of tokens across the sequence and batch).\nif we are correct that transformers often expend more compute than they need to make their predictions, then it is an empirical question as to how aggressively we can shrink each block's capacity, and hence, how many tokens we can afford to route around each block.there are two learned routing schemes we consider (see figure): token-choice and expert-choice.\nin token-choice routing, a router produces per-token probability distributions across computational paths (e.g., across expert identities in moe transformers).\nsecond, since the top-\ud835\udc58 operation depends on the magnitude of the router weights, this routing scheme allows for relative routing weights to help determine which tokens most need the block's computations; routers can try to ensure that the most critical tokens are among the top-\ud835\udc58 by setting their weight appropriately, which is not possible with token-choice routing schemes.\nhowever, because we use just a single path, we leverage the implicit knowledge that tokens will be dropped if \ud835\udc58 is less than the sequence length so that we can route tokens away from the self-attention and mlp computations, thus expending fewer flops in a given forward pass of the model.as a reminder of the high-level intuition, each token is processed by a router to produce a scalar weight, and the top-\ud835\udc58 weights are then used to choose the token identities that will route through a transformer's block, which comprises self-attention and the subsequent mlp.\nthis puts the router weights along the \"gradient path\", thus subjecting them to the forces of gradient descent through the course of the language modeling task (we experimented with versions where the router weights are also included along the computational path for those tokens that bypass the block's computations, but it seems to be sufficient-and implementationally simpler-to only include the router weights along the computational path for those tokens that do not bypass the block's computations).while expert-choice routing has a number of advantages, it has one distinct problem: the top-\ud835\udc58 operation is non-causal."
    },
    {
      "header": "Results",
      "content": "notably, model #3 achieves equal performance to the isoflop optimal baseline but steps 66% faster, due to the relatively fewer flops needed per forward pass.the isoflop optimal baseline (also 220m, figure 3 model #1), but is upwards of 60% faster to step during training.\nso, it seems the network is robust to significant capacity reductions as long as there is frequent opportunity for full capacity self-attention and mlp computations.learned routing is crucial, as mod transformers that use stochastic routing (implemented using a top-\ud835\udc58 operation on router weights sampled from a gaussian distribution) perform drastically worse than both the baseline and normal mod transformer (figure).depicted in figureis an isoflop analysis for 6e18, 2e19, and 1e20 total flops.\nnotably, there exist mod variants that are appreciably faster to step than the isoflop-optimal baseline (measured as steps per second when training on equivalent hardware) while also achieving a lower loss (in figurewe depict normalized flops per forward pass rather than wall-clock step time per se, but from our experiments the two are tightly correlated.\ndepicted on the right are the relative flops per forward pass (normalized to the isoflop optimal baseline).there exist mod variants that are both faster to step (by virtue of requiring fewer flops per forward pass) and better performing than the isoflop optimal baseline.\naltogether, then, there exist mod transformers that perform as well as isoflop-optimal baselines and are faster to step, both because they use fewer flops per parameter and because they use fewer parameters.figurealso reveals another important finding: the optimal mod transformer is that which uses as many flops per forward pass as the isoflop optimal baseline.\nthis finding allows one to directly predict which sized mod transformer will perform optimally for a given isoflop training budget: one just needs to tune the model size for a given mod configuration (i.e., capacity and routing frequency) to produce a model that uses as many flops per forward pass as the isoflop-optimal baseline, and they will have the optimally performing mod variant for that configuration.\nas in the training setting, there exist mod variants that are better performing than the isoflop-optimal baseline, while requiring fewer flops per forward pass."
    },
    {
      "header": "Discussion",
      "content": "rather, it is crucial to use learned routing decisions-much like in mixture-of-experts transformers-to determine whether a token should participate in self-attention and the subsequent mlp (requiring flops), or not (saving flops).we can then use any saved flops by, for example, making the model bigger or training it for longer.\nour results show that indeed flops may be inefficiently used in vanilla transformer models, and that there may be more efficient ways for them to be expended.learned routing mechanisms are sometimes non-causal; that is, information about the future is used to determine a given token's routing decision.\nhowever, top-k routing mechanisms present difficulties in post-training autoregressive sampling, where it is impossible to use information about future token identities to determine routing decisions.\neiher a simple auxiliary classifier, or auxiliary loss on the router, is sufficient to learn the top-\ud835\udc58 routing decisions such that it can mimic the top-\ud835\udc58 decisions during autoregressive intuitively, a token might learn to route around blocks because the prediction being made at that step is easier, and hence, does not require as much compute.\nthus, whether tokens decide to route or not impacts both the current step's prediction and future predictions via causal self-attention, and how the network balances these effects is guided by their influence on the overall language modeling objective.this insight opens the door to mod variants that decouple the routing for queries, keys and values.\nlearned routing could be a powerful mechanism for deciding which tokens these might be, perhaps funnelling them into a long-term memory buffer that is available during future self-attention.\nthis is more computationally efficient than performing a full content-based lookup across an entire memory buffer for each step in the future, and could be one step towards drastically increasing the context-length available for making a prediction.unlike moe transformers that route between effectively the same computation (usually mlps), mod transformers demonstrate the value of routing among different types of computations.\nin general, the routing machinery we deployed provides a knob for adjusting the types of computations available to the network and their relative cost (in total flops); if one wants to introduce an expensive computation, then this can be offset by setting its capacity to some small amount, and hence, by routing only a small number of tokens to it.altogether, mod transformers are another tool one can use to tune a model's compute per forward pass (and hence inference time)."
    }
  ]
}
