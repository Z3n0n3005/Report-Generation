{"id": -3799295574644242751, "name": "Onah_et_al._-_2023_-_A_Data-driven_Latent_Semantic_Analysis_for_Automat", "abstract_seg": "With the advent and popularity of big data mining and huge text analysis in modern 8mes, automated text summariza8on became prominent for extrac8ng and retrieving important informa8on from documents. This research inves8gates aspects of automa8c text summariza8on from the perspec8ves of single and mul8ple documents. Summariza8on is a task of condensing huge text ar8cles into short, summarized versions. The text is reduced in size for summariza8on purpose but preserving key vital informa8on and retaining the meaning of the original document. This study presents the Latent Dirichlet Alloca8on (LDA) approach used to perform topic modelling from summarised medical science journal ar8cles with topics related to genes and diseases. In this study, PyLDAvis webbased interac8ve visualiza8on tool was used to visualise the selected topics. The visualisa8on provides an overarching view of the main topics while allowing and aHribu8ng deep meaning to the prevalence individual topic. This study presents a novel approach to summariza8on of single and mul8ple documents. The results suggest the terms ranked purely by considering their probability of the topic prevalence within the processed document using extrac8ve summariza8on technique. PyLDAvis visualiza8on describes the flexibility of exploring the terms of the topics' associa8on to the fiHed LDA model. The topic modelling result shows prevalence within topics 1 and 2. This associa8on reveals that there is similarity between the terms in topic 1 and 2 in this study. The efficacy of the LDA and the extrac8ve summariza8on methods were measured using Latent Seman8c Analysis (LSA) and Recall-Oriented Understudy for Gis8ng Evalua8on (ROUGE) metrics to evaluate the reliability and validity of the model.", "segments": [{"header": "I. INTRODUCTION", "content": "However, this study presents a novel approach to topic modelling by performing extracDve summarizaDon on over 100 arDcles related to genes and associated diseases and feeding the summary as an input argument a Latent Dirichlet AllocaDon (LDA) model in order to perform the topic modelling.\nWith so many documents being extracted from social media, review comments from online plaWorms and microblogs as TwiVer, a huge amount of natural language data is being mined and are available to be analysed AutomaDc text summarizaDon is the process of performing specific NLP task by producing a concise summary of documents (single or mulDple) without any manual support while preserving the meaning or important points of the original document \u2022 How automated text summarizaDon techniques were used in an extracDve summary of arDcles?\u2022 How topic modelling models were used in producing emerging terms that are related to mulDple and different journal arDcles?\nThe experimental results show that the proposed model achieves good performance in terms of the document summary and the topic modelling informaDon retrieved from the full document."}, {"header": "II. RELATED WORK", "content": "Unless these text data are extracted and make meaning, then the most important and relevant informaDon would be lost.\nSummarizaDon is described as the process of presenDng huge data informaDon in a concise manner while focusing on the most useful secDons of the data whilst preserving the original meaning "}, {"header": "A. Summariza-on", "content": ""}, {"header": "B. Topic Modelling", "content": "Topic modelling is the process of labelling and describing documents into topics.\nThis is an unsupervised machine learning technique for abstracDng topics from collecDons of documents "}, {"header": "C. Latent Dirichlet Alloca-on", "content": ""}, {"header": "III. METHODS", "content": "This dicDonary of terms was used to build a vectorised corpus of lexicon LDA model.\nOne of the key approaches that was used in the experiment was the 'pyLDAvis.gensim.prepare' method which takes as an argument our LDA model, the corpus and the derived lexicon which contains the dicDonary terms for the study "}, {"header": "A. Model Descrip-on", "content": ""}, {"header": "1)", "content": ""}, {"header": "Sentscores[S] = Wordfreq[W]", "content": "(1)During the sentence model processing interval, the length of the sentence is either increased or reduced by certain values within the sentence scores dicDonary.\nTherefore, new sentences are added into the sentence dicDonary scores."}, {"header": "Sentscores[S]+ = Wordfreq[W]", "content": "(2)2) Word frequency:: DicDonary of word frequency corpus was generated within the model.\nThe word frequencies were selected automaDcally based on the prevalence or occurrence of the words in the corpus dicDonary created in the model (see Figure The next equaDon allows us to calculate the maximum word in the word frequencies (see equaDon 4)."}, {"header": "Word", "content": ""}, {"header": "B. Research Pipeline", "content": "The arDcles scraped from the web were all related to medical science research.\nPapers related to diseases and the mutated genes causaDon were extracted for this study "}, {"header": "2)", "content": "In the feature extracDon process, we parse the web arDcles source code in order to extract the textual material needed for the final summary.\nDuring the process of formaTng the clean arDcles, we performed extra filtering of special characters from the processed text in order to find and replace these symbols automaDcally."}, {"header": "3)", "content": "Stopwords: We further removed a list of stop-words from the propocessed arDcles.\nWords such as pronounce that are not necessary or essenDal for the final summary (see Figure "}, {"header": "4) Topic Modelling & Visualiza-on:", "content": ""}, {"header": "IV. MODEL", "content": ""}, {"header": "B. Defining Saliency Term", "content": "The possibility that the emerge word w was generated from the LDA topic model (TM).We also compute the marginal probability P(TM): -with the possibility that any word w 0 randomly selected was generated by TM."}, {"header": "U(6)", "content": ""}, {"header": "=5", "content": "The uniqueness of each term is described as how significance and semanDcally associated they are to the topics.\nFor example, a term could be semanDcally associated to more than one topic."}, {"header": "V. LATENT SEMANTIC ANALYSIS", "content": "Latent SemanDc Analysis (LSA) is a robust Algebraic and StaDsDcal method which extracts hidden semanDc structures of words and sentences."}, {"header": "A. Sample Extracted Summary", "content": "The heapq is used in implemenDng the priority queues for word frequencies in sentences with higher weight is given more priority in processing the summary."}, {"header": "B. Findings", "content": ""}, {"header": "VI. ROUGE: RELIABILITY & VALIDITY OF MODEL", "content": "ROUGE is a metric evaluaDon model which stands for Recall Oriented Understudy for Gisting Evaluation.\nIt is an intrinsic metric for automaDcally evaluaDng document summaries "}, {"header": "TABLE III ROUGE METRICS MEASUREMENT SUMMARIES", "content": "System and Human Annotated Summaries Type Summary SSummary 'Some of the genes in the BCAA metabolic pathway such as MLYCD (rank 164)HADHB (rank 354)IVD (rank 713)MUT (rank 921)and PCCB (rank 684) are also ranked highly by Hridaya."}, {"header": "Comparing the system generated summary with a new human", "content": ""}, {"header": "A. Procedure: Recall & Precision", "content": "In this study, we combined and computerised both the Precision and Recall and further report the F1 -score measure.In order to ascertain the validity of the study, we measured ROUGE-N, ROUGE -S and ROUGE -L which are the granularity of texts that was compared between the system summaries and the reference or human annotated model summaries."}, {"header": "VII. RESULTS & FINDINGS", "content": "The terms in the topic modelling show text which are mostly frequent in the document these were depicted by the size of the circle (as seen in Figures The distance between two or more topics is an approximaDon of their semanDc relaDonship."}, {"header": "IX. DISCUSSION", "content": ""}, {"header": "X. CONCLUSION", "content": ""}]}